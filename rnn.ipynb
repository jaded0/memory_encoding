{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 18422222637 sequences.\n",
      "\n",
      "Batch 1\n",
      "Inputs shape: torch.Size([1, 100])\n",
      "Sequence: d or read of cremation i had had the misfortune to break my slate a few days before and the biggest \n",
      "\n",
      "Batch 2\n",
      "Inputs shape: torch.Size([1, 100])\n",
      "Sequence: ut of the in years answered cousin molly belle there is another road from her house to where everyda\n"
     ]
    }
   ],
   "source": [
    "from dataset_creation import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Instantiate the dataset\n",
    "text_dataset = TextDataset(directory='data/SPGC-tokens-2018-07-18/', sequence_length=100)\n",
    "print(f\"Dataset created with {len(text_dataset)} sequences.\")\n",
    "\n",
    "# Create a DataLoader without a sampler\n",
    "dataloader = DataLoader(text_dataset, batch_size=1)\n",
    "\n",
    "# Iterate over a few batches and print their contents\n",
    "for i, (inputs) in enumerate(dataloader):\n",
    "    if i >= 2:  # Adjust this value to see more/less batches\n",
    "        break\n",
    "\n",
    "    print(f\"\\nBatch {i+1}\")\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "\n",
    "    # Optionally print the actual sequences (comment out if too verbose)\n",
    "    sequence = ''.join([text_dataset.idx_to_char[int(idx)] for idx in inputs[0]])\n",
    "    # target = text_dataset.idx_to_char[int(targets[0])]\n",
    "    print(f\"Sequence: {sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 70\n",
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',', '.', ';', \"'\", '\"', '?', '!', ' ']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# def create_dataset(processed_text, sequence_length=100):\n",
    "#     characters = list(set(processed_text))\n",
    "#     char_to_idx = {char: idx for idx, char in enumerate(characters)}\n",
    "#     idx_to_char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "#     for i in range(len(processed_text) - sequence_length):\n",
    "#         input_seq = processed_text[i:i + sequence_length]\n",
    "#         target_char = processed_text[i + sequence_length]\n",
    "#         inputs.append([char_to_idx[char] for char in input_seq])\n",
    "#         targets.append(char_to_idx[target_char])\n",
    "\n",
    "#     return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long), idx_to_char, char_to_idx\n",
    "\n",
    "# # Example usage\n",
    "# sequence_length = 100\n",
    "# inputs, targets, idx_to_char, char_to_idx = create_dataset(processed_text, sequence_length)\n",
    "\n",
    "# Define chars using keys of char_to_idx\n",
    "chars = list(text_dataset.char_to_idx.keys())\n",
    "\n",
    "n_characters = len(chars)  # Number of unique characters\n",
    "print(f\"Number of unique characters: {n_characters}\")\n",
    "print(f\"Characters: {chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = torch.nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)  # Update dim to 1 for batch processing\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)  # Change dimension to 1\n",
    "\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# Ensure the input size matches the number of features for each input\n",
    "input_size = n_characters\n",
    "n_hidden = 128\n",
    "rnn = SimpleRNN(input_size, n_hidden, len(chars))\n",
    "\n",
    "# Define the loss function (criterion) and optimizer\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "\n",
    "# Apply Gradient Clipping\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)\n",
    "torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1.0)  # Clip gradients during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_line_tensor, target_char_tensor):\n",
    "    hidden = rnn.initHidden()  # Pass batch_size to initHidden\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # Process the entire input sequence\n",
    "    output, hidden = rnn(input_line_tensor, hidden)  # No need for loop here\n",
    "\n",
    "    # Modify the target tensor shape\n",
    "    target_char_tensor = target_char_tensor.view(-1)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(output.view(-1, len(chars)), target_char_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_line_tensor.shape: torch.Size([100])\n",
      "Epoch 1, Batch 0, Char 0 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 0: History: \" \", Predicted: \" \"\n",
      "Epoch 1, Batch 0, Char 1 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 1: History: \" l\", Predicted: \" l\"\n",
      "Epoch 1, Batch 0, Char 2 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 2: History: \" la\", Predicted: \" la\"\n",
      "Epoch 1, Batch 0, Char 3 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 3: History: \" lan\", Predicted: \" lan\"\n",
      "Epoch 1, Batch 0, Char 4 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 4: History: \" lang\", Predicted: \" lang\"\n",
      "Epoch 1, Batch 0, Char 5 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 5: History: \" langu\", Predicted: \" langu\"\n",
      "Epoch 1, Batch 0, Char 6 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 6: History: \" langui\", Predicted: \" langui\"\n",
      "Epoch 1, Batch 0, Char 7 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 7: History: \" languid\", Predicted: \" languid\"\n",
      "Epoch 1, Batch 0, Char 8 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 8: History: \" languidl\", Predicted: \" languidl\"\n",
      "Epoch 1, Batch 0, Char 9 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 9: History: \" languidly\", Predicted: \" languidly\"\n",
      "Epoch 1, Batch 0, Char 10 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 10: History: \" languidly \", Predicted: \" languidly \"\n",
      "Epoch 1, Batch 0, Char 11 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 11: History: \" languidly m\", Predicted: \" languidly m\"\n",
      "Epoch 1, Batch 0, Char 12 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 12: History: \" languidly my\", Predicted: \" languidly my\"\n",
      "Epoch 1, Batch 0, Char 13 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 13: History: \" languidly my \", Predicted: \" languidly my \"\n",
      "Epoch 1, Batch 0, Char 14 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 14: History: \" languidly my f\", Predicted: \" languidly my f\"\n",
      "Epoch 1, Batch 0, Char 15 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 15: History: \" languidly my fa\", Predicted: \" languidly my fa\"\n",
      "Epoch 1, Batch 0, Char 16 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 16: History: \" languidly my fat\", Predicted: \" languidly my fat\"\n",
      "Epoch 1, Batch 0, Char 17 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 17: History: \" languidly my fath\", Predicted: \" languidly my fath\"\n",
      "Epoch 1, Batch 0, Char 18 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 18: History: \" languidly my fathe\", Predicted: \" languidly my fathe\"\n",
      "Epoch 1, Batch 0, Char 19 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 19: History: \" languidly my father\", Predicted: \" languidly my father\"\n",
      "Epoch 1, Batch 0, Char 20 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 20: History: \" languidly my father \", Predicted: \" languidly my father \"\n",
      "Epoch 1, Batch 0, Char 21 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 21: History: \" languidly my father s\", Predicted: \" languidly my father s\"\n",
      "Epoch 1, Batch 0, Char 22 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 22: History: \" languidly my father sm\", Predicted: \" languidly my father sm\"\n",
      "Epoch 1, Batch 0, Char 23 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 23: History: \" languidly my father smo\", Predicted: \" languidly my father smo\"\n",
      "Epoch 1, Batch 0, Char 24 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 24: History: \" languidly my father smok\", Predicted: \" languidly my father smok\"\n",
      "Epoch 1, Batch 0, Char 25 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 25: History: \" languidly my father smoke\", Predicted: \" languidly my father smoke\"\n",
      "Epoch 1, Batch 0, Char 26 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 26: History: \" languidly my father smoked\", Predicted: \" languidly my father smoked\"\n",
      "Epoch 1, Batch 0, Char 27 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 27: History: \" languidly my father smoked \", Predicted: \" languidly my father smoked \"\n",
      "Epoch 1, Batch 0, Char 28 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 28: History: \" languidly my father smoked h\", Predicted: \" languidly my father smoked h\"\n",
      "Epoch 1, Batch 0, Char 29 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 29: History: \" languidly my father smoked hi\", Predicted: \" languidly my father smoked hi\"\n",
      "Epoch 1, Batch 0, Char 30 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 30: History: \" languidly my father smoked his\", Predicted: \" languidly my father smoked his\"\n",
      "Epoch 1, Batch 0, Char 31 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 31: History: \" languidly my father smoked his \", Predicted: \" languidly my father smoked his \"\n",
      "Epoch 1, Batch 0, Char 32 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 32: History: \" languidly my father smoked his p\", Predicted: \" languidly my father smoked his p\"\n",
      "Epoch 1, Batch 0, Char 33 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 33: History: \" languidly my father smoked his po\", Predicted: \" languidly my father smoked his po\"\n",
      "Epoch 1, Batch 0, Char 34 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 34: History: \" languidly my father smoked his pow\", Predicted: \" languidly my father smoked his pow\"\n",
      "Epoch 1, Batch 0, Char 35 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 35: History: \" languidly my father smoked his powh\", Predicted: \" languidly my father smoked his powh\"\n",
      "Epoch 1, Batch 0, Char 36 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 36: History: \" languidly my father smoked his powha\", Predicted: \" languidly my father smoked his powha\"\n",
      "Epoch 1, Batch 0, Char 37 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 37: History: \" languidly my father smoked his powhat\", Predicted: \" languidly my father smoked his powhat\"\n",
      "Epoch 1, Batch 0, Char 38 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 38: History: \" languidly my father smoked his powhata\", Predicted: \" languidly my father smoked his powhata\"\n",
      "Epoch 1, Batch 0, Char 39 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 39: History: \" languidly my father smoked his powhatan\", Predicted: \" languidly my father smoked his powhatan\"\n",
      "Epoch 1, Batch 0, Char 40 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 40: History: \" languidly my father smoked his powhatan \", Predicted: \" languidly my father smoked his powhatan \"\n",
      "Epoch 1, Batch 0, Char 41 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 41: History: \" languidly my father smoked his powhatan p\", Predicted: \" languidly my father smoked his powhatan p\"\n",
      "Epoch 1, Batch 0, Char 42 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 42: History: \" languidly my father smoked his powhatan pi\", Predicted: \" languidly my father smoked his powhatan pi\"\n",
      "Epoch 1, Batch 0, Char 43 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 43: History: \" languidly my father smoked his powhatan pip\", Predicted: \" languidly my father smoked his powhatan pip\"\n",
      "Epoch 1, Batch 0, Char 44 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 44: History: \" languidly my father smoked his powhatan pipe\", Predicted: \" languidly my father smoked his powhatan pipe\"\n",
      "Epoch 1, Batch 0, Char 45 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 45: History: \" languidly my father smoked his powhatan pipe \", Predicted: \" languidly my father smoked his powhatan pipe \"\n",
      "Epoch 1, Batch 0, Char 46 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 46: History: \" languidly my father smoked his powhatan pipe u\", Predicted: \" languidly my father smoked his powhatan pipe u\"\n",
      "Epoch 1, Batch 0, Char 47 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 47: History: \" languidly my father smoked his powhatan pipe up\", Predicted: \" languidly my father smoked his powhatan pipe up\"\n",
      "Epoch 1, Batch 0, Char 48 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 48: History: \" languidly my father smoked his powhatan pipe upo\", Predicted: \" languidly my father smoked his powhatan pipe upo\"\n",
      "Epoch 1, Batch 0, Char 49 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 49: History: \" languidly my father smoked his powhatan pipe upon\", Predicted: \" languidly my father smoked his powhatan pipe upon\"\n",
      "Epoch 1, Batch 0, Char 50 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 50: History: \" languidly my father smoked his powhatan pipe upon \", Predicted: \" languidly my father smoked his powhatan pipe upon \"\n",
      "Epoch 1, Batch 0, Char 51 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 51: History: \" languidly my father smoked his powhatan pipe upon t\", Predicted: \" languidly my father smoked his powhatan pipe upon t\"\n",
      "Epoch 1, Batch 0, Char 52 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 52: History: \" languidly my father smoked his powhatan pipe upon th\", Predicted: \" languidly my father smoked his powhatan pipe upon th\"\n",
      "Epoch 1, Batch 0, Char 53 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 53: History: \" languidly my father smoked his powhatan pipe upon the\", Predicted: \" languidly my father smoked his powhatan pipe upon the\"\n",
      "Epoch 1, Batch 0, Char 54 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 54: History: \" languidly my father smoked his powhatan pipe upon the \", Predicted: \" languidly my father smoked his powhatan pipe upon the \"\n",
      "Epoch 1, Batch 0, Char 55 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 55: History: \" languidly my father smoked his powhatan pipe upon the s\", Predicted: \" languidly my father smoked his powhatan pipe upon the s\"\n",
      "Epoch 1, Batch 0, Char 56 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 56: History: \" languidly my father smoked his powhatan pipe upon the st\", Predicted: \" languidly my father smoked his powhatan pipe upon the st\"\n",
      "Epoch 1, Batch 0, Char 57 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 57: History: \" languidly my father smoked his powhatan pipe upon the ste\", Predicted: \" languidly my father smoked his powhatan pipe upon the ste\"\n",
      "Epoch 1, Batch 0, Char 58 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 58: History: \" languidly my father smoked his powhatan pipe upon the step\", Predicted: \" languidly my father smoked his powhatan pipe upon the step\"\n",
      "Epoch 1, Batch 0, Char 59 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 59: History: \" languidly my father smoked his powhatan pipe upon the steps\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps\"\n",
      "Epoch 1, Batch 0, Char 60 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 60: History: \" languidly my father smoked his powhatan pipe upon the steps \", Predicted: \" languidly my father smoked his powhatan pipe upon the steps \"\n",
      "Epoch 1, Batch 0, Char 61 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 61: History: \" languidly my father smoked his powhatan pipe upon the steps l\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps l\"\n",
      "Epoch 1, Batch 0, Char 62 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 62: History: \" languidly my father smoked his powhatan pipe upon the steps le\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps le\"\n",
      "Epoch 1, Batch 0, Char 63 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 63: History: \" languidly my father smoked his powhatan pipe upon the steps lea\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps lea\"\n",
      "Epoch 1, Batch 0, Char 64 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 64: History: \" languidly my father smoked his powhatan pipe upon the steps lean\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps lean\"\n",
      "Epoch 1, Batch 0, Char 65 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 65: History: \" languidly my father smoked his powhatan pipe upon the steps leani\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leani\"\n",
      "Epoch 1, Batch 0, Char 66 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 66: History: \" languidly my father smoked his powhatan pipe upon the steps leanin\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leanin\"\n",
      "Epoch 1, Batch 0, Char 67 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 67: History: \" languidly my father smoked his powhatan pipe upon the steps leaning\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning\"\n",
      "Epoch 1, Batch 0, Char 68 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 68: History: \" languidly my father smoked his powhatan pipe upon the steps leaning \", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning \"\n",
      "Epoch 1, Batch 0, Char 69 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 69: History: \" languidly my father smoked his powhatan pipe upon the steps leaning a\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning a\"\n",
      "Epoch 1, Batch 0, Char 70 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 70: History: \" languidly my father smoked his powhatan pipe upon the steps leaning ag\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning ag\"\n",
      "Epoch 1, Batch 0, Char 71 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 71: History: \" languidly my father smoked his powhatan pipe upon the steps leaning aga\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning aga\"\n",
      "Epoch 1, Batch 0, Char 72 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 72: History: \" languidly my father smoked his powhatan pipe upon the steps leaning agai\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning agai\"\n",
      "Epoch 1, Batch 0, Char 73 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 73: History: \" languidly my father smoked his powhatan pipe upon the steps leaning again\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning again\"\n",
      "Epoch 1, Batch 0, Char 74 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 74: History: \" languidly my father smoked his powhatan pipe upon the steps leaning agains\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning agains\"\n",
      "Epoch 1, Batch 0, Char 75 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 75: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against\"\n",
      "Epoch 1, Batch 0, Char 76 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 76: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against \", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against \"\n",
      "Epoch 1, Batch 0, Char 77 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 77: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against o\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against o\"\n",
      "Epoch 1, Batch 0, Char 78 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 78: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against on\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against on\"\n",
      "Epoch 1, Batch 0, Char 79 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 79: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one\"\n",
      "Epoch 1, Batch 0, Char 80 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 80: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one \", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one \"\n",
      "Epoch 1, Batch 0, Char 81 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 81: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one p\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one p\"\n",
      "Epoch 1, Batch 0, Char 82 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 82: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pi\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pi\"\n",
      "Epoch 1, Batch 0, Char 83 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 83: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pil\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pil\"\n",
      "Epoch 1, Batch 0, Char 84 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 84: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pill\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pill\"\n",
      "Epoch 1, Batch 0, Char 85 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 85: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pilla\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pilla\"\n",
      "Epoch 1, Batch 0, Char 86 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 86: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar\"\n",
      "Epoch 1, Batch 0, Char 87 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 87: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar \", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar \"\n",
      "Epoch 1, Batch 0, Char 88 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 88: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar o\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar o\"\n",
      "Epoch 1, Batch 0, Char 89 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 89: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of\"\n",
      "Epoch 1, Batch 0, Char 90 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 90: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of \", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of \"\n",
      "Epoch 1, Batch 0, Char 91 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 91: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of t\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of t\"\n",
      "Epoch 1, Batch 0, Char 92 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 92: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of th\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of th\"\n",
      "Epoch 1, Batch 0, Char 93 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 93: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the\"\n",
      "Epoch 1, Batch 0, Char 94 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 94: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the \", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the \"\n",
      "Epoch 1, Batch 0, Char 95 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 95: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the r\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the r\"\n",
      "Epoch 1, Batch 0, Char 96 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 96: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the ro\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the ro\"\n",
      "Epoch 1, Batch 0, Char 97 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 97: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the roo\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the roo\"\n",
      "Epoch 1, Batch 0, Char 98 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 98: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the roof\", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the roof\"\n",
      "Epoch 1, Batch 0, Char 99 Loss: 0.0\n",
      "Epoch 1, Batch 0, Char 99: History: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the roof \", Predicted: \" languidly my father smoked his powhatan pipe upon the steps leaning against one pillar of the roof \"\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n",
      "input_line_tensor.shape: torch.Size([100])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train and calculate loss for each character\u001b[39;00m\n\u001b[1;32m     15\u001b[0m target_char \u001b[38;5;241m=\u001b[39m input_line_tensor[char_idx]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m output, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhot_input_char_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_char\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Char \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_line_tensor, target_char_tensor)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chars)), target_char_tensor)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, 101):\n",
    "    for batch_idx, (inputs) in enumerate(dataloader):\n",
    "        # Initialize variables to store the history and predicted characters for each batch\n",
    "        history = []\n",
    "        predicted_chars = []\n",
    "\n",
    "        input_line_tensor = inputs[0]  # Get the first character\n",
    "        print(f'input_line_tensor.shape: {input_line_tensor.shape}')\n",
    "        for char_idx in range(input_line_tensor.shape[0]):\n",
    "            # Convert to one-hot encoding for each character\n",
    "            hot_input_char_tensor = torch.nn.functional.one_hot(input_line_tensor[char_idx], num_classes=n_characters).type(torch.float)\n",
    "            \n",
    "            # Train and calculate loss for each character\n",
    "            target_char = input_line_tensor[char_idx].unsqueeze(0)\n",
    "            output, loss = train(hot_input_char_tensor.unsqueeze(0), target_char)\n",
    "           \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Char {char_idx} Loss: {loss}')\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                # Use the output to generate a character prediction\n",
    "                topv, topi = output.topk(1, dim=1)  # Change dim to 1\n",
    "                predicted_char = text_dataset.idx_to_char[topi[0, 0].item()]\n",
    "                target_char = text_dataset.idx_to_char[target_char.item()]\n",
    "\n",
    "                # Append the current character and prediction to their respective lists\n",
    "                history.append(target_char)\n",
    "                predicted_chars.append(predicted_char)\n",
    "\n",
    "                # Display the summarized history\n",
    "                history_str = ''.join(history)\n",
    "                predicted_str = ''.join(predicted_chars)\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Char {char_idx}: History: \"{history_str}\", Predicted: \"{predicted_str}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
