{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_creation import create_dataset\n",
    "\n",
    "# Create dataset from all token files in the directory\n",
    "sequence_length = 100\n",
    "inputs, targets, idx_to_char, char_to_idx = create_dataset('data/SPGC-tokens-2018-07-18/', sequence_length)\n",
    "\n",
    "# Print statement to verify dataset creation\n",
    "if len(inputs) > 0:\n",
    "    first_seq = ''.join([idx_to_char[idx.item()] for idx in inputs[0]])\n",
    "    target_char = idx_to_char[targets[0].item()]\n",
    "    print(f\"Dataset created successfully with {len(inputs)} sequences.\")\n",
    "    print(f\"First sequence: '{first_seq}'\")\n",
    "    print(f\"Target character for the first sequence: '{target_char}'\")\n",
    "else:\n",
    "    print(\"No sequences were created. Check the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# def create_dataset(processed_text, sequence_length=100):\n",
    "#     characters = list(set(processed_text))\n",
    "#     char_to_idx = {char: idx for idx, char in enumerate(characters)}\n",
    "#     idx_to_char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "#     for i in range(len(processed_text) - sequence_length):\n",
    "#         input_seq = processed_text[i:i + sequence_length]\n",
    "#         target_char = processed_text[i + sequence_length]\n",
    "#         inputs.append([char_to_idx[char] for char in input_seq])\n",
    "#         targets.append(char_to_idx[target_char])\n",
    "\n",
    "#     return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long), idx_to_char, char_to_idx\n",
    "\n",
    "# # Example usage\n",
    "# sequence_length = 100\n",
    "# inputs, targets, idx_to_char, char_to_idx = create_dataset(processed_text, sequence_length)\n",
    "\n",
    "# Define chars using keys of char_to_idx\n",
    "chars = list(char_to_idx.keys())\n",
    "\n",
    "n_characters = len(chars)  # Number of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Adjust the input size as needed\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = torch.nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# Ensure the input size matches the number of features for each input\n",
    "input_size = n_characters\n",
    "n_hidden = 128\n",
    "rnn = SimpleRNN(input_size, n_hidden, len(chars))\n",
    "\n",
    "# Define the loss function (criterion) and optimizer\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_line_tensor, target_char_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # Process the entire input sequence\n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output, hidden = rnn(input_line_tensor[i].view(1, -1), hidden)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(output, target_char_tensor.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Loss: 3.2974753379821777\n",
      "Epoch 200 Loss: 2.916781187057495\n",
      "Epoch 300 Loss: 3.62199068069458\n",
      "Epoch 400 Loss: 1.910137414932251\n",
      "Epoch 500 Loss: 3.6185874938964844\n",
      "Epoch 500: Input Sequence: \"e world is very different now for man holds in his mortal hands the power to abolish all forms of hu\"\n",
      "Predicted \" \", Target \"m\"\n",
      "Epoch 600 Loss: 3.1989240646362305\n",
      "Epoch 700 Loss: 3.1318187713623047\n",
      "Epoch 800 Loss: 3.9293618202209473\n",
      "Epoch 900 Loss: 0.7651025652885437\n",
      "Epoch 1000 Loss: 3.552541732788086\n",
      "Epoch 1000: Input Sequence: \" been passed to a new generation of americans born in this century tempered by war disciplined by a \"\n",
      "Predicted \"d\", Target \"h\"\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, 1001):\n",
    "    index = epoch % len(inputs)\n",
    "    input_line_tensor = inputs[index]\n",
    "    target_char_tensor = targets[index]\n",
    "\n",
    "    # Convert to one-hot encoding\n",
    "    input_line_tensor = torch.nn.functional.one_hot(input_line_tensor, num_classes=n_characters).type(torch.float)\n",
    "\n",
    "    # Train and calculate loss\n",
    "    output, loss = train(input_line_tensor, target_char_tensor)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch} Loss: {loss}')\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        # Use the output to generate a character prediction\n",
    "        topv, topi = output.topk(1)\n",
    "        predicted_char = idx_to_char[topi[0].item()]\n",
    "        target_char = idx_to_char[target_char_tensor.item()]\n",
    "\n",
    "        # Retrieve the input sequence as characters\n",
    "        input_seq = ''.join([idx_to_char[idx.item()] for idx in inputs[index]])\n",
    "        \n",
    "        print(f'Epoch {epoch}: Input Sequence: \"{input_seq}\"')\n",
    "        print(f'Predicted \"{predicted_char}\", Target \"{target_char}\"')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
