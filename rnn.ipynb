{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 18422222637 sequences.\n",
      "\n",
      "Batch 1\n",
      "Inputs shape: torch.Size([1, 100])\n",
      "Sequence: ndlesticks it was july and the days were at their longest according to the warrock almanac that hung\n",
      "\n",
      "Batch 2\n",
      "Inputs shape: torch.Size([1, 100])\n",
      "Sequence:  we wore outside of these quilted skirts interlined with wool my mother had a nervous dread of fire \n"
     ]
    }
   ],
   "source": [
    "from dataset_creation import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Instantiate the dataset\n",
    "text_dataset = TextDataset(directory='data/SPGC-tokens-2018-07-18/', sequence_length=100)\n",
    "print(f\"Dataset created with {len(text_dataset)} sequences.\")\n",
    "\n",
    "# Create a DataLoader without a sampler\n",
    "dataloader = DataLoader(text_dataset, batch_size=1)\n",
    "\n",
    "# Iterate over a few batches and print their contents\n",
    "for i, (inputs) in enumerate(dataloader):\n",
    "    if i >= 2:  # Adjust this value to see more/less batches\n",
    "        break\n",
    "\n",
    "    print(f\"\\nBatch {i+1}\")\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "\n",
    "    # Optionally print the actual sequences (comment out if too verbose)\n",
    "    sequence = ''.join([text_dataset.idx_to_char[int(idx)] for idx in inputs[0]])\n",
    "    # target = text_dataset.idx_to_char[int(targets[0])]\n",
    "    print(f\"Sequence: {sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 70\n",
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',', '.', ';', \"'\", '\"', '?', '!', ' ']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# def create_dataset(processed_text, sequence_length=100):\n",
    "#     characters = list(set(processed_text))\n",
    "#     char_to_idx = {char: idx for idx, char in enumerate(characters)}\n",
    "#     idx_to_char = {idx: char for idx, char in enumerate(characters)}\n",
    "\n",
    "#     inputs = []\n",
    "#     targets = []\n",
    "#     for i in range(len(processed_text) - sequence_length):\n",
    "#         input_seq = processed_text[i:i + sequence_length]\n",
    "#         target_char = processed_text[i + sequence_length]\n",
    "#         inputs.append([char_to_idx[char] for char in input_seq])\n",
    "#         targets.append(char_to_idx[target_char])\n",
    "\n",
    "#     return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long), idx_to_char, char_to_idx\n",
    "\n",
    "# # Example usage\n",
    "# sequence_length = 100\n",
    "# inputs, targets, idx_to_char, char_to_idx = create_dataset(processed_text, sequence_length)\n",
    "\n",
    "# Define chars using keys of char_to_idx\n",
    "chars = list(text_dataset.char_to_idx.keys())\n",
    "\n",
    "n_characters = len(chars)  # Number of unique characters\n",
    "print(f\"Number of unique characters: {n_characters}\")\n",
    "print(f\"Characters: {chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = torch.nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)  # Update dim to 1 for batch processing\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)  # Change dimension to 1\n",
    "\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "# Ensure the input size matches the number of features for each input\n",
    "input_size = n_characters\n",
    "output_size = n_characters\n",
    "n_hidden = 128\n",
    "rnn = SimpleRNN(input_size, n_hidden, output_size)\n",
    "\n",
    "# Define the loss function (criterion) and optimizer\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "\n",
    "# Apply Gradient Clipping\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)\n",
    "torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1.0)  # Clip gradients during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_line_tensor, target_char_tensor):\n",
    "    hidden = rnn.initHidden()  # Pass batch_size to initHidden\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # Process the entire input sequence\n",
    "    output, hidden = rnn(input_line_tensor, hidden)  # No need for loop here\n",
    "\n",
    "    # Modify the target tensor shape\n",
    "    target_char_tensor = target_char_tensor.view(-1)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = criterion(output.view(-1, len(chars)), target_char_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Char 0 Loss: 4.248201847076416\n",
      "Epoch 1, Batch 0, Char 0\n",
      "target: l predicted: .\n",
      "History: \"l\", Predicted: \".\"\n",
      "Epoch 1, Batch 0, Char 1 Loss: 4.173136234283447\n",
      "Epoch 1, Batch 0, Char 1\n",
      "target: i predicted: .\n",
      "History: \"li\", Predicted: \"..\"\n",
      "Epoch 1, Batch 0, Char 2 Loss: 4.2358174324035645\n",
      "Epoch 1, Batch 0, Char 2\n",
      "target: f predicted: Z\n",
      "History: \"lif\", Predicted: \"..Z\"\n",
      "Epoch 1, Batch 0, Char 3 Loss: 4.230856895446777\n",
      "Epoch 1, Batch 0, Char 3\n",
      "target: t predicted: V\n",
      "History: \"lift\", Predicted: \"..ZV\"\n",
      "Epoch 1, Batch 0, Char 4 Loss: 4.3440656661987305\n",
      "Epoch 1, Batch 0, Char 4\n",
      "target: e predicted: t\n",
      "History: \"lifte\", Predicted: \"..ZVt\"\n",
      "Epoch 1, Batch 0, Char 5 Loss: 4.243505477905273\n",
      "Epoch 1, Batch 0, Char 5\n",
      "target: d predicted: Y\n",
      "History: \"lifted\", Predicted: \"..ZVtY\"\n",
      "Epoch 1, Batch 0, Char 6 Loss: 4.220548629760742\n",
      "Epoch 1, Batch 0, Char 6\n",
      "target:   predicted: t\n",
      "History: \"lifted \", Predicted: \"..ZVtYt\"\n",
      "Epoch 1, Batch 0, Char 7 Loss: 4.2895307540893555\n",
      "Epoch 1, Batch 0, Char 7\n",
      "target: u predicted: .\n",
      "History: \"lifted u\", Predicted: \"..ZVtYt.\"\n",
      "Epoch 1, Batch 0, Char 8 Loss: 4.25580358505249\n",
      "Epoch 1, Batch 0, Char 8\n",
      "target: p predicted: f\n",
      "History: \"lifted up\", Predicted: \"..ZVtYt.f\"\n",
      "Epoch 1, Batch 0, Char 9 Loss: 4.148636817932129\n",
      "Epoch 1, Batch 0, Char 9\n",
      "target:   predicted: f\n",
      "History: \"lifted up \", Predicted: \"..ZVtYt.ff\"\n",
      "Epoch 1, Batch 0, Char 10 Loss: 4.224078178405762\n",
      "Epoch 1, Batch 0, Char 10\n",
      "target: o predicted: l\n",
      "History: \"lifted up o\", Predicted: \"..ZVtYt.ffl\"\n",
      "Epoch 1, Batch 0, Char 11 Loss: 4.258388042449951\n",
      "Epoch 1, Batch 0, Char 11\n",
      "target: u predicted: 5\n",
      "History: \"lifted up ou\", Predicted: \"..ZVtYt.ffl5\"\n",
      "Epoch 1, Batch 0, Char 12 Loss: 4.354650497436523\n",
      "Epoch 1, Batch 0, Char 12\n",
      "target: r predicted: f\n",
      "History: \"lifted up our\", Predicted: \"..ZVtYt.ffl5f\"\n",
      "Epoch 1, Batch 0, Char 13 Loss: 4.184997081756592\n",
      "Epoch 1, Batch 0, Char 13\n",
      "target:   predicted: f\n",
      "History: \"lifted up our \", Predicted: \"..ZVtYt.ffl5ff\"\n",
      "Epoch 1, Batch 0, Char 14 Loss: 4.245953559875488\n",
      "Epoch 1, Batch 0, Char 14\n",
      "target: v predicted: l\n",
      "History: \"lifted up our v\", Predicted: \"..ZVtYt.ffl5ffl\"\n",
      "Epoch 1, Batch 0, Char 15 Loss: 4.151878833770752\n",
      "Epoch 1, Batch 0, Char 15\n",
      "target: o predicted: l\n",
      "History: \"lifted up our vo\", Predicted: \"..ZVtYt.ffl5ffll\"\n",
      "Epoch 1, Batch 0, Char 16 Loss: 4.1926140785217285\n",
      "Epoch 1, Batch 0, Char 16\n",
      "target: i predicted:  \n",
      "History: \"lifted up our voi\", Predicted: \"..ZVtYt.ffl5ffll \"\n",
      "Epoch 1, Batch 0, Char 17 Loss: 4.179976940155029\n",
      "Epoch 1, Batch 0, Char 17\n",
      "target: c predicted: d\n",
      "History: \"lifted up our voic\", Predicted: \"..ZVtYt.ffl5ffll d\"\n",
      "Epoch 1, Batch 0, Char 18 Loss: 4.16604471206665\n",
      "Epoch 1, Batch 0, Char 18\n",
      "target: e predicted: f\n",
      "History: \"lifted up our voice\", Predicted: \"..ZVtYt.ffl5ffll df\"\n",
      "Epoch 1, Batch 0, Char 19 Loss: 4.305053234100342\n",
      "Epoch 1, Batch 0, Char 19\n",
      "target: s predicted: f\n",
      "History: \"lifted up our voices\", Predicted: \"..ZVtYt.ffl5ffll dff\"\n",
      "Epoch 1, Batch 0, Char 20 Loss: 4.180628299713135\n",
      "Epoch 1, Batch 0, Char 20\n",
      "target:   predicted: t\n",
      "History: \"lifted up our voices \", Predicted: \"..ZVtYt.ffl5ffll dfft\"\n",
      "Epoch 1, Batch 0, Char 21 Loss: 4.299813270568848\n",
      "Epoch 1, Batch 0, Char 21\n",
      "target: w predicted: l\n",
      "History: \"lifted up our voices w\", Predicted: \"..ZVtYt.ffl5ffll dfftl\"\n",
      "Epoch 1, Batch 0, Char 22 Loss: 4.205631732940674\n",
      "Epoch 1, Batch 0, Char 22\n",
      "target: e predicted: d\n",
      "History: \"lifted up our voices we\", Predicted: \"..ZVtYt.ffl5ffll dfftld\"\n",
      "Epoch 1, Batch 0, Char 23 Loss: 4.288203716278076\n",
      "Epoch 1, Batch 0, Char 23\n",
      "target: a predicted:  \n",
      "History: \"lifted up our voices wea\", Predicted: \"..ZVtYt.ffl5ffll dfftld \"\n",
      "Epoch 1, Batch 0, Char 24 Loss: 4.224920272827148\n",
      "Epoch 1, Batch 0, Char 24\n",
      "target: k predicted: t\n",
      "History: \"lifted up our voices weak\", Predicted: \"..ZVtYt.ffl5ffll dfftld t\"\n",
      "Epoch 1, Batch 0, Char 25 Loss: 4.131422519683838\n",
      "Epoch 1, Batch 0, Char 25\n",
      "target:   predicted: t\n",
      "History: \"lifted up our voices weak \", Predicted: \"..ZVtYt.ffl5ffll dfftld tt\"\n",
      "Epoch 1, Batch 0, Char 26 Loss: 4.2530198097229\n",
      "Epoch 1, Batch 0, Char 26\n",
      "target: w predicted: l\n",
      "History: \"lifted up our voices weak w\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttl\"\n",
      "Epoch 1, Batch 0, Char 27 Loss: 4.0912184715271\n",
      "Epoch 1, Batch 0, Char 27\n",
      "target: i predicted: d\n",
      "History: \"lifted up our voices weak wi\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld\"\n",
      "Epoch 1, Batch 0, Char 28 Loss: 4.1250901222229\n",
      "Epoch 1, Batch 0, Char 28\n",
      "target: t predicted:  \n",
      "History: \"lifted up our voices weak wit\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld \"\n",
      "Epoch 1, Batch 0, Char 29 Loss: 4.31070613861084\n",
      "Epoch 1, Batch 0, Char 29\n",
      "target: h predicted: t\n",
      "History: \"lifted up our voices weak with\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld t\"\n",
      "Epoch 1, Batch 0, Char 30 Loss: 4.11009407043457\n",
      "Epoch 1, Batch 0, Char 30\n",
      "target:   predicted: i\n",
      "History: \"lifted up our voices weak with \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld ti\"\n",
      "Epoch 1, Batch 0, Char 31 Loss: 4.201924800872803\n",
      "Epoch 1, Batch 0, Char 31\n",
      "target: w predicted: l\n",
      "History: \"lifted up our voices weak with w\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld til\"\n",
      "Epoch 1, Batch 0, Char 32 Loss: 4.113861560821533\n",
      "Epoch 1, Batch 0, Char 32\n",
      "target: e predicted: i\n",
      "History: \"lifted up our voices weak with we\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili\"\n",
      "Epoch 1, Batch 0, Char 33 Loss: 4.111666202545166\n",
      "Epoch 1, Batch 0, Char 33\n",
      "target: e predicted:  \n",
      "History: \"lifted up our voices weak with wee\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili \"\n",
      "Epoch 1, Batch 0, Char 34 Loss: 4.103423118591309\n",
      "Epoch 1, Batch 0, Char 34\n",
      "target: p predicted:  \n",
      "History: \"lifted up our voices weak with weep\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili  \"\n",
      "Epoch 1, Batch 0, Char 35 Loss: 4.125892162322998\n",
      "Epoch 1, Batch 0, Char 35\n",
      "target: i predicted:  \n",
      "History: \"lifted up our voices weak with weepi\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili   \"\n",
      "Epoch 1, Batch 0, Char 36 Loss: 4.226704120635986\n",
      "Epoch 1, Batch 0, Char 36\n",
      "target: n predicted:  \n",
      "History: \"lifted up our voices weak with weepin\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    \"\n",
      "Epoch 1, Batch 0, Char 37 Loss: 4.284295558929443\n",
      "Epoch 1, Batch 0, Char 37\n",
      "target: g predicted: t\n",
      "History: \"lifted up our voices weak with weeping\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t\"\n",
      "Epoch 1, Batch 0, Char 38 Loss: 3.9977188110351562\n",
      "Epoch 1, Batch 0, Char 38\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t \"\n",
      "Epoch 1, Batch 0, Char 39 Loss: 4.121384143829346\n",
      "Epoch 1, Batch 0, Char 39\n",
      "target: i predicted: l\n",
      "History: \"lifted up our voices weak with weeping i\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l\"\n",
      "Epoch 1, Batch 0, Char 40 Loss: 4.186473369598389\n",
      "Epoch 1, Batch 0, Char 40\n",
      "target: n predicted:  \n",
      "History: \"lifted up our voices weak with weeping in\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l \"\n",
      "Epoch 1, Batch 0, Char 41 Loss: 4.0967535972595215\n",
      "Epoch 1, Batch 0, Char 41\n",
      "target:   predicted: t\n",
      "History: \"lifted up our voices weak with weeping in \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l t\"\n",
      "Epoch 1, Batch 0, Char 42 Loss: 4.20770263671875\n",
      "Epoch 1, Batch 0, Char 42\n",
      "target: a predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl\"\n",
      "Epoch 1, Batch 0, Char 43 Loss: 3.9834187030792236\n",
      "Epoch 1, Batch 0, Char 43\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl \"\n",
      "Epoch 1, Batch 0, Char 44 Loss: 4.077353477478027\n",
      "Epoch 1, Batch 0, Char 44\n",
      "target: t predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a t\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl l\"\n",
      "Epoch 1, Batch 0, Char 45 Loss: 4.197690010070801\n",
      "Epoch 1, Batch 0, Char 45\n",
      "target: h predicted: t\n",
      "History: \"lifted up our voices weak with weeping in a th\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lt\"\n",
      "Epoch 1, Batch 0, Char 46 Loss: 3.9316351413726807\n",
      "Epoch 1, Batch 0, Char 46\n",
      "target: i predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thi\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti\"\n",
      "Epoch 1, Batch 0, Char 47 Loss: 4.112029552459717\n",
      "Epoch 1, Batch 0, Char 47\n",
      "target: n predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti \"\n",
      "Epoch 1, Batch 0, Char 48 Loss: 4.0200371742248535\n",
      "Epoch 1, Batch 0, Char 48\n",
      "target:   predicted: t\n",
      "History: \"lifted up our voices weak with weeping in a thin \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti t\"\n",
      "Epoch 1, Batch 0, Char 49 Loss: 4.211663722991943\n",
      "Epoch 1, Batch 0, Char 49\n",
      "target: s predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin s\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl\"\n",
      "Epoch 1, Batch 0, Char 50 Loss: 4.084478378295898\n",
      "Epoch 1, Batch 0, Char 50\n",
      "target: c predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin sc\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl \"\n",
      "Epoch 1, Batch 0, Char 51 Loss: 4.1983466148376465\n",
      "Epoch 1, Batch 0, Char 51\n",
      "target: r predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin scr\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl  \"\n",
      "Epoch 1, Batch 0, Char 52 Loss: 4.097373008728027\n",
      "Epoch 1, Batch 0, Char 52\n",
      "target: e predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin scre\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl   \"\n",
      "Epoch 1, Batch 0, Char 53 Loss: 3.963876962661743\n",
      "Epoch 1, Batch 0, Char 53\n",
      "target: e predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin scree\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl    \"\n",
      "Epoch 1, Batch 0, Char 54 Loss: 4.062439918518066\n",
      "Epoch 1, Batch 0, Char 54\n",
      "target: c predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screec\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl     \"\n",
      "Epoch 1, Batch 0, Char 55 Loss: 4.247925758361816\n",
      "Epoch 1, Batch 0, Char 55\n",
      "target: h predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      \"\n",
      "Epoch 1, Batch 0, Char 56 Loss: 3.901203155517578\n",
      "Epoch 1, Batch 0, Char 56\n",
      "target:   predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thin screech \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      i\"\n",
      "Epoch 1, Batch 0, Char 57 Loss: 3.973384380340576\n",
      "Epoch 1, Batch 0, Char 57\n",
      "target: i predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il\"\n",
      "Epoch 1, Batch 0, Char 58 Loss: 3.88871169090271\n",
      "Epoch 1, Batch 0, Char 58\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il \"\n",
      "Epoch 1, Batch 0, Char 59 Loss: 4.127561569213867\n",
      "Epoch 1, Batch 0, Char 59\n",
      "target: s predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i s\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l\"\n",
      "Epoch 1, Batch 0, Char 60 Loss: 4.115694522857666\n",
      "Epoch 1, Batch 0, Char 60\n",
      "target: a predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i sa\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l \"\n",
      "Epoch 1, Batch 0, Char 61 Loss: 3.90641713142395\n",
      "Epoch 1, Batch 0, Char 61\n",
      "target: i predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i sai\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l  \"\n",
      "Epoch 1, Batch 0, Char 62 Loss: 3.968440532684326\n",
      "Epoch 1, Batch 0, Char 62\n",
      "target: d predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l   \"\n",
      "Epoch 1, Batch 0, Char 63 Loss: 3.853522777557373\n",
      "Epoch 1, Batch 0, Char 63\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    \"\n",
      "Epoch 1, Batch 0, Char 64 Loss: 4.233541965484619\n",
      "Epoch 1, Batch 0, Char 64\n",
      "target: h predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said h\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    l\"\n",
      "Epoch 1, Batch 0, Char 65 Loss: 3.934008836746216\n",
      "Epoch 1, Batch 0, Char 65\n",
      "target: e predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said he\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li\"\n",
      "Epoch 1, Batch 0, Char 66 Loss: 4.070044040679932\n",
      "Epoch 1, Batch 0, Char 66\n",
      "target: l predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said hel\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li \"\n",
      "Epoch 1, Batch 0, Char 67 Loss: 4.004849433898926\n",
      "Epoch 1, Batch 0, Char 67\n",
      "target: p predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i\"\n",
      "Epoch 1, Batch 0, Char 68 Loss: 3.753884792327881\n",
      "Epoch 1, Batch 0, Char 68\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i \"\n",
      "Epoch 1, Batch 0, Char 69 Loss: 4.182643890380859\n",
      "Epoch 1, Batch 0, Char 69\n",
      "target: h predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help h\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i l\"\n",
      "Epoch 1, Batch 0, Char 70 Loss: 3.8830032348632812\n",
      "Epoch 1, Batch 0, Char 70\n",
      "target: e predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help he\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i li\"\n",
      "Epoch 1, Batch 0, Char 71 Loss: 4.021811485290527\n",
      "Epoch 1, Batch 0, Char 71\n",
      "target: l predicted: e\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help hel\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i lie\"\n",
      "Epoch 1, Batch 0, Char 72 Loss: 3.954014301300049\n",
      "Epoch 1, Batch 0, Char 72\n",
      "target: p predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei\"\n",
      "Epoch 1, Batch 0, Char 73 Loss: 3.7070181369781494\n",
      "Epoch 1, Batch 0, Char 73\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei \"\n",
      "Epoch 1, Batch 0, Char 74 Loss: 4.127331256866455\n",
      "Epoch 1, Batch 0, Char 74\n",
      "target: h predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help h\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei l\"\n",
      "Epoch 1, Batch 0, Char 75 Loss: 3.8259339332580566\n",
      "Epoch 1, Batch 0, Char 75\n",
      "target: e predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help he\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei li\"\n",
      "Epoch 1, Batch 0, Char 76 Loss: 3.9649744033813477\n",
      "Epoch 1, Batch 0, Char 76\n",
      "target: l predicted: e\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help hel\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei lie\"\n",
      "Epoch 1, Batch 0, Char 77 Loss: 3.8925440311431885\n",
      "Epoch 1, Batch 0, Char 77\n",
      "target: p predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei\"\n",
      "Epoch 1, Batch 0, Char 78 Loss: 3.6527793407440186\n",
      "Epoch 1, Batch 0, Char 78\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei \"\n",
      "Epoch 1, Batch 0, Char 79 Loss: 3.9819705486297607\n",
      "Epoch 1, Batch 0, Char 79\n",
      "target: s predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help s\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l\"\n",
      "Epoch 1, Batch 0, Char 80 Loss: 4.084338665008545\n",
      "Epoch 1, Batch 0, Char 80\n",
      "target: h predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help sh\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l \"\n",
      "Epoch 1, Batch 0, Char 81 Loss: 3.757575750350952\n",
      "Epoch 1, Batch 0, Char 81\n",
      "target: e predicted: i\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l i\"\n",
      "Epoch 1, Batch 0, Char 82 Loss: 3.7871975898742676\n",
      "Epoch 1, Batch 0, Char 82\n",
      "target:   predicted: e\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l ie\"\n",
      "Epoch 1, Batch 0, Char 83 Loss: 3.9814324378967285\n",
      "Epoch 1, Batch 0, Char 83\n",
      "target: c predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she c\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iel\"\n",
      "Epoch 1, Batch 0, Char 84 Loss: 4.012210369110107\n",
      "Epoch 1, Batch 0, Char 84\n",
      "target: r predicted: e\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cr\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele\"\n",
      "Epoch 1, Batch 0, Char 85 Loss: 3.8051462173461914\n",
      "Epoch 1, Batch 0, Char 85\n",
      "target: i predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cri\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele \"\n",
      "Epoch 1, Batch 0, Char 86 Loss: 3.904129981994629\n",
      "Epoch 1, Batch 0, Char 86\n",
      "target: e predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she crie\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  \"\n",
      "Epoch 1, Batch 0, Char 87 Loss: 3.814621925354004\n",
      "Epoch 1, Batch 0, Char 87\n",
      "target: d predicted: e\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e\"\n",
      "Epoch 1, Batch 0, Char 88 Loss: 3.6743338108062744\n",
      "Epoch 1, Batch 0, Char 88\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e \"\n",
      "Epoch 1, Batch 0, Char 89 Loss: 4.417038440704346\n",
      "Epoch 1, Batch 0, Char 89\n",
      "target: m predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried m\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l\"\n",
      "Epoch 1, Batch 0, Char 90 Loss: 4.0224714279174805\n",
      "Epoch 1, Batch 0, Char 90\n",
      "target: u predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried mu\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l \"\n",
      "Epoch 1, Batch 0, Char 91 Loss: 4.016544818878174\n",
      "Epoch 1, Batch 0, Char 91\n",
      "target: r predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried mur\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l  \"\n",
      "Epoch 1, Batch 0, Char 92 Loss: 3.93760085105896\n",
      "Epoch 1, Batch 0, Char 92\n",
      "target: d predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried murd\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l   \"\n",
      "Epoch 1, Batch 0, Char 93 Loss: 3.8532469272613525\n",
      "Epoch 1, Batch 0, Char 93\n",
      "target: e predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried murde\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l    \"\n",
      "Epoch 1, Batch 0, Char 94 Loss: 4.042365550994873\n",
      "Epoch 1, Batch 0, Char 94\n",
      "target: r predicted: e\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried murder\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l    e\"\n",
      "Epoch 1, Batch 0, Char 95 Loss: 3.660759687423706\n",
      "Epoch 1, Batch 0, Char 95\n",
      "target:   predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried murder \", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l    e \"\n",
      "Epoch 1, Batch 0, Char 96 Loss: 4.3453264236450195\n",
      "Epoch 1, Batch 0, Char 96\n",
      "target: m predicted: l\n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried murder m\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l    e l\"\n",
      "Epoch 1, Batch 0, Char 97 Loss: 3.94454026222229\n",
      "Epoch 1, Batch 0, Char 97\n",
      "target: u predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried murder mu\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l    e l \"\n",
      "Epoch 1, Batch 0, Char 98 Loss: 3.9437859058380127\n",
      "Epoch 1, Batch 0, Char 98\n",
      "target: r predicted:  \n",
      "History: \"lifted up our voices weak with weeping in a thin screech i said help help help she cried murder mur\", Predicted: \"..ZVtYt.ffl5ffll dfftld ttld tili    t l tl lti tl      il l    li i liei liei l iele  e l    e l  \"\n",
      "Epoch 1, Batch 100, Char 0 Loss: 3.2121262550354004\n",
      "Epoch 1, Batch 100, Char 1 Loss: 1.6818411350250244\n",
      "Epoch 1, Batch 100, Char 2 Loss: 3.377211809158325\n",
      "Epoch 1, Batch 100, Char 3 Loss: 1.077301025390625\n",
      "Epoch 1, Batch 100, Char 4 Loss: 2.133037567138672\n",
      "Epoch 1, Batch 100, Char 5 Loss: 1.2756099700927734\n",
      "Epoch 1, Batch 100, Char 6 Loss: 2.0817294120788574\n",
      "Epoch 1, Batch 100, Char 7 Loss: 0.9886314272880554\n",
      "Epoch 1, Batch 100, Char 8 Loss: 2.2527949810028076\n",
      "Epoch 1, Batch 100, Char 9 Loss: 4.119077682495117\n",
      "Epoch 1, Batch 100, Char 10 Loss: 2.1185848712921143\n",
      "Epoch 1, Batch 100, Char 11 Loss: 1.651491403579712\n",
      "Epoch 1, Batch 100, Char 12 Loss: 2.3688199520111084\n",
      "Epoch 1, Batch 100, Char 13 Loss: 1.2007540464401245\n",
      "Epoch 1, Batch 100, Char 14 Loss: 2.1704018115997314\n",
      "Epoch 1, Batch 100, Char 15 Loss: 1.0382792949676514\n",
      "Epoch 1, Batch 100, Char 16 Loss: 0.6286579966545105\n",
      "Epoch 1, Batch 100, Char 17 Loss: 1.1597890853881836\n",
      "Epoch 1, Batch 100, Char 18 Loss: 2.2351083755493164\n",
      "Epoch 1, Batch 100, Char 19 Loss: 4.699092864990234\n",
      "Epoch 1, Batch 100, Char 20 Loss: 3.206427574157715\n",
      "Epoch 1, Batch 100, Char 21 Loss: 2.8658430576324463\n",
      "Epoch 1, Batch 100, Char 22 Loss: 3.4422073364257812\n",
      "Epoch 1, Batch 100, Char 23 Loss: 3.1208083629608154\n",
      "Epoch 1, Batch 100, Char 24 Loss: 1.314986228942871\n",
      "Epoch 1, Batch 100, Char 25 Loss: 2.6576502323150635\n",
      "Epoch 1, Batch 100, Char 26 Loss: 0.6272116303443909\n",
      "Epoch 1, Batch 100, Char 27 Loss: 1.1447899341583252\n",
      "Epoch 1, Batch 100, Char 28 Loss: 5.329617977142334\n",
      "Epoch 1, Batch 100, Char 29 Loss: 3.09026837348938\n",
      "Epoch 1, Batch 100, Char 30 Loss: 2.985902786254883\n",
      "Epoch 1, Batch 100, Char 31 Loss: 3.724844455718994\n",
      "Epoch 1, Batch 100, Char 32 Loss: 4.17188024520874\n",
      "Epoch 1, Batch 100, Char 33 Loss: 1.1094940900802612\n",
      "Epoch 1, Batch 100, Char 34 Loss: 2.190673828125\n",
      "Epoch 1, Batch 100, Char 35 Loss: 2.5671472549438477\n",
      "Epoch 1, Batch 100, Char 36 Loss: 1.8099427223205566\n",
      "Epoch 1, Batch 100, Char 37 Loss: 2.076880693435669\n",
      "Epoch 1, Batch 100, Char 38 Loss: 2.173405647277832\n",
      "Epoch 1, Batch 100, Char 39 Loss: 3.974194049835205\n",
      "Epoch 1, Batch 100, Char 40 Loss: 1.9913036823272705\n",
      "Epoch 1, Batch 100, Char 41 Loss: 1.609238624572754\n",
      "Epoch 1, Batch 100, Char 42 Loss: 2.270458459854126\n",
      "Epoch 1, Batch 100, Char 43 Loss: 1.1990553140640259\n",
      "Epoch 1, Batch 100, Char 44 Loss: 2.759153366088867\n",
      "Epoch 1, Batch 100, Char 45 Loss: 1.9043020009994507\n",
      "Epoch 1, Batch 100, Char 46 Loss: 1.1881766319274902\n",
      "Epoch 1, Batch 100, Char 47 Loss: 2.6232643127441406\n",
      "Epoch 1, Batch 100, Char 48 Loss: 0.6285296082496643\n",
      "Epoch 1, Batch 100, Char 49 Loss: 1.1050212383270264\n",
      "Epoch 1, Batch 100, Char 50 Loss: 2.610596179962158\n",
      "Epoch 1, Batch 100, Char 51 Loss: 0.6258501410484314\n",
      "Epoch 1, Batch 100, Char 52 Loss: 2.9310102462768555\n",
      "Epoch 1, Batch 100, Char 53 Loss: 4.175414085388184\n",
      "Epoch 1, Batch 100, Char 54 Loss: 1.8558515310287476\n",
      "Epoch 1, Batch 100, Char 55 Loss: 2.56052565574646\n",
      "Epoch 1, Batch 100, Char 56 Loss: 0.4743233919143677\n",
      "Epoch 1, Batch 100, Char 57 Loss: 2.654414415359497\n",
      "Epoch 1, Batch 100, Char 58 Loss: 1.64681077003479\n",
      "Epoch 1, Batch 100, Char 59 Loss: 1.0909136533737183\n",
      "Epoch 1, Batch 100, Char 60 Loss: 3.9642627239227295\n",
      "Epoch 1, Batch 100, Char 61 Loss: 4.474079132080078\n",
      "Epoch 1, Batch 100, Char 62 Loss: 0.29002851247787476\n",
      "Epoch 1, Batch 100, Char 63 Loss: 1.9929778575897217\n",
      "Epoch 1, Batch 100, Char 64 Loss: 1.2552051544189453\n",
      "Epoch 1, Batch 100, Char 65 Loss: 2.743870258331299\n",
      "Epoch 1, Batch 100, Char 66 Loss: 2.891592502593994\n",
      "Epoch 1, Batch 100, Char 67 Loss: 2.212294101715088\n",
      "Epoch 1, Batch 100, Char 68 Loss: 2.614839553833008\n",
      "Epoch 1, Batch 100, Char 69 Loss: 2.1343183517456055\n",
      "Epoch 1, Batch 100, Char 70 Loss: 1.6609158515930176\n",
      "Epoch 1, Batch 100, Char 71 Loss: 4.34223747253418\n",
      "Epoch 1, Batch 100, Char 72 Loss: 0.32466304302215576\n",
      "Epoch 1, Batch 100, Char 73 Loss: 2.180403232574463\n",
      "Epoch 1, Batch 100, Char 74 Loss: 3.132412910461426\n",
      "Epoch 1, Batch 100, Char 75 Loss: 3.961091995239258\n",
      "Epoch 1, Batch 100, Char 76 Loss: 1.5790834426879883\n",
      "Epoch 1, Batch 100, Char 77 Loss: 2.949647903442383\n",
      "Epoch 1, Batch 100, Char 78 Loss: 1.067933201789856\n",
      "Epoch 1, Batch 100, Char 79 Loss: 2.146821975708008\n",
      "Epoch 1, Batch 100, Char 80 Loss: 1.6338739395141602\n",
      "Epoch 1, Batch 100, Char 81 Loss: 1.6216719150543213\n",
      "Epoch 1, Batch 100, Char 82 Loss: 0.46384087204933167\n",
      "Epoch 1, Batch 100, Char 83 Loss: 2.5464253425598145\n",
      "Epoch 1, Batch 100, Char 84 Loss: 0.5855494737625122\n",
      "Epoch 1, Batch 100, Char 85 Loss: 1.0901424884796143\n",
      "Epoch 1, Batch 100, Char 86 Loss: 2.701179265975952\n",
      "Epoch 1, Batch 100, Char 87 Loss: 2.887299060821533\n",
      "Epoch 1, Batch 100, Char 88 Loss: 5.819308757781982\n",
      "Epoch 1, Batch 100, Char 89 Loss: 3.1546597480773926\n",
      "Epoch 1, Batch 100, Char 90 Loss: 2.52290415763855\n",
      "Epoch 1, Batch 100, Char 91 Loss: 3.435102701187134\n",
      "Epoch 1, Batch 100, Char 92 Loss: 3.6131224632263184\n",
      "Epoch 1, Batch 100, Char 93 Loss: 3.097355842590332\n",
      "Epoch 1, Batch 100, Char 94 Loss: 3.1396284103393555\n",
      "Epoch 1, Batch 100, Char 95 Loss: 3.211233377456665\n",
      "Epoch 1, Batch 100, Char 96 Loss: 3.504737615585327\n",
      "Epoch 1, Batch 100, Char 97 Loss: 0.9244579076766968\n",
      "Epoch 1, Batch 100, Char 98 Loss: 2.7640292644500732\n",
      "Epoch 1, Batch 200, Char 0 Loss: 2.531461715698242\n",
      "Epoch 1, Batch 200, Char 1 Loss: 11.104578971862793\n",
      "Epoch 1, Batch 200, Char 2 Loss: 0.20501713454723358\n",
      "Epoch 1, Batch 200, Char 3 Loss: 2.1637773513793945\n",
      "Epoch 1, Batch 200, Char 4 Loss: 1.2692509889602661\n",
      "Epoch 1, Batch 200, Char 5 Loss: 1.911634922027588\n",
      "Epoch 1, Batch 200, Char 6 Loss: 2.4168128967285156\n",
      "Epoch 1, Batch 200, Char 7 Loss: 1.4362901449203491\n",
      "Epoch 1, Batch 200, Char 8 Loss: 3.2025465965270996\n",
      "Epoch 1, Batch 200, Char 9 Loss: 1.9296492338180542\n",
      "Epoch 1, Batch 200, Char 10 Loss: 2.3939170837402344\n",
      "Epoch 1, Batch 200, Char 11 Loss: 2.586235523223877\n",
      "Epoch 1, Batch 200, Char 12 Loss: 1.6075531244277954\n",
      "Epoch 1, Batch 200, Char 13 Loss: 1.2747838497161865\n",
      "Epoch 1, Batch 200, Char 14 Loss: 1.9022653102874756\n",
      "Epoch 1, Batch 200, Char 15 Loss: 2.7367897033691406\n",
      "Epoch 1, Batch 200, Char 16 Loss: 2.4577503204345703\n",
      "Epoch 1, Batch 200, Char 17 Loss: 2.0263705253601074\n",
      "Epoch 1, Batch 200, Char 18 Loss: 2.344174385070801\n",
      "Epoch 1, Batch 200, Char 19 Loss: 2.5406389236450195\n",
      "Epoch 1, Batch 200, Char 20 Loss: 1.5849395990371704\n",
      "Epoch 1, Batch 200, Char 21 Loss: 1.252526879310608\n",
      "Epoch 1, Batch 200, Char 22 Loss: 4.467683792114258\n",
      "Epoch 1, Batch 200, Char 23 Loss: 2.4489831924438477\n",
      "Epoch 1, Batch 200, Char 24 Loss: 1.7301522493362427\n",
      "Epoch 1, Batch 200, Char 25 Loss: 3.537703037261963\n",
      "Epoch 1, Batch 200, Char 26 Loss: 1.8862041234970093\n",
      "Epoch 1, Batch 200, Char 27 Loss: 2.4937539100646973\n",
      "Epoch 1, Batch 200, Char 28 Loss: 1.5677111148834229\n",
      "Epoch 1, Batch 200, Char 29 Loss: 1.226480484008789\n",
      "Epoch 1, Batch 200, Char 30 Loss: 2.6973564624786377\n",
      "Epoch 1, Batch 200, Char 31 Loss: 1.462382197380066\n",
      "Epoch 1, Batch 200, Char 32 Loss: 1.5936362743377686\n",
      "Epoch 1, Batch 200, Char 33 Loss: 2.147257089614868\n",
      "Epoch 1, Batch 200, Char 34 Loss: 2.0207998752593994\n",
      "Epoch 1, Batch 200, Char 35 Loss: 3.2155251502990723\n",
      "Epoch 1, Batch 200, Char 36 Loss: 3.0282514095306396\n",
      "Epoch 1, Batch 200, Char 37 Loss: 1.5678890943527222\n",
      "Epoch 1, Batch 200, Char 38 Loss: 2.667717933654785\n",
      "Epoch 1, Batch 200, Char 39 Loss: 2.2353687286376953\n",
      "Epoch 1, Batch 200, Char 40 Loss: 1.401082158088684\n",
      "Epoch 1, Batch 200, Char 41 Loss: 2.9641640186309814\n",
      "Epoch 1, Batch 200, Char 42 Loss: 3.030285120010376\n",
      "Epoch 1, Batch 200, Char 43 Loss: 1.5389715433120728\n",
      "Epoch 1, Batch 200, Char 44 Loss: 2.5910158157348633\n",
      "Epoch 1, Batch 200, Char 45 Loss: 4.119831562042236\n",
      "Epoch 1, Batch 200, Char 46 Loss: 3.437194585800171\n",
      "Epoch 1, Batch 200, Char 47 Loss: 0.7746257781982422\n",
      "Epoch 1, Batch 200, Char 48 Loss: 2.9269261360168457\n",
      "Epoch 1, Batch 200, Char 49 Loss: 1.3933391571044922\n",
      "Epoch 1, Batch 200, Char 50 Loss: 0.20716798305511475\n",
      "Epoch 1, Batch 200, Char 51 Loss: 2.8506991863250732\n",
      "Epoch 1, Batch 200, Char 52 Loss: 0.7473965883255005\n",
      "Epoch 1, Batch 200, Char 53 Loss: 2.5569403171539307\n",
      "Epoch 1, Batch 200, Char 54 Loss: 2.430360794067383\n",
      "Epoch 1, Batch 200, Char 55 Loss: 3.392547607421875\n",
      "Epoch 1, Batch 200, Char 56 Loss: 1.368007779121399\n",
      "Epoch 1, Batch 200, Char 57 Loss: 2.123971462249756\n",
      "Epoch 1, Batch 200, Char 58 Loss: 1.9886095523834229\n",
      "Epoch 1, Batch 200, Char 59 Loss: 2.0438480377197266\n",
      "Epoch 1, Batch 200, Char 60 Loss: 2.6430768966674805\n",
      "Epoch 1, Batch 200, Char 61 Loss: 2.2693090438842773\n",
      "Epoch 1, Batch 200, Char 62 Loss: 3.9443771839141846\n",
      "Epoch 1, Batch 200, Char 63 Loss: 1.17937171459198\n",
      "Epoch 1, Batch 200, Char 64 Loss: 2.8225257396698\n",
      "Epoch 1, Batch 200, Char 65 Loss: 2.558241844177246\n",
      "Epoch 1, Batch 200, Char 66 Loss: 3.1305978298187256\n",
      "Epoch 1, Batch 200, Char 67 Loss: 2.868795394897461\n",
      "Epoch 1, Batch 200, Char 68 Loss: 2.878263473510742\n",
      "Epoch 1, Batch 200, Char 69 Loss: 3.314560651779175\n",
      "Epoch 1, Batch 200, Char 70 Loss: 3.482112407684326\n",
      "Epoch 1, Batch 200, Char 71 Loss: 1.8193727731704712\n",
      "Epoch 1, Batch 200, Char 72 Loss: 2.133291721343994\n",
      "Epoch 1, Batch 200, Char 73 Loss: 2.6136133670806885\n",
      "Epoch 1, Batch 200, Char 74 Loss: 2.3951563835144043\n",
      "Epoch 1, Batch 200, Char 75 Loss: 1.9475958347320557\n",
      "Epoch 1, Batch 200, Char 76 Loss: 1.9572535753250122\n",
      "Epoch 1, Batch 200, Char 77 Loss: 1.5008783340454102\n",
      "Epoch 1, Batch 200, Char 78 Loss: 1.1553798913955688\n",
      "Epoch 1, Batch 200, Char 79 Loss: 2.584822416305542\n",
      "Epoch 1, Batch 200, Char 80 Loss: 2.7227025032043457\n",
      "Epoch 1, Batch 200, Char 81 Loss: 0.7473284006118774\n",
      "Epoch 1, Batch 200, Char 82 Loss: 1.1463592052459717\n",
      "Epoch 1, Batch 200, Char 83 Loss: 2.554356098175049\n",
      "Epoch 1, Batch 200, Char 84 Loss: 2.21614933013916\n",
      "Epoch 1, Batch 200, Char 85 Loss: 3.918944835662842\n",
      "Epoch 1, Batch 200, Char 86 Loss: 4.094805717468262\n",
      "Epoch 1, Batch 200, Char 87 Loss: 0.7872708439826965\n",
      "Epoch 1, Batch 200, Char 88 Loss: 2.92983341217041\n",
      "Epoch 1, Batch 200, Char 89 Loss: 1.9274073839187622\n",
      "Epoch 1, Batch 200, Char 90 Loss: 0.20291341841220856\n",
      "Epoch 1, Batch 200, Char 91 Loss: 2.0916905403137207\n",
      "Epoch 1, Batch 200, Char 92 Loss: 1.3012187480926514\n",
      "Epoch 1, Batch 200, Char 93 Loss: 1.9237961769104004\n",
      "Epoch 1, Batch 200, Char 94 Loss: 2.3155922889709473\n",
      "Epoch 1, Batch 200, Char 95 Loss: 1.3543250560760498\n",
      "Epoch 1, Batch 200, Char 96 Loss: 3.7041690349578857\n",
      "Epoch 1, Batch 200, Char 97 Loss: 1.9896936416625977\n",
      "Epoch 1, Batch 200, Char 98 Loss: 1.9844874143600464\n",
      "Epoch 1, Batch 300, Char 0 Loss: 2.4270858764648438\n",
      "Epoch 1, Batch 300, Char 1 Loss: 1.376247525215149\n",
      "Epoch 1, Batch 300, Char 2 Loss: 2.0093698501586914\n",
      "Epoch 1, Batch 300, Char 3 Loss: 1.0565767288208008\n",
      "Epoch 1, Batch 300, Char 4 Loss: 0.8080908060073853\n",
      "Epoch 1, Batch 300, Char 5 Loss: 1.231449007987976\n",
      "Epoch 1, Batch 300, Char 6 Loss: 3.9845900535583496\n",
      "Epoch 1, Batch 300, Char 7 Loss: 2.409821033477783\n",
      "Epoch 1, Batch 300, Char 8 Loss: 2.451146125793457\n",
      "Epoch 1, Batch 300, Char 9 Loss: 2.433784246444702\n",
      "Epoch 1, Batch 300, Char 10 Loss: 3.284044027328491\n",
      "Epoch 1, Batch 300, Char 11 Loss: 1.0787031650543213\n",
      "Epoch 1, Batch 300, Char 12 Loss: 1.9663771390914917\n",
      "Epoch 1, Batch 300, Char 13 Loss: 2.6538023948669434\n",
      "Epoch 1, Batch 300, Char 14 Loss: 2.2563092708587646\n",
      "Epoch 1, Batch 300, Char 15 Loss: 3.502687931060791\n",
      "Epoch 1, Batch 300, Char 16 Loss: 1.5835540294647217\n",
      "Epoch 1, Batch 300, Char 17 Loss: 3.0181217193603516\n",
      "Epoch 1, Batch 300, Char 18 Loss: 1.4662550687789917\n",
      "Epoch 1, Batch 300, Char 19 Loss: 1.3349952697753906\n",
      "Epoch 1, Batch 300, Char 20 Loss: 2.894644260406494\n",
      "Epoch 1, Batch 300, Char 21 Loss: 3.2074527740478516\n",
      "Epoch 1, Batch 300, Char 22 Loss: 0.23747272789478302\n",
      "Epoch 1, Batch 300, Char 23 Loss: 1.8099944591522217\n",
      "Epoch 1, Batch 300, Char 24 Loss: 1.423429250717163\n",
      "Epoch 1, Batch 300, Char 25 Loss: 1.93303644657135\n",
      "Epoch 1, Batch 300, Char 26 Loss: 1.0406639575958252\n",
      "Epoch 1, Batch 300, Char 27 Loss: 0.7793883085250854\n",
      "Epoch 1, Batch 300, Char 28 Loss: 1.1954572200775146\n",
      "Epoch 1, Batch 300, Char 29 Loss: 2.170487403869629\n",
      "Epoch 1, Batch 300, Char 30 Loss: 2.2919600009918213\n",
      "Epoch 1, Batch 300, Char 31 Loss: 1.6340824365615845\n",
      "Epoch 1, Batch 300, Char 32 Loss: 2.3713181018829346\n",
      "Epoch 1, Batch 300, Char 33 Loss: 3.768770694732666\n",
      "Epoch 1, Batch 300, Char 34 Loss: 2.9789319038391113\n",
      "Epoch 1, Batch 300, Char 35 Loss: 2.857113838195801\n",
      "Epoch 1, Batch 300, Char 36 Loss: 1.1257667541503906\n",
      "Epoch 1, Batch 300, Char 37 Loss: 1.2864863872528076\n",
      "Epoch 1, Batch 300, Char 38 Loss: 1.908774733543396\n",
      "Epoch 1, Batch 300, Char 39 Loss: 1.0247808694839478\n",
      "Epoch 1, Batch 300, Char 40 Loss: 0.7608850598335266\n",
      "Epoch 1, Batch 300, Char 41 Loss: 1.181545615196228\n",
      "Epoch 1, Batch 300, Char 42 Loss: 2.682314872741699\n",
      "Epoch 1, Batch 300, Char 43 Loss: 4.628600597381592\n",
      "Epoch 1, Batch 300, Char 44 Loss: 1.4727109670639038\n",
      "Epoch 1, Batch 300, Char 45 Loss: 2.361586809158325\n",
      "Epoch 1, Batch 300, Char 46 Loss: 2.3474979400634766\n",
      "Epoch 1, Batch 300, Char 47 Loss: 1.8902060985565186\n",
      "Epoch 1, Batch 300, Char 48 Loss: 0.9934055805206299\n",
      "Epoch 1, Batch 300, Char 49 Loss: 1.5342037677764893\n",
      "Epoch 1, Batch 300, Char 50 Loss: 2.131798267364502\n",
      "Epoch 1, Batch 300, Char 51 Loss: 1.40713369846344\n",
      "Epoch 1, Batch 300, Char 52 Loss: 3.04982328414917\n",
      "Epoch 1, Batch 300, Char 53 Loss: 1.9530725479125977\n",
      "Epoch 1, Batch 300, Char 54 Loss: 3.0155959129333496\n",
      "Epoch 1, Batch 300, Char 55 Loss: 1.9024310111999512\n",
      "Epoch 1, Batch 300, Char 56 Loss: 2.5005550384521484\n",
      "Epoch 1, Batch 300, Char 57 Loss: 3.2709896564483643\n",
      "Epoch 1, Batch 300, Char 58 Loss: 1.3367937803268433\n",
      "Epoch 1, Batch 300, Char 59 Loss: 2.105806350708008\n",
      "Epoch 1, Batch 300, Char 60 Loss: 0.36815664172172546\n",
      "Epoch 1, Batch 300, Char 61 Loss: 1.8523293733596802\n",
      "Epoch 1, Batch 300, Char 62 Loss: 0.9623810052871704\n",
      "Epoch 1, Batch 300, Char 63 Loss: 0.7824718952178955\n",
      "Epoch 1, Batch 300, Char 64 Loss: 1.151026725769043\n",
      "Epoch 1, Batch 300, Char 65 Loss: 2.863032341003418\n",
      "Epoch 1, Batch 300, Char 66 Loss: 2.356519937515259\n",
      "Epoch 1, Batch 300, Char 67 Loss: 3.2806925773620605\n",
      "Epoch 1, Batch 300, Char 68 Loss: 4.040432453155518\n",
      "Epoch 1, Batch 300, Char 69 Loss: 2.553532600402832\n",
      "Epoch 1, Batch 300, Char 70 Loss: 4.359593868255615\n",
      "Epoch 1, Batch 300, Char 71 Loss: 3.8613319396972656\n",
      "Epoch 1, Batch 300, Char 72 Loss: 2.185086965560913\n",
      "Epoch 1, Batch 300, Char 73 Loss: 3.8073906898498535\n",
      "Epoch 1, Batch 300, Char 74 Loss: 1.4168862104415894\n",
      "Epoch 1, Batch 300, Char 75 Loss: 1.821816325187683\n",
      "Epoch 1, Batch 300, Char 76 Loss: 0.9362291693687439\n",
      "Epoch 1, Batch 300, Char 77 Loss: 0.7779366374015808\n",
      "Epoch 1, Batch 300, Char 78 Loss: 1.140504240989685\n",
      "Epoch 1, Batch 300, Char 79 Loss: 3.014261484146118\n",
      "Epoch 1, Batch 300, Char 80 Loss: 2.756147861480713\n",
      "Epoch 1, Batch 300, Char 81 Loss: 4.014549255371094\n",
      "Epoch 1, Batch 300, Char 82 Loss: 7.774761199951172\n",
      "Epoch 1, Batch 300, Char 83 Loss: 2.3293752670288086\n",
      "Epoch 1, Batch 300, Char 84 Loss: 2.38765549659729\n",
      "Epoch 1, Batch 300, Char 85 Loss: 1.099752426147461\n",
      "Epoch 1, Batch 300, Char 86 Loss: 2.283689022064209\n",
      "Epoch 1, Batch 300, Char 87 Loss: 0.9765670299530029\n",
      "Epoch 1, Batch 300, Char 88 Loss: 2.661609411239624\n",
      "Epoch 1, Batch 300, Char 89 Loss: 3.4182395935058594\n",
      "Epoch 1, Batch 300, Char 90 Loss: 2.2908782958984375\n",
      "Epoch 1, Batch 300, Char 91 Loss: 2.2388834953308105\n",
      "Epoch 1, Batch 300, Char 92 Loss: 0.9612685441970825\n",
      "Epoch 1, Batch 300, Char 93 Loss: 2.896770477294922\n",
      "Epoch 1, Batch 300, Char 94 Loss: 2.1314713954925537\n",
      "Epoch 1, Batch 300, Char 95 Loss: 4.5387372970581055\n",
      "Epoch 1, Batch 300, Char 96 Loss: 0.35813620686531067\n",
      "Epoch 1, Batch 300, Char 97 Loss: 3.2959418296813965\n",
      "Epoch 1, Batch 300, Char 98 Loss: 2.3452982902526855\n",
      "Epoch 1, Batch 400, Char 0 Loss: 2.893080234527588\n",
      "Epoch 1, Batch 400, Char 1 Loss: 2.1694159507751465\n",
      "Epoch 1, Batch 400, Char 2 Loss: 1.594762921333313\n",
      "Epoch 1, Batch 400, Char 3 Loss: 5.898517608642578\n",
      "Epoch 1, Batch 400, Char 4 Loss: 2.8177103996276855\n",
      "Epoch 1, Batch 400, Char 5 Loss: 2.6576170921325684\n",
      "Epoch 1, Batch 400, Char 6 Loss: 0.6905282139778137\n",
      "Epoch 1, Batch 400, Char 7 Loss: 1.9579627513885498\n",
      "Epoch 1, Batch 400, Char 8 Loss: 1.2099472284317017\n",
      "Epoch 1, Batch 400, Char 9 Loss: 2.4652442932128906\n",
      "Epoch 1, Batch 400, Char 10 Loss: 2.1674089431762695\n",
      "Epoch 1, Batch 400, Char 11 Loss: 3.4840757846832275\n",
      "Epoch 1, Batch 400, Char 12 Loss: 2.3459577560424805\n",
      "Epoch 1, Batch 400, Char 13 Loss: 1.559984803199768\n",
      "Epoch 1, Batch 400, Char 14 Loss: 1.6849037408828735\n",
      "Epoch 1, Batch 400, Char 15 Loss: 2.635288715362549\n",
      "Epoch 1, Batch 400, Char 16 Loss: 0.8899449706077576\n",
      "Epoch 1, Batch 400, Char 17 Loss: 3.963853120803833\n",
      "Epoch 1, Batch 400, Char 18 Loss: 5.356659412384033\n",
      "Epoch 1, Batch 400, Char 19 Loss: 1.7283010482788086\n",
      "Epoch 1, Batch 400, Char 20 Loss: 1.6073848009109497\n",
      "Epoch 1, Batch 400, Char 21 Loss: 1.9153783321380615\n",
      "Epoch 1, Batch 400, Char 22 Loss: 2.7050023078918457\n",
      "Epoch 1, Batch 400, Char 23 Loss: 4.218304634094238\n",
      "Epoch 1, Batch 400, Char 24 Loss: 2.8252933025360107\n",
      "Epoch 1, Batch 400, Char 25 Loss: 3.515740156173706\n",
      "Epoch 1, Batch 400, Char 26 Loss: 1.8932454586029053\n",
      "Epoch 1, Batch 400, Char 27 Loss: 2.5346076488494873\n",
      "Epoch 1, Batch 400, Char 28 Loss: 0.859532356262207\n",
      "Epoch 1, Batch 400, Char 29 Loss: 2.1899776458740234\n",
      "Epoch 1, Batch 400, Char 30 Loss: 1.519856572151184\n",
      "Epoch 1, Batch 400, Char 31 Loss: 1.803279995918274\n",
      "Epoch 1, Batch 400, Char 32 Loss: 0.38839036226272583\n",
      "Epoch 1, Batch 400, Char 33 Loss: 2.176384449005127\n",
      "Epoch 1, Batch 400, Char 34 Loss: 3.0341334342956543\n",
      "Epoch 1, Batch 400, Char 35 Loss: 8.449541091918945\n",
      "Epoch 1, Batch 400, Char 36 Loss: 0.31700360774993896\n",
      "Epoch 1, Batch 400, Char 37 Loss: 2.7564427852630615\n",
      "Epoch 1, Batch 400, Char 38 Loss: 2.4456381797790527\n",
      "Epoch 1, Batch 400, Char 39 Loss: 4.511174201965332\n",
      "Epoch 1, Batch 400, Char 40 Loss: 2.144620418548584\n",
      "Epoch 1, Batch 400, Char 41 Loss: 1.6614984273910522\n",
      "Epoch 1, Batch 400, Char 42 Loss: 2.491150379180908\n",
      "Epoch 1, Batch 400, Char 43 Loss: 0.8619847297668457\n",
      "Epoch 1, Batch 400, Char 44 Loss: 2.7317357063293457\n",
      "Epoch 1, Batch 400, Char 45 Loss: 0.9100074172019958\n",
      "Epoch 1, Batch 400, Char 46 Loss: 1.38550865650177\n",
      "Epoch 1, Batch 400, Char 47 Loss: 2.7167396545410156\n",
      "Epoch 1, Batch 400, Char 48 Loss: 0.900969922542572\n",
      "Epoch 1, Batch 400, Char 49 Loss: 1.747422218322754\n",
      "Epoch 1, Batch 400, Char 50 Loss: 2.7585039138793945\n",
      "Epoch 1, Batch 400, Char 51 Loss: 4.997829437255859\n",
      "Epoch 1, Batch 400, Char 52 Loss: 1.4583121538162231\n",
      "Epoch 1, Batch 400, Char 53 Loss: 1.3798880577087402\n",
      "Epoch 1, Batch 400, Char 54 Loss: 3.461545944213867\n",
      "Epoch 1, Batch 400, Char 55 Loss: 2.4131312370300293\n",
      "Epoch 1, Batch 400, Char 56 Loss: 1.4384276866912842\n",
      "Epoch 1, Batch 400, Char 57 Loss: 1.7182248830795288\n",
      "Epoch 1, Batch 400, Char 58 Loss: 0.41235655546188354\n",
      "Epoch 1, Batch 400, Char 59 Loss: 2.854942798614502\n",
      "Epoch 1, Batch 400, Char 60 Loss: 2.339381694793701\n",
      "Epoch 1, Batch 400, Char 61 Loss: 0.2472684532403946\n",
      "Epoch 1, Batch 400, Char 62 Loss: 2.1068115234375\n",
      "Epoch 1, Batch 400, Char 63 Loss: 1.3963202238082886\n",
      "Epoch 1, Batch 400, Char 64 Loss: 6.374536514282227\n",
      "Epoch 1, Batch 400, Char 65 Loss: 3.5909197330474854\n",
      "Epoch 1, Batch 400, Char 66 Loss: 4.07939338684082\n",
      "Epoch 1, Batch 400, Char 67 Loss: 0.9796014428138733\n",
      "Epoch 1, Batch 400, Char 68 Loss: 2.810011625289917\n",
      "Epoch 1, Batch 400, Char 69 Loss: 2.327409267425537\n",
      "Epoch 1, Batch 400, Char 70 Loss: 2.2926859855651855\n",
      "Epoch 1, Batch 400, Char 71 Loss: 3.842412233352661\n",
      "Epoch 1, Batch 400, Char 72 Loss: 2.3076956272125244\n",
      "Epoch 1, Batch 400, Char 73 Loss: 2.4572598934173584\n",
      "Epoch 1, Batch 400, Char 74 Loss: 0.845539927482605\n",
      "Epoch 1, Batch 400, Char 75 Loss: 2.6099886894226074\n",
      "Epoch 1, Batch 400, Char 76 Loss: 2.7805402278900146\n",
      "Epoch 1, Batch 400, Char 77 Loss: 2.160598039627075\n",
      "Epoch 1, Batch 400, Char 78 Loss: 3.6818461418151855\n",
      "Epoch 1, Batch 400, Char 79 Loss: 1.763884425163269\n",
      "Epoch 1, Batch 400, Char 80 Loss: 2.174692153930664\n",
      "Epoch 1, Batch 400, Char 81 Loss: 3.3325557708740234\n",
      "Epoch 1, Batch 400, Char 82 Loss: 2.2401974201202393\n",
      "Epoch 1, Batch 400, Char 83 Loss: 2.242232084274292\n",
      "Epoch 1, Batch 400, Char 84 Loss: 2.3982467651367188\n",
      "Epoch 1, Batch 400, Char 85 Loss: 2.6026620864868164\n",
      "Epoch 1, Batch 400, Char 86 Loss: 0.41790109872817993\n",
      "Epoch 1, Batch 400, Char 87 Loss: 2.7239623069763184\n",
      "Epoch 1, Batch 400, Char 88 Loss: 2.2274274826049805\n",
      "Epoch 1, Batch 400, Char 89 Loss: 0.2406439483165741\n",
      "Epoch 1, Batch 400, Char 90 Loss: 2.7470340728759766\n",
      "Epoch 1, Batch 400, Char 91 Loss: 2.8546977043151855\n",
      "Epoch 1, Batch 400, Char 92 Loss: 2.189058542251587\n",
      "Epoch 1, Batch 400, Char 93 Loss: 1.3359289169311523\n",
      "Epoch 1, Batch 400, Char 94 Loss: 4.135271072387695\n",
      "Epoch 1, Batch 400, Char 95 Loss: 1.0351508855819702\n",
      "Epoch 1, Batch 400, Char 96 Loss: 2.9390976428985596\n",
      "Epoch 1, Batch 400, Char 97 Loss: 1.4108779430389404\n",
      "Epoch 1, Batch 400, Char 98 Loss: 1.8637490272521973\n",
      "Epoch 1, Batch 500, Char 0 Loss: 1.9761285781860352\n",
      "Epoch 1, Batch 500, Char 1 Loss: 1.3813698291778564\n",
      "Epoch 1, Batch 500, Char 2 Loss: 2.8305678367614746\n",
      "Epoch 1, Batch 500, Char 3 Loss: 3.554903030395508\n",
      "Epoch 1, Batch 500, Char 4 Loss: 4.079359531402588\n",
      "Epoch 1, Batch 500, Char 5 Loss: 1.4020174741744995\n",
      "Epoch 1, Batch 500, Char 6 Loss: 4.089133262634277\n",
      "Epoch 1, Batch 500, Char 7 Loss: 2.6145355701446533\n",
      "Epoch 1, Batch 500, Char 8 Loss: 3.8387913703918457\n",
      "Epoch 1, Batch 500, Char 9 Loss: 2.0894975662231445\n",
      "Epoch 1, Batch 500, Char 10 Loss: 4.3593525886535645\n",
      "Epoch 1, Batch 500, Char 11 Loss: 3.273507833480835\n",
      "Epoch 1, Batch 500, Char 12 Loss: 4.389357566833496\n",
      "Epoch 1, Batch 500, Char 13 Loss: 2.5625810623168945\n",
      "Epoch 1, Batch 500, Char 14 Loss: 1.822555422782898\n",
      "Epoch 1, Batch 500, Char 15 Loss: 3.066892147064209\n",
      "Epoch 1, Batch 500, Char 16 Loss: 1.559959888458252\n",
      "Epoch 1, Batch 500, Char 17 Loss: 1.89394211769104\n",
      "Epoch 1, Batch 500, Char 18 Loss: 3.5451273918151855\n",
      "Epoch 1, Batch 500, Char 19 Loss: 3.5575649738311768\n",
      "Epoch 1, Batch 500, Char 20 Loss: 1.3439805507659912\n",
      "Epoch 1, Batch 500, Char 21 Loss: 3.2391979694366455\n",
      "Epoch 1, Batch 500, Char 22 Loss: 2.1804442405700684\n",
      "Epoch 1, Batch 500, Char 23 Loss: 0.16465702652931213\n",
      "Epoch 1, Batch 500, Char 24 Loss: 1.913822054862976\n",
      "Epoch 1, Batch 500, Char 25 Loss: 1.0276967287063599\n",
      "Epoch 1, Batch 500, Char 26 Loss: 0.6956738233566284\n",
      "Epoch 1, Batch 500, Char 27 Loss: 0.9231278300285339\n",
      "Epoch 1, Batch 500, Char 28 Loss: 2.617807388305664\n",
      "Epoch 1, Batch 500, Char 29 Loss: 1.8310496807098389\n",
      "Epoch 1, Batch 500, Char 30 Loss: 1.384737253189087\n",
      "Epoch 1, Batch 500, Char 31 Loss: 1.4860477447509766\n",
      "Epoch 1, Batch 500, Char 32 Loss: 3.3047471046447754\n",
      "Epoch 1, Batch 500, Char 33 Loss: 3.419685125350952\n",
      "Epoch 1, Batch 500, Char 34 Loss: 2.43853759765625\n",
      "Epoch 1, Batch 500, Char 35 Loss: 2.0254287719726562\n",
      "Epoch 1, Batch 500, Char 36 Loss: 1.7993950843811035\n",
      "Epoch 1, Batch 500, Char 37 Loss: 1.4613926410675049\n",
      "Epoch 1, Batch 500, Char 38 Loss: 0.37302830815315247\n",
      "Epoch 1, Batch 500, Char 39 Loss: 3.367234468460083\n",
      "Epoch 1, Batch 500, Char 40 Loss: 2.5602874755859375\n",
      "Epoch 1, Batch 500, Char 41 Loss: 1.4647777080535889\n",
      "Epoch 1, Batch 500, Char 42 Loss: 4.21187686920166\n",
      "Epoch 1, Batch 500, Char 43 Loss: 3.1417784690856934\n",
      "Epoch 1, Batch 500, Char 44 Loss: 3.2131786346435547\n",
      "Epoch 1, Batch 500, Char 45 Loss: 3.5153579711914062\n",
      "Epoch 1, Batch 500, Char 46 Loss: 3.1438674926757812\n",
      "Epoch 1, Batch 500, Char 47 Loss: 2.315145969390869\n",
      "Epoch 1, Batch 500, Char 48 Loss: 2.5430588722229004\n",
      "Epoch 1, Batch 500, Char 49 Loss: 1.1738369464874268\n",
      "Epoch 1, Batch 500, Char 50 Loss: 2.173340082168579\n",
      "Epoch 1, Batch 500, Char 51 Loss: 0.9918439388275146\n",
      "Epoch 1, Batch 500, Char 52 Loss: 2.8259100914001465\n",
      "Epoch 1, Batch 500, Char 53 Loss: 2.252675771713257\n",
      "Epoch 1, Batch 500, Char 54 Loss: 1.3877403736114502\n",
      "Epoch 1, Batch 500, Char 55 Loss: 2.9543652534484863\n",
      "Epoch 1, Batch 500, Char 56 Loss: 0.6956211924552917\n",
      "Epoch 1, Batch 500, Char 57 Loss: 1.942747712135315\n",
      "Epoch 1, Batch 500, Char 58 Loss: 1.3103528022766113\n",
      "Epoch 1, Batch 500, Char 59 Loss: 3.855330467224121\n",
      "Epoch 1, Batch 500, Char 60 Loss: 2.4434897899627686\n",
      "Epoch 1, Batch 500, Char 61 Loss: 3.4991044998168945\n",
      "Epoch 1, Batch 500, Char 62 Loss: 2.14982271194458\n",
      "Epoch 1, Batch 500, Char 63 Loss: 2.005023956298828\n",
      "Epoch 1, Batch 500, Char 64 Loss: 1.7628406286239624\n",
      "Epoch 1, Batch 500, Char 65 Loss: 1.3527626991271973\n",
      "Epoch 1, Batch 500, Char 66 Loss: 4.039806365966797\n",
      "Epoch 1, Batch 500, Char 67 Loss: 3.5140578746795654\n",
      "Epoch 1, Batch 500, Char 68 Loss: 1.9959007501602173\n",
      "Epoch 1, Batch 500, Char 69 Loss: 4.115424633026123\n",
      "Epoch 1, Batch 500, Char 70 Loss: 3.104860305786133\n",
      "Epoch 1, Batch 500, Char 71 Loss: 4.3882598876953125\n",
      "Epoch 1, Batch 500, Char 72 Loss: 2.864281177520752\n",
      "Epoch 1, Batch 500, Char 73 Loss: 3.182056427001953\n",
      "Epoch 1, Batch 500, Char 74 Loss: 8.242986679077148\n",
      "Epoch 1, Batch 500, Char 75 Loss: 3.184492588043213\n",
      "Epoch 1, Batch 500, Char 76 Loss: 2.3877458572387695\n",
      "Epoch 1, Batch 500, Char 77 Loss: 3.661529064178467\n",
      "Epoch 1, Batch 500, Char 78 Loss: 4.801497459411621\n",
      "Epoch 1, Batch 500, Char 79 Loss: 1.1216970682144165\n",
      "Epoch 1, Batch 500, Char 80 Loss: 8.123741149902344\n",
      "Epoch 1, Batch 500, Char 81 Loss: 0.016396192833781242\n",
      "Epoch 1, Batch 500, Char 82 Loss: 5.133863925933838\n",
      "Epoch 1, Batch 500, Char 83 Loss: 4.131220817565918\n",
      "Epoch 1, Batch 500, Char 84 Loss: 0.2493990659713745\n",
      "Epoch 1, Batch 500, Char 85 Loss: 1.8973522186279297\n",
      "Epoch 1, Batch 500, Char 86 Loss: 1.4474717378616333\n",
      "Epoch 1, Batch 500, Char 87 Loss: 2.396782398223877\n",
      "Epoch 1, Batch 500, Char 88 Loss: 0.36933913826942444\n",
      "Epoch 1, Batch 500, Char 89 Loss: 1.977033019065857\n",
      "Epoch 1, Batch 500, Char 90 Loss: 1.0399317741394043\n",
      "Epoch 1, Batch 500, Char 91 Loss: 4.37885856628418\n",
      "Epoch 1, Batch 500, Char 92 Loss: 2.241522789001465\n",
      "Epoch 1, Batch 500, Char 93 Loss: 1.6413050889968872\n",
      "Epoch 1, Batch 500, Char 94 Loss: 3.769772529602051\n",
      "Epoch 1, Batch 500, Char 95 Loss: 1.5193802118301392\n",
      "Epoch 1, Batch 500, Char 96 Loss: 2.320277452468872\n",
      "Epoch 1, Batch 500, Char 97 Loss: 2.979677438735962\n",
      "Epoch 1, Batch 500, Char 98 Loss: 1.6269413232803345\n",
      "Epoch 1, Batch 600, Char 0 Loss: 0.4319364130496979\n",
      "Epoch 1, Batch 600, Char 1 Loss: 1.4913315773010254\n",
      "Epoch 1, Batch 600, Char 2 Loss: 2.0324065685272217\n",
      "Epoch 1, Batch 600, Char 3 Loss: 1.4896756410598755\n",
      "Epoch 1, Batch 600, Char 4 Loss: 3.64717435836792\n",
      "Epoch 1, Batch 600, Char 5 Loss: 2.507246494293213\n",
      "Epoch 1, Batch 600, Char 6 Loss: 2.9655072689056396\n",
      "Epoch 1, Batch 600, Char 7 Loss: 4.8506317138671875\n",
      "Epoch 1, Batch 600, Char 8 Loss: 1.3380744457244873\n",
      "Epoch 1, Batch 600, Char 9 Loss: 1.9800400733947754\n",
      "Epoch 1, Batch 600, Char 10 Loss: 2.038985252380371\n",
      "Epoch 1, Batch 600, Char 11 Loss: 1.472378134727478\n",
      "Epoch 1, Batch 600, Char 12 Loss: 2.8201494216918945\n",
      "Epoch 1, Batch 600, Char 13 Loss: 0.6236321330070496\n",
      "Epoch 1, Batch 600, Char 14 Loss: 1.8328559398651123\n",
      "Epoch 1, Batch 600, Char 15 Loss: 1.2893792390823364\n",
      "Epoch 1, Batch 600, Char 16 Loss: 3.4702157974243164\n",
      "Epoch 1, Batch 600, Char 17 Loss: 2.5768134593963623\n",
      "Epoch 1, Batch 600, Char 18 Loss: 3.1483778953552246\n",
      "Epoch 1, Batch 600, Char 19 Loss: 2.3056657314300537\n",
      "Epoch 1, Batch 600, Char 20 Loss: 2.8266472816467285\n",
      "Epoch 1, Batch 600, Char 21 Loss: 2.2898526191711426\n",
      "Epoch 1, Batch 600, Char 22 Loss: 3.6353979110717773\n",
      "Epoch 1, Batch 600, Char 23 Loss: 2.024453639984131\n",
      "Epoch 1, Batch 600, Char 24 Loss: 2.5918996334075928\n",
      "Epoch 1, Batch 600, Char 25 Loss: 0.23322364687919617\n",
      "Epoch 1, Batch 600, Char 26 Loss: 2.561178684234619\n",
      "Epoch 1, Batch 600, Char 27 Loss: 2.1859045028686523\n",
      "Epoch 1, Batch 600, Char 28 Loss: 0.8736295700073242\n",
      "Epoch 1, Batch 600, Char 29 Loss: 2.776555061340332\n",
      "Epoch 1, Batch 600, Char 30 Loss: 3.1795661449432373\n",
      "Epoch 1, Batch 600, Char 31 Loss: 0.6169214844703674\n",
      "Epoch 1, Batch 600, Char 32 Loss: 1.066940426826477\n",
      "Epoch 1, Batch 600, Char 33 Loss: 2.0820860862731934\n",
      "Epoch 1, Batch 600, Char 34 Loss: 1.162164330482483\n",
      "Epoch 1, Batch 600, Char 35 Loss: 0.6112250685691833\n",
      "Epoch 1, Batch 600, Char 36 Loss: 1.0590293407440186\n",
      "Epoch 1, Batch 600, Char 37 Loss: 2.737907886505127\n",
      "Epoch 1, Batch 600, Char 38 Loss: 8.181679725646973\n",
      "Epoch 1, Batch 600, Char 39 Loss: 1.74575936794281\n",
      "Epoch 1, Batch 600, Char 40 Loss: 3.1213178634643555\n",
      "Epoch 1, Batch 600, Char 41 Loss: 3.652106761932373\n",
      "Epoch 1, Batch 600, Char 42 Loss: 2.2502939701080322\n",
      "Epoch 1, Batch 600, Char 43 Loss: 2.545928955078125\n",
      "Epoch 1, Batch 600, Char 44 Loss: 1.9968459606170654\n",
      "Epoch 1, Batch 600, Char 45 Loss: 1.431909203529358\n",
      "Epoch 1, Batch 600, Char 46 Loss: 2.055018901824951\n",
      "Epoch 1, Batch 600, Char 47 Loss: 1.143295168876648\n",
      "Epoch 1, Batch 600, Char 48 Loss: 2.252017021179199\n",
      "Epoch 1, Batch 600, Char 49 Loss: 1.1281390190124512\n",
      "Epoch 1, Batch 600, Char 50 Loss: 1.6994507312774658\n",
      "Epoch 1, Batch 600, Char 51 Loss: 0.8679294586181641\n",
      "Epoch 1, Batch 600, Char 52 Loss: 4.251469612121582\n",
      "Epoch 1, Batch 600, Char 53 Loss: 2.1205265522003174\n",
      "Epoch 1, Batch 600, Char 54 Loss: 1.9092528820037842\n",
      "Epoch 1, Batch 600, Char 55 Loss: 3.8875656127929688\n",
      "Epoch 1, Batch 600, Char 56 Loss: 4.4646382331848145\n",
      "Epoch 1, Batch 600, Char 57 Loss: 4.1628947257995605\n",
      "Epoch 1, Batch 600, Char 58 Loss: 0.24619142711162567\n",
      "Epoch 1, Batch 600, Char 59 Loss: 1.8462005853652954\n",
      "Epoch 1, Batch 600, Char 60 Loss: 1.2518378496170044\n",
      "Epoch 1, Batch 600, Char 61 Loss: 2.6605639457702637\n",
      "Epoch 1, Batch 600, Char 62 Loss: 3.3099851608276367\n",
      "Epoch 1, Batch 600, Char 63 Loss: 4.953276634216309\n",
      "Epoch 1, Batch 600, Char 64 Loss: 2.2044625282287598\n",
      "Epoch 1, Batch 600, Char 65 Loss: 2.8011040687561035\n",
      "Epoch 1, Batch 600, Char 66 Loss: 2.198552131652832\n",
      "Epoch 1, Batch 600, Char 67 Loss: 3.513951301574707\n",
      "Epoch 1, Batch 600, Char 68 Loss: 1.9370485544204712\n",
      "Epoch 1, Batch 600, Char 69 Loss: 2.4687042236328125\n",
      "Epoch 1, Batch 600, Char 70 Loss: 0.2488587200641632\n",
      "Epoch 1, Batch 600, Char 71 Loss: 2.0141091346740723\n",
      "Epoch 1, Batch 600, Char 72 Loss: 1.134765386581421\n",
      "Epoch 1, Batch 600, Char 73 Loss: 1.9189008474349976\n",
      "Epoch 1, Batch 600, Char 74 Loss: 1.9634392261505127\n",
      "Epoch 1, Batch 600, Char 75 Loss: 1.4325838088989258\n",
      "Epoch 1, Batch 600, Char 76 Loss: 2.5748658180236816\n",
      "Epoch 1, Batch 600, Char 77 Loss: 2.0898075103759766\n",
      "Epoch 1, Batch 600, Char 78 Loss: 0.904314398765564\n",
      "Epoch 1, Batch 600, Char 79 Loss: 2.75749135017395\n",
      "Epoch 1, Batch 600, Char 80 Loss: 1.577375888824463\n",
      "Epoch 1, Batch 600, Char 81 Loss: 3.4606707096099854\n",
      "Epoch 1, Batch 600, Char 82 Loss: 2.1051788330078125\n",
      "Epoch 1, Batch 600, Char 83 Loss: 3.7313108444213867\n",
      "Epoch 1, Batch 600, Char 84 Loss: 5.4571661949157715\n",
      "Epoch 1, Batch 600, Char 85 Loss: 0.7729269862174988\n",
      "Epoch 1, Batch 600, Char 86 Loss: 2.610872983932495\n",
      "Epoch 1, Batch 600, Char 87 Loss: 2.82839298248291\n",
      "Epoch 1, Batch 600, Char 88 Loss: 2.484426736831665\n",
      "Epoch 1, Batch 600, Char 89 Loss: 3.330150604248047\n",
      "Epoch 1, Batch 600, Char 90 Loss: 2.7500877380371094\n",
      "Epoch 1, Batch 600, Char 91 Loss: 1.142425537109375\n",
      "Epoch 1, Batch 600, Char 92 Loss: 1.6403377056121826\n",
      "Epoch 1, Batch 600, Char 93 Loss: 0.837035059928894\n",
      "Epoch 1, Batch 600, Char 94 Loss: 3.0312538146972656\n",
      "Epoch 1, Batch 600, Char 95 Loss: 2.0097038745880127\n",
      "Epoch 1, Batch 600, Char 96 Loss: 1.703671932220459\n",
      "Epoch 1, Batch 600, Char 97 Loss: 1.9788730144500732\n",
      "Epoch 1, Batch 600, Char 98 Loss: 1.141463041305542\n",
      "Epoch 1, Batch 700, Char 0 Loss: 1.826187252998352\n",
      "Epoch 1, Batch 700, Char 1 Loss: 1.1018273830413818\n",
      "Epoch 1, Batch 700, Char 2 Loss: 2.7879080772399902\n",
      "Epoch 1, Batch 700, Char 3 Loss: 2.142484188079834\n",
      "Epoch 1, Batch 700, Char 4 Loss: 2.4458189010620117\n",
      "Epoch 1, Batch 700, Char 5 Loss: 0.8765613436698914\n",
      "Epoch 1, Batch 700, Char 6 Loss: 3.222959518432617\n",
      "Epoch 1, Batch 700, Char 7 Loss: 1.532240629196167\n",
      "Epoch 1, Batch 700, Char 8 Loss: 1.7508814334869385\n",
      "Epoch 1, Batch 700, Char 9 Loss: 2.057755708694458\n",
      "Epoch 1, Batch 700, Char 10 Loss: 2.3915393352508545\n",
      "Epoch 1, Batch 700, Char 11 Loss: 0.3839762508869171\n",
      "Epoch 1, Batch 700, Char 12 Loss: 2.815012216567993\n",
      "Epoch 1, Batch 700, Char 13 Loss: 2.2338037490844727\n",
      "Epoch 1, Batch 700, Char 14 Loss: 3.116511821746826\n",
      "Epoch 1, Batch 700, Char 15 Loss: 0.8656283617019653\n",
      "Epoch 1, Batch 700, Char 16 Loss: 2.7893407344818115\n",
      "Epoch 1, Batch 700, Char 17 Loss: 2.970707416534424\n",
      "Epoch 1, Batch 700, Char 18 Loss: 19.528099060058594\n",
      "Epoch 1, Batch 700, Char 19 Loss: 2.935642719268799\n",
      "Epoch 1, Batch 700, Char 20 Loss: 4.077085971832275\n",
      "Epoch 1, Batch 700, Char 21 Loss: 3.417958974838257\n",
      "Epoch 1, Batch 700, Char 22 Loss: 3.220909595489502\n",
      "Epoch 1, Batch 700, Char 23 Loss: 1.757351279258728\n",
      "Epoch 1, Batch 700, Char 24 Loss: 2.538059949874878\n",
      "Epoch 1, Batch 700, Char 25 Loss: 2.5833358764648438\n",
      "Epoch 1, Batch 700, Char 26 Loss: 1.4684953689575195\n",
      "Epoch 1, Batch 700, Char 27 Loss: 1.8059120178222656\n",
      "Epoch 1, Batch 700, Char 28 Loss: 0.878197431564331\n",
      "Epoch 1, Batch 700, Char 29 Loss: 2.5019690990448\n",
      "Epoch 1, Batch 700, Char 30 Loss: 2.5433597564697266\n",
      "Epoch 1, Batch 700, Char 31 Loss: 1.7901687622070312\n",
      "Epoch 1, Batch 700, Char 32 Loss: 4.552909851074219\n",
      "Epoch 1, Batch 700, Char 33 Loss: 1.032989263534546\n",
      "Epoch 1, Batch 700, Char 34 Loss: 3.6972196102142334\n",
      "Epoch 1, Batch 700, Char 35 Loss: 2.371969699859619\n",
      "Epoch 1, Batch 700, Char 36 Loss: 2.109009027481079\n",
      "Epoch 1, Batch 700, Char 37 Loss: 1.698622226715088\n",
      "Epoch 1, Batch 700, Char 38 Loss: 2.604032039642334\n",
      "Epoch 1, Batch 700, Char 39 Loss: 2.104541778564453\n",
      "Epoch 1, Batch 700, Char 40 Loss: 2.744990110397339\n",
      "Epoch 1, Batch 700, Char 41 Loss: 1.8822633028030396\n",
      "Epoch 1, Batch 700, Char 42 Loss: 0.7279943227767944\n",
      "Epoch 1, Batch 700, Char 43 Loss: 2.7283477783203125\n",
      "Epoch 1, Batch 700, Char 44 Loss: 1.6718567609786987\n",
      "Epoch 1, Batch 700, Char 45 Loss: 2.234349012374878\n",
      "Epoch 1, Batch 700, Char 46 Loss: 19.89210319519043\n",
      "Epoch 1, Batch 700, Char 47 Loss: 2.394648790359497\n",
      "Epoch 1, Batch 700, Char 48 Loss: 4.175344467163086\n",
      "Epoch 1, Batch 700, Char 49 Loss: 2.7226402759552\n",
      "Epoch 1, Batch 700, Char 50 Loss: 2.326504707336426\n",
      "Epoch 1, Batch 700, Char 51 Loss: 2.7863385677337646\n",
      "Epoch 1, Batch 700, Char 52 Loss: 1.355086326599121\n",
      "Epoch 1, Batch 700, Char 53 Loss: 1.7349830865859985\n",
      "Epoch 1, Batch 700, Char 54 Loss: 3.827786445617676\n",
      "Epoch 1, Batch 700, Char 55 Loss: 1.0052069425582886\n",
      "Epoch 1, Batch 700, Char 56 Loss: 4.351700782775879\n",
      "Epoch 1, Batch 700, Char 57 Loss: 1.5517126321792603\n",
      "Epoch 1, Batch 700, Char 58 Loss: 6.051179885864258\n",
      "Epoch 1, Batch 700, Char 59 Loss: 2.812483787536621\n",
      "Epoch 1, Batch 700, Char 60 Loss: 1.540428876876831\n",
      "Epoch 1, Batch 700, Char 61 Loss: 2.65706467628479\n",
      "Epoch 1, Batch 700, Char 62 Loss: 2.2048559188842773\n",
      "Epoch 1, Batch 700, Char 63 Loss: 2.6622605323791504\n",
      "Epoch 1, Batch 700, Char 64 Loss: 2.53905987739563\n",
      "Epoch 1, Batch 700, Char 65 Loss: 2.3292765617370605\n",
      "Epoch 1, Batch 700, Char 66 Loss: 1.3325073719024658\n",
      "Epoch 1, Batch 700, Char 67 Loss: 1.6808596849441528\n",
      "Epoch 1, Batch 700, Char 68 Loss: 1.069631576538086\n",
      "Epoch 1, Batch 700, Char 69 Loss: 2.6878786087036133\n",
      "Epoch 1, Batch 700, Char 70 Loss: 2.8358969688415527\n",
      "Epoch 1, Batch 700, Char 71 Loss: 19.23466682434082\n",
      "Epoch 1, Batch 700, Char 72 Loss: 2.711127758026123\n",
      "Epoch 1, Batch 700, Char 73 Loss: 3.1541223526000977\n",
      "Epoch 1, Batch 700, Char 74 Loss: 3.436861515045166\n",
      "Epoch 1, Batch 700, Char 75 Loss: 1.2988438606262207\n",
      "Epoch 1, Batch 700, Char 76 Loss: 2.4038920402526855\n",
      "Epoch 1, Batch 700, Char 77 Loss: 0.8885707855224609\n",
      "Epoch 1, Batch 700, Char 78 Loss: 3.9626855850219727\n",
      "Epoch 1, Batch 700, Char 79 Loss: 2.462129592895508\n",
      "Epoch 1, Batch 700, Char 80 Loss: 2.477066993713379\n",
      "Epoch 1, Batch 700, Char 81 Loss: 1.589335322380066\n",
      "Epoch 1, Batch 700, Char 82 Loss: 2.496030330657959\n",
      "Epoch 1, Batch 700, Char 83 Loss: 3.546278476715088\n",
      "Epoch 1, Batch 700, Char 84 Loss: 2.091492176055908\n",
      "Epoch 1, Batch 700, Char 85 Loss: 2.2301254272460938\n",
      "Epoch 1, Batch 700, Char 86 Loss: 3.2261271476745605\n",
      "Epoch 1, Batch 700, Char 87 Loss: 2.125741958618164\n",
      "Epoch 1, Batch 700, Char 88 Loss: 3.4583587646484375\n",
      "Epoch 1, Batch 700, Char 89 Loss: 1.0303165912628174\n",
      "Epoch 1, Batch 700, Char 90 Loss: 2.328812599182129\n",
      "Epoch 1, Batch 700, Char 91 Loss: 2.894899845123291\n",
      "Epoch 1, Batch 700, Char 92 Loss: 2.563453197479248\n",
      "Epoch 1, Batch 700, Char 93 Loss: 1.4754164218902588\n",
      "Epoch 1, Batch 700, Char 94 Loss: 1.8807939291000366\n",
      "Epoch 1, Batch 700, Char 95 Loss: 1.0176953077316284\n",
      "Epoch 1, Batch 700, Char 96 Loss: 1.6920770406723022\n",
      "Epoch 1, Batch 700, Char 97 Loss: 2.1706271171569824\n",
      "Epoch 1, Batch 700, Char 98 Loss: 3.1486635208129883\n",
      "Epoch 1, Batch 800, Char 0 Loss: 1.8923074007034302\n",
      "Epoch 1, Batch 800, Char 1 Loss: 3.564399242401123\n",
      "Epoch 1, Batch 800, Char 2 Loss: 4.90116548538208\n",
      "Epoch 1, Batch 800, Char 3 Loss: 1.5062867403030396\n",
      "Epoch 1, Batch 800, Char 4 Loss: 1.4036056995391846\n",
      "Epoch 1, Batch 800, Char 5 Loss: 1.989703893661499\n",
      "Epoch 1, Batch 800, Char 6 Loss: 1.0030734539031982\n",
      "Epoch 1, Batch 800, Char 7 Loss: 1.8784594535827637\n",
      "Epoch 1, Batch 800, Char 8 Loss: 1.2362017631530762\n",
      "Epoch 1, Batch 800, Char 9 Loss: 1.964309573173523\n",
      "Epoch 1, Batch 800, Char 10 Loss: 1.3752260208129883\n",
      "Epoch 1, Batch 800, Char 11 Loss: 1.9298985004425049\n",
      "Epoch 1, Batch 800, Char 12 Loss: 3.789623260498047\n",
      "Epoch 1, Batch 800, Char 13 Loss: 0.9234917759895325\n",
      "Epoch 1, Batch 800, Char 14 Loss: 2.889268159866333\n",
      "Epoch 1, Batch 800, Char 15 Loss: 3.2705423831939697\n",
      "Epoch 1, Batch 800, Char 16 Loss: 1.847033143043518\n",
      "Epoch 1, Batch 800, Char 17 Loss: 2.8383946418762207\n",
      "Epoch 1, Batch 800, Char 18 Loss: 4.014440536499023\n",
      "Epoch 1, Batch 800, Char 19 Loss: 3.3638052940368652\n",
      "Epoch 1, Batch 800, Char 20 Loss: 1.6259673833847046\n",
      "Epoch 1, Batch 800, Char 21 Loss: 2.282528877258301\n",
      "Epoch 1, Batch 800, Char 22 Loss: 1.8546773195266724\n",
      "Epoch 1, Batch 800, Char 23 Loss: 2.012545108795166\n",
      "Epoch 1, Batch 800, Char 24 Loss: 1.8205169439315796\n",
      "Epoch 1, Batch 800, Char 25 Loss: 2.8434557914733887\n",
      "Epoch 1, Batch 800, Char 26 Loss: 0.8176458477973938\n",
      "Epoch 1, Batch 800, Char 27 Loss: 2.9308083057403564\n",
      "Epoch 1, Batch 800, Char 28 Loss: 2.258331298828125\n",
      "Epoch 1, Batch 800, Char 29 Loss: 3.601749897003174\n",
      "Epoch 1, Batch 800, Char 30 Loss: 1.3987619876861572\n",
      "Epoch 1, Batch 800, Char 31 Loss: 2.5637927055358887\n",
      "Epoch 1, Batch 800, Char 32 Loss: 2.4295296669006348\n",
      "Epoch 1, Batch 800, Char 33 Loss: 2.837994337081909\n",
      "Epoch 1, Batch 800, Char 34 Loss: 2.3364670276641846\n",
      "Epoch 1, Batch 800, Char 35 Loss: 3.60343861579895\n",
      "Epoch 1, Batch 800, Char 36 Loss: 1.3823784589767456\n",
      "Epoch 1, Batch 800, Char 37 Loss: 2.093189001083374\n",
      "Epoch 1, Batch 800, Char 38 Loss: 2.6192831993103027\n",
      "Epoch 1, Batch 800, Char 39 Loss: 2.5330722332000732\n",
      "Epoch 1, Batch 800, Char 40 Loss: 1.7407563924789429\n",
      "Epoch 1, Batch 800, Char 41 Loss: 2.8819944858551025\n",
      "Epoch 1, Batch 800, Char 42 Loss: 3.005835771560669\n",
      "Epoch 1, Batch 800, Char 43 Loss: 6.517933368682861\n",
      "Epoch 1, Batch 800, Char 44 Loss: 1.2429651021957397\n",
      "Epoch 1, Batch 800, Char 45 Loss: 2.204341411590576\n",
      "Epoch 1, Batch 800, Char 46 Loss: 2.955073118209839\n",
      "Epoch 1, Batch 800, Char 47 Loss: 2.003592014312744\n",
      "Epoch 1, Batch 800, Char 48 Loss: 0.9631258845329285\n",
      "Epoch 1, Batch 800, Char 49 Loss: 6.867827415466309\n",
      "Epoch 1, Batch 800, Char 50 Loss: 2.0654075145721436\n",
      "Epoch 1, Batch 800, Char 51 Loss: 8.880969047546387\n",
      "Epoch 1, Batch 800, Char 52 Loss: 0.25763070583343506\n",
      "Epoch 1, Batch 800, Char 53 Loss: 2.0712313652038574\n",
      "Epoch 1, Batch 800, Char 54 Loss: 1.6345405578613281\n",
      "Epoch 1, Batch 800, Char 55 Loss: 1.4793864488601685\n",
      "Epoch 1, Batch 800, Char 56 Loss: 0.4181070327758789\n",
      "Epoch 1, Batch 800, Char 57 Loss: 2.7252182960510254\n",
      "Epoch 1, Batch 800, Char 58 Loss: 1.0677272081375122\n",
      "Epoch 1, Batch 800, Char 59 Loss: 2.8064091205596924\n",
      "Epoch 1, Batch 800, Char 60 Loss: 4.369821071624756\n",
      "Epoch 1, Batch 800, Char 61 Loss: 1.8166043758392334\n",
      "Epoch 1, Batch 800, Char 62 Loss: 4.2740092277526855\n",
      "Epoch 1, Batch 800, Char 63 Loss: 0.2517877221107483\n",
      "Epoch 1, Batch 800, Char 64 Loss: 2.0380043983459473\n",
      "Epoch 1, Batch 800, Char 65 Loss: 1.610343098640442\n",
      "Epoch 1, Batch 800, Char 66 Loss: 1.4482897520065308\n",
      "Epoch 1, Batch 800, Char 67 Loss: 0.41202935576438904\n",
      "Epoch 1, Batch 800, Char 68 Loss: 4.049229145050049\n",
      "Epoch 1, Batch 800, Char 69 Loss: 2.6750283241271973\n",
      "Epoch 1, Batch 800, Char 70 Loss: 4.373429775238037\n",
      "Epoch 1, Batch 800, Char 71 Loss: 0.12591156363487244\n",
      "Epoch 1, Batch 800, Char 72 Loss: 0.9811506271362305\n",
      "Epoch 1, Batch 800, Char 73 Loss: 1.8582843542099\n",
      "Epoch 1, Batch 800, Char 74 Loss: 1.2366503477096558\n",
      "Epoch 1, Batch 800, Char 75 Loss: 1.8240065574645996\n",
      "Epoch 1, Batch 800, Char 76 Loss: 2.0992870330810547\n",
      "Epoch 1, Batch 800, Char 77 Loss: 1.3400540351867676\n",
      "Epoch 1, Batch 800, Char 78 Loss: 2.8245067596435547\n",
      "Epoch 1, Batch 800, Char 79 Loss: 1.7983503341674805\n",
      "Epoch 1, Batch 800, Char 80 Loss: 3.893009901046753\n",
      "Epoch 1, Batch 800, Char 81 Loss: 0.12185202538967133\n",
      "Epoch 1, Batch 800, Char 82 Loss: 0.9705366492271423\n",
      "Epoch 1, Batch 800, Char 83 Loss: 2.6845626831054688\n",
      "Epoch 1, Batch 800, Char 84 Loss: 1.0404657125473022\n",
      "Epoch 1, Batch 800, Char 85 Loss: 3.3861680030822754\n",
      "Epoch 1, Batch 800, Char 86 Loss: 2.5216755867004395\n",
      "Epoch 1, Batch 800, Char 87 Loss: 1.2459328174591064\n",
      "Epoch 1, Batch 800, Char 88 Loss: 2.863581657409668\n",
      "Epoch 1, Batch 800, Char 89 Loss: 1.9257922172546387\n",
      "Epoch 1, Batch 800, Char 90 Loss: 2.668642282485962\n",
      "Epoch 1, Batch 800, Char 91 Loss: 1.9501571655273438\n",
      "Epoch 1, Batch 800, Char 92 Loss: 0.9626317024230957\n",
      "Epoch 1, Batch 800, Char 93 Loss: 2.8370492458343506\n",
      "Epoch 1, Batch 800, Char 94 Loss: 2.2172064781188965\n",
      "Epoch 1, Batch 800, Char 95 Loss: 1.3330192565917969\n",
      "Epoch 1, Batch 800, Char 96 Loss: 2.4152750968933105\n",
      "Epoch 1, Batch 800, Char 97 Loss: 0.9576669335365295\n",
      "Epoch 1, Batch 800, Char 98 Loss: 2.9318652153015137\n",
      "Epoch 1, Batch 900, Char 0 Loss: 3.7160708904266357\n",
      "Epoch 1, Batch 900, Char 1 Loss: 2.7333250045776367\n",
      "Epoch 1, Batch 900, Char 2 Loss: 1.4243887662887573\n",
      "Epoch 1, Batch 900, Char 3 Loss: 1.920232892036438\n",
      "Epoch 1, Batch 900, Char 4 Loss: 0.7516298294067383\n",
      "Epoch 1, Batch 900, Char 5 Loss: 2.4487619400024414\n",
      "Epoch 1, Batch 900, Char 6 Loss: 0.901592493057251\n",
      "Epoch 1, Batch 900, Char 7 Loss: 2.235001564025879\n",
      "Epoch 1, Batch 900, Char 8 Loss: 1.3380372524261475\n",
      "Epoch 1, Batch 900, Char 9 Loss: 2.5308053493499756\n",
      "Epoch 1, Batch 900, Char 10 Loss: 4.208866119384766\n",
      "Epoch 1, Batch 900, Char 11 Loss: 1.5849579572677612\n",
      "Epoch 1, Batch 900, Char 12 Loss: 3.531510353088379\n",
      "Epoch 1, Batch 900, Char 13 Loss: 4.081343173980713\n",
      "Epoch 1, Batch 900, Char 14 Loss: 0.1340806782245636\n",
      "Epoch 1, Batch 900, Char 15 Loss: 0.9472402930259705\n",
      "Epoch 1, Batch 900, Char 16 Loss: 2.691697835922241\n",
      "Epoch 1, Batch 900, Char 17 Loss: 1.4482611417770386\n",
      "Epoch 1, Batch 900, Char 18 Loss: 1.7383477687835693\n",
      "Epoch 1, Batch 900, Char 19 Loss: 0.9272204637527466\n",
      "Epoch 1, Batch 900, Char 20 Loss: 2.0645813941955566\n",
      "Epoch 1, Batch 900, Char 21 Loss: 1.6757744550704956\n",
      "Epoch 1, Batch 900, Char 22 Loss: 2.662184476852417\n",
      "Epoch 1, Batch 900, Char 23 Loss: 2.566493511199951\n",
      "Epoch 1, Batch 900, Char 24 Loss: 1.8858810663223267\n",
      "Epoch 1, Batch 900, Char 25 Loss: 2.918222427368164\n",
      "Epoch 1, Batch 900, Char 26 Loss: 0.8747885823249817\n",
      "Epoch 1, Batch 900, Char 27 Loss: 2.503024101257324\n",
      "Epoch 1, Batch 900, Char 28 Loss: 2.5552282333374023\n",
      "Epoch 1, Batch 900, Char 29 Loss: 0.8691766262054443\n",
      "Epoch 1, Batch 900, Char 30 Loss: 2.688748598098755\n",
      "Epoch 1, Batch 900, Char 31 Loss: 0.38158655166625977\n",
      "Epoch 1, Batch 900, Char 32 Loss: 2.8414359092712402\n",
      "Epoch 1, Batch 900, Char 33 Loss: 2.027637004852295\n",
      "Epoch 1, Batch 900, Char 34 Loss: 2.0385866165161133\n",
      "Epoch 1, Batch 900, Char 35 Loss: 1.3966400623321533\n",
      "Epoch 1, Batch 900, Char 36 Loss: 0.9641343951225281\n",
      "Epoch 1, Batch 900, Char 37 Loss: 3.199233055114746\n",
      "Epoch 1, Batch 900, Char 38 Loss: 1.6021157503128052\n",
      "Epoch 1, Batch 900, Char 39 Loss: 2.0116381645202637\n",
      "Epoch 1, Batch 900, Char 40 Loss: 1.33632230758667\n",
      "Epoch 1, Batch 900, Char 41 Loss: 2.424166202545166\n",
      "Epoch 1, Batch 900, Char 42 Loss: 0.8491507768630981\n",
      "Epoch 1, Batch 900, Char 43 Loss: 2.1774024963378906\n",
      "Epoch 1, Batch 900, Char 44 Loss: 1.3282524347305298\n",
      "Epoch 1, Batch 900, Char 45 Loss: 3.780998945236206\n",
      "Epoch 1, Batch 900, Char 46 Loss: 2.592311382293701\n",
      "Epoch 1, Batch 900, Char 47 Loss: 2.549025297164917\n",
      "Epoch 1, Batch 900, Char 48 Loss: 2.10119891166687\n",
      "Epoch 1, Batch 900, Char 49 Loss: 4.055777549743652\n",
      "Epoch 1, Batch 900, Char 50 Loss: 1.3816096782684326\n",
      "Epoch 1, Batch 900, Char 51 Loss: 2.712019681930542\n",
      "Epoch 1, Batch 900, Char 52 Loss: 3.498023509979248\n",
      "Epoch 1, Batch 900, Char 53 Loss: 0.8878266215324402\n",
      "Epoch 1, Batch 900, Char 54 Loss: 1.683140516281128\n",
      "Epoch 1, Batch 900, Char 55 Loss: 0.9096671342849731\n",
      "Epoch 1, Batch 900, Char 56 Loss: 1.8190813064575195\n",
      "Epoch 1, Batch 900, Char 57 Loss: 1.94193696975708\n",
      "Epoch 1, Batch 900, Char 58 Loss: 1.267940878868103\n",
      "Epoch 1, Batch 900, Char 59 Loss: 3.1408743858337402\n",
      "Epoch 1, Batch 900, Char 60 Loss: 1.5439281463623047\n",
      "Epoch 1, Batch 900, Char 61 Loss: 1.8957754373550415\n",
      "Epoch 1, Batch 900, Char 62 Loss: 1.3132864236831665\n",
      "Epoch 1, Batch 900, Char 63 Loss: 2.8048901557922363\n",
      "Epoch 1, Batch 900, Char 64 Loss: 2.250791549682617\n",
      "Epoch 1, Batch 900, Char 65 Loss: 0.210667222738266\n",
      "Epoch 1, Batch 900, Char 66 Loss: 2.42327618598938\n",
      "Epoch 1, Batch 900, Char 67 Loss: 2.895124912261963\n",
      "Epoch 1, Batch 900, Char 68 Loss: 1.4443031549453735\n",
      "Epoch 1, Batch 900, Char 69 Loss: 1.244523525238037\n",
      "Epoch 1, Batch 900, Char 70 Loss: 3.08744478225708\n",
      "Epoch 1, Batch 900, Char 71 Loss: 2.4005942344665527\n",
      "Epoch 1, Batch 900, Char 72 Loss: 2.786346197128296\n",
      "Epoch 1, Batch 900, Char 73 Loss: 1.8958126306533813\n",
      "Epoch 1, Batch 900, Char 74 Loss: 4.90286922454834\n",
      "Epoch 1, Batch 900, Char 75 Loss: 1.0197042226791382\n",
      "Epoch 1, Batch 900, Char 76 Loss: 2.75998854637146\n",
      "Epoch 1, Batch 900, Char 77 Loss: 1.9498813152313232\n",
      "Epoch 1, Batch 900, Char 78 Loss: 1.8346682786941528\n",
      "Epoch 1, Batch 900, Char 79 Loss: 2.711338996887207\n",
      "Epoch 1, Batch 900, Char 80 Loss: 2.3499984741210938\n",
      "Epoch 1, Batch 900, Char 81 Loss: 2.1948776245117188\n",
      "Epoch 1, Batch 900, Char 82 Loss: 1.2132925987243652\n",
      "Epoch 1, Batch 900, Char 83 Loss: 2.717693328857422\n",
      "Epoch 1, Batch 900, Char 84 Loss: 2.0557892322540283\n",
      "Epoch 1, Batch 900, Char 85 Loss: 2.643441915512085\n",
      "Epoch 1, Batch 900, Char 86 Loss: 2.1720826625823975\n",
      "Epoch 1, Batch 900, Char 87 Loss: 1.0063287019729614\n",
      "Epoch 1, Batch 900, Char 88 Loss: 2.504061698913574\n",
      "Epoch 1, Batch 900, Char 89 Loss: 2.668395519256592\n",
      "Epoch 1, Batch 900, Char 90 Loss: 2.393616199493408\n",
      "Epoch 1, Batch 900, Char 91 Loss: 2.0638842582702637\n",
      "Epoch 1, Batch 900, Char 92 Loss: 2.624126434326172\n",
      "Epoch 1, Batch 900, Char 93 Loss: 5.21589469909668\n",
      "Epoch 1, Batch 900, Char 94 Loss: 2.0688889026641846\n",
      "Epoch 1, Batch 900, Char 95 Loss: 1.7180211544036865\n",
      "Epoch 1, Batch 900, Char 96 Loss: 2.288219928741455\n",
      "Epoch 1, Batch 900, Char 97 Loss: 3.0262813568115234\n",
      "Epoch 1, Batch 900, Char 98 Loss: 1.3266808986663818\n",
      "Epoch 1, Batch 1000, Char 0 Loss: 2.9550671577453613\n",
      "Epoch 1, Batch 1000, Char 0\n",
      "target: o predicted: d\n",
      "History: \"o\", Predicted: \"d\"\n",
      "Epoch 1, Batch 1000, Char 1 Loss: 7.455118179321289\n",
      "Epoch 1, Batch 1000, Char 1\n",
      "target: b predicted:  \n",
      "History: \"ob\", Predicted: \"d \"\n",
      "Epoch 1, Batch 1000, Char 2 Loss: 2.092628240585327\n",
      "Epoch 1, Batch 1000, Char 2\n",
      "target: o predicted: e\n",
      "History: \"obo\", Predicted: \"d e\"\n",
      "Epoch 1, Batch 1000, Char 3 Loss: 4.216033935546875\n",
      "Epoch 1, Batch 1000, Char 3\n",
      "target: d predicted:  \n",
      "History: \"obod\", Predicted: \"d e \"\n",
      "Epoch 1, Batch 1000, Char 4 Loss: 4.974202632904053\n",
      "Epoch 1, Batch 1000, Char 4\n",
      "target: y predicted:  \n",
      "History: \"obody\", Predicted: \"d e  \"\n",
      "Epoch 1, Batch 1000, Char 5 Loss: 0.1882658451795578\n",
      "Epoch 1, Batch 1000, Char 5\n",
      "target:   predicted:  \n",
      "History: \"obody \", Predicted: \"d e   \"\n",
      "Epoch 1, Batch 1000, Char 6 Loss: 3.0928385257720947\n",
      "Epoch 1, Batch 1000, Char 6\n",
      "target: b predicted: a\n",
      "History: \"obody b\", Predicted: \"d e   a\"\n",
      "Epoch 1, Batch 1000, Char 7 Loss: 2.2282752990722656\n",
      "Epoch 1, Batch 1000, Char 7\n",
      "target: u predicted: e\n",
      "History: \"obody bu\", Predicted: \"d e   ae\"\n",
      "Epoch 1, Batch 1000, Char 8 Loss: 2.200639486312866\n",
      "Epoch 1, Batch 1000, Char 8\n",
      "target: t predicted: l\n",
      "History: \"obody but\", Predicted: \"d e   ael\"\n",
      "Epoch 1, Batch 1000, Char 9 Loss: 1.2519325017929077\n",
      "Epoch 1, Batch 1000, Char 9\n",
      "target:   predicted: h\n",
      "History: \"obody but \", Predicted: \"d e   aelh\"\n",
      "Epoch 1, Batch 1000, Char 10 Loss: 2.574798583984375\n",
      "Epoch 1, Batch 1000, Char 10\n",
      "target: h predicted: a\n",
      "History: \"obody but h\", Predicted: \"d e   aelha\"\n",
      "Epoch 1, Batch 1000, Char 11 Loss: 0.7246538996696472\n",
      "Epoch 1, Batch 1000, Char 11\n",
      "target: e predicted: e\n",
      "History: \"obody but he\", Predicted: \"d e   aelhae\"\n",
      "Epoch 1, Batch 1000, Char 12 Loss: 1.7210394144058228\n",
      "Epoch 1, Batch 1000, Char 12\n",
      "target: r predicted:  \n",
      "History: \"obody but her\", Predicted: \"d e   aelhae \"\n",
      "Epoch 1, Batch 1000, Char 13 Loss: 0.9935227632522583\n",
      "Epoch 1, Batch 1000, Char 13\n",
      "target:   predicted:  \n",
      "History: \"obody but her \", Predicted: \"d e   aelhae  \"\n",
      "Epoch 1, Batch 1000, Char 14 Loss: 3.3542113304138184\n",
      "Epoch 1, Batch 1000, Char 14\n",
      "target: c predicted: a\n",
      "History: \"obody but her c\", Predicted: \"d e   aelhae  a\"\n",
      "Epoch 1, Batch 1000, Char 15 Loss: 1.8271186351776123\n",
      "Epoch 1, Batch 1000, Char 15\n",
      "target: o predicted: h\n",
      "History: \"obody but her co\", Predicted: \"d e   aelhae  ah\"\n",
      "Epoch 1, Batch 1000, Char 16 Loss: 3.1003060340881348\n",
      "Epoch 1, Batch 1000, Char 16\n",
      "target: l predicted:  \n",
      "History: \"obody but her col\", Predicted: \"d e   aelhae  ah \"\n",
      "Epoch 1, Batch 1000, Char 17 Loss: 2.2319283485412598\n",
      "Epoch 1, Batch 1000, Char 17\n",
      "target: o predicted: e\n",
      "History: \"obody but her colo\", Predicted: \"d e   aelhae  ah e\"\n",
      "Epoch 1, Batch 1000, Char 18 Loss: 2.027369260787964\n",
      "Epoch 1, Batch 1000, Char 18\n",
      "target: r predicted:  \n",
      "History: \"obody but her color\", Predicted: \"d e   aelhae  ah e \"\n",
      "Epoch 1, Batch 1000, Char 19 Loss: 1.8517613410949707\n",
      "Epoch 1, Batch 1000, Char 19\n",
      "target: e predicted:  \n",
      "History: \"obody but her colore\", Predicted: \"d e   aelhae  ah e  \"\n",
      "Epoch 1, Batch 1000, Char 20 Loss: 2.324665069580078\n",
      "Epoch 1, Batch 1000, Char 20\n",
      "target: d predicted:  \n",
      "History: \"obody but her colored\", Predicted: \"d e   aelhae  ah e   \"\n",
      "Epoch 1, Batch 1000, Char 21 Loss: 0.3164054751396179\n",
      "Epoch 1, Batch 1000, Char 21\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored \", Predicted: \"d e   aelhae  ah e    \"\n",
      "Epoch 1, Batch 1000, Char 22 Loss: 2.5850536823272705\n",
      "Epoch 1, Batch 1000, Char 22\n",
      "target: s predicted: a\n",
      "History: \"obody but her colored s\", Predicted: \"d e   aelhae  ah e    a\"\n",
      "Epoch 1, Batch 1000, Char 23 Loss: 2.2323012351989746\n",
      "Epoch 1, Batch 1000, Char 23\n",
      "target: e predicted:  \n",
      "History: \"obody but her colored se\", Predicted: \"d e   aelhae  ah e    a \"\n",
      "Epoch 1, Batch 1000, Char 24 Loss: 1.6872111558914185\n",
      "Epoch 1, Batch 1000, Char 24\n",
      "target: r predicted:  \n",
      "History: \"obody but her colored ser\", Predicted: \"d e   aelhae  ah e    a  \"\n",
      "Epoch 1, Batch 1000, Char 25 Loss: 8.816264152526855\n",
      "Epoch 1, Batch 1000, Char 25\n",
      "target: v predicted:  \n",
      "History: \"obody but her colored serv\", Predicted: \"d e   aelhae  ah e    a   \"\n",
      "Epoch 1, Batch 1000, Char 26 Loss: 5.44550895690918\n",
      "Epoch 1, Batch 1000, Char 26\n",
      "target: a predicted: e\n",
      "History: \"obody but her colored serva\", Predicted: \"d e   aelhae  ah e    a   e\"\n",
      "Epoch 1, Batch 1000, Char 27 Loss: 1.3503702878952026\n",
      "Epoch 1, Batch 1000, Char 27\n",
      "target: n predicted: n\n",
      "History: \"obody but her colored servan\", Predicted: \"d e   aelhae  ah e    a   en\"\n",
      "Epoch 1, Batch 1000, Char 28 Loss: 2.5304341316223145\n",
      "Epoch 1, Batch 1000, Char 28\n",
      "target: t predicted: d\n",
      "History: \"obody but her colored servant\", Predicted: \"d e   aelhae  ah e    a   end\"\n",
      "Epoch 1, Batch 1000, Char 29 Loss: 5.4653706550598145\n",
      "Epoch 1, Batch 1000, Char 29\n",
      "target: s predicted: h\n",
      "History: \"obody but her colored servants\", Predicted: \"d e   aelhae  ah e    a   endh\"\n",
      "Epoch 1, Batch 1000, Char 30 Loss: 0.866770327091217\n",
      "Epoch 1, Batch 1000, Char 30\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants \", Predicted: \"d e   aelhae  ah e    a   endh \"\n",
      "Epoch 1, Batch 1000, Char 31 Loss: 3.240544319152832\n",
      "Epoch 1, Batch 1000, Char 31\n",
      "target: f predicted: a\n",
      "History: \"obody but her colored servants f\", Predicted: \"d e   aelhae  ah e    a   endh a\"\n",
      "Epoch 1, Batch 1000, Char 32 Loss: 1.9515669345855713\n",
      "Epoch 1, Batch 1000, Char 32\n",
      "target: o predicted:  \n",
      "History: \"obody but her colored servants fo\", Predicted: \"d e   aelhae  ah e    a   endh a \"\n",
      "Epoch 1, Batch 1000, Char 33 Loss: 1.9674999713897705\n",
      "Epoch 1, Batch 1000, Char 33\n",
      "target: r predicted:  \n",
      "History: \"obody but her colored servants for\", Predicted: \"d e   aelhae  ah e    a   endh a  \"\n",
      "Epoch 1, Batch 1000, Char 34 Loss: 1.0014599561691284\n",
      "Epoch 1, Batch 1000, Char 34\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants for \", Predicted: \"d e   aelhae  ah e    a   endh a   \"\n",
      "Epoch 1, Batch 1000, Char 35 Loss: 2.0304782390594482\n",
      "Epoch 1, Batch 1000, Char 35\n",
      "target: a predicted: a\n",
      "History: \"obody but her colored servants for a\", Predicted: \"d e   aelhae  ah e    a   endh a   a\"\n",
      "Epoch 1, Batch 1000, Char 36 Loss: 2.641488790512085\n",
      "Epoch 1, Batch 1000, Char 36\n",
      "target:   predicted: n\n",
      "History: \"obody but her colored servants for a \", Predicted: \"d e   aelhae  ah e    a   endh a   an\"\n",
      "Epoch 1, Batch 1000, Char 37 Loss: 2.4668359756469727\n",
      "Epoch 1, Batch 1000, Char 37\n",
      "target: w predicted: a\n",
      "History: \"obody but her colored servants for a w\", Predicted: \"d e   aelhae  ah e    a   endh a   ana\"\n",
      "Epoch 1, Batch 1000, Char 38 Loss: 1.9577014446258545\n",
      "Epoch 1, Batch 1000, Char 38\n",
      "target: e predicted: a\n",
      "History: \"obody but her colored servants for a we\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa\"\n",
      "Epoch 1, Batch 1000, Char 39 Loss: 3.408512592315674\n",
      "Epoch 1, Batch 1000, Char 39\n",
      "target: e predicted:  \n",
      "History: \"obody but her colored servants for a wee\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa \"\n",
      "Epoch 1, Batch 1000, Char 40 Loss: 11.901178359985352\n",
      "Epoch 1, Batch 1000, Char 40\n",
      "target: k predicted:  \n",
      "History: \"obody but her colored servants for a week\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  \"\n",
      "Epoch 1, Batch 1000, Char 41 Loss: 1.2035173177719116\n",
      "Epoch 1, Batch 1000, Char 41\n",
      "target:   predicted: e\n",
      "History: \"obody but her colored servants for a week \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  e\"\n",
      "Epoch 1, Batch 1000, Char 42 Loss: 2.0149521827697754\n",
      "Epoch 1, Batch 1000, Char 42\n",
      "target: a predicted: a\n",
      "History: \"obody but her colored servants for a week a\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  ea\"\n",
      "Epoch 1, Batch 1000, Char 43 Loss: 2.143561840057373\n",
      "Epoch 1, Batch 1000, Char 43\n",
      "target: t predicted: n\n",
      "History: \"obody but her colored servants for a week at\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  ean\"\n",
      "Epoch 1, Batch 1000, Char 44 Loss: 1.2332121133804321\n",
      "Epoch 1, Batch 1000, Char 44\n",
      "target:   predicted: h\n",
      "History: \"obody but her colored servants for a week at \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanh\"\n",
      "Epoch 1, Batch 1000, Char 45 Loss: 2.0005455017089844\n",
      "Epoch 1, Batch 1000, Char 45\n",
      "target: a predicted: a\n",
      "History: \"obody but her colored servants for a week at a\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanha\"\n",
      "Epoch 1, Batch 1000, Char 46 Loss: 2.591459274291992\n",
      "Epoch 1, Batch 1000, Char 46\n",
      "target:   predicted: n\n",
      "History: \"obody but her colored servants for a week at a \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhan\"\n",
      "Epoch 1, Batch 1000, Char 47 Loss: 2.18186616897583\n",
      "Epoch 1, Batch 1000, Char 47\n",
      "target: t predicted: a\n",
      "History: \"obody but her colored servants for a week at a t\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhana\"\n",
      "Epoch 1, Batch 1000, Char 48 Loss: 3.132765769958496\n",
      "Epoch 1, Batch 1000, Char 48\n",
      "target: i predicted: h\n",
      "History: \"obody but her colored servants for a week at a ti\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanah\"\n",
      "Epoch 1, Batch 1000, Char 49 Loss: 3.2551002502441406\n",
      "Epoch 1, Batch 1000, Char 49\n",
      "target: m predicted: n\n",
      "History: \"obody but her colored servants for a week at a tim\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahn\"\n",
      "Epoch 1, Batch 1000, Char 50 Loss: 1.5409337282180786\n",
      "Epoch 1, Batch 1000, Char 50\n",
      "target: e predicted: e\n",
      "History: \"obody but her colored servants for a week at a time\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne\"\n",
      "Epoch 1, Batch 1000, Char 51 Loss: 1.1980838775634766\n",
      "Epoch 1, Batch 1000, Char 51\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants for a week at a time \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne \"\n",
      "Epoch 1, Batch 1000, Char 52 Loss: 2.5526325702667236\n",
      "Epoch 1, Batch 1000, Char 52\n",
      "target: s predicted: a\n",
      "History: \"obody but her colored servants for a week at a time s\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a\"\n",
      "Epoch 1, Batch 1000, Char 53 Loss: 2.4513704776763916\n",
      "Epoch 1, Batch 1000, Char 53\n",
      "target: h predicted:  \n",
      "History: \"obody but her colored servants for a week at a time sh\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a \"\n",
      "Epoch 1, Batch 1000, Char 54 Loss: 0.7012860178947449\n",
      "Epoch 1, Batch 1000, Char 54\n",
      "target: e predicted: e\n",
      "History: \"obody but her colored servants for a week at a time she\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e\"\n",
      "Epoch 1, Batch 1000, Char 55 Loss: 1.1880848407745361\n",
      "Epoch 1, Batch 1000, Char 55\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e \"\n",
      "Epoch 1, Batch 1000, Char 56 Loss: 2.5833792686462402\n",
      "Epoch 1, Batch 1000, Char 56\n",
      "target: h predicted: a\n",
      "History: \"obody but her colored servants for a week at a time she h\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e a\"\n",
      "Epoch 1, Batch 1000, Char 57 Loss: 0.6948041319847107\n",
      "Epoch 1, Batch 1000, Char 57\n",
      "target: e predicted: e\n",
      "History: \"obody but her colored servants for a week at a time she he\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae\"\n",
      "Epoch 1, Batch 1000, Char 58 Loss: 3.0998644828796387\n",
      "Epoch 1, Batch 1000, Char 58\n",
      "target: l predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she hel\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae \"\n",
      "Epoch 1, Batch 1000, Char 59 Loss: 2.072389602661133\n",
      "Epoch 1, Batch 1000, Char 59\n",
      "target: d predicted: e\n",
      "History: \"obody but her colored servants for a week at a time she held\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e\"\n",
      "Epoch 1, Batch 1000, Char 60 Loss: 0.3126826286315918\n",
      "Epoch 1, Batch 1000, Char 60\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e \"\n",
      "Epoch 1, Batch 1000, Char 61 Loss: 2.558915376663208\n",
      "Epoch 1, Batch 1000, Char 61\n",
      "target: h predicted: a\n",
      "History: \"obody but her colored servants for a week at a time she held h\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e a\"\n",
      "Epoch 1, Batch 1000, Char 62 Loss: 0.6795375943183899\n",
      "Epoch 1, Batch 1000, Char 62\n",
      "target: e predicted: e\n",
      "History: \"obody but her colored servants for a week at a time she held he\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae\"\n",
      "Epoch 1, Batch 1000, Char 63 Loss: 1.665236473083496\n",
      "Epoch 1, Batch 1000, Char 63\n",
      "target: r predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held her\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae \"\n",
      "Epoch 1, Batch 1000, Char 64 Loss: 3.3171231746673584\n",
      "Epoch 1, Batch 1000, Char 64\n",
      "target: s predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held hers\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae  \"\n",
      "Epoch 1, Batch 1000, Char 65 Loss: 2.161787986755371\n",
      "Epoch 1, Batch 1000, Char 65\n",
      "target: e predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herse\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae   \"\n",
      "Epoch 1, Batch 1000, Char 66 Loss: 3.0614066123962402\n",
      "Epoch 1, Batch 1000, Char 66\n",
      "target: l predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held hersel\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    \"\n",
      "Epoch 1, Batch 1000, Char 67 Loss: 4.914727687835693\n",
      "Epoch 1, Batch 1000, Char 67\n",
      "target: f predicted: e\n",
      "History: \"obody but her colored servants for a week at a time she held herself\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e\"\n",
      "Epoch 1, Batch 1000, Char 68 Loss: 0.8595198392868042\n",
      "Epoch 1, Batch 1000, Char 68\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e \"\n",
      "Epoch 1, Batch 1000, Char 69 Loss: 5.425764560699463\n",
      "Epoch 1, Batch 1000, Char 69\n",
      "target: v predicted: a\n",
      "History: \"obody but her colored servants for a week at a time she held herself v\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e a\"\n",
      "Epoch 1, Batch 1000, Char 70 Loss: 0.07546023279428482\n",
      "Epoch 1, Batch 1000, Char 70\n",
      "target: e predicted: e\n",
      "History: \"obody but her colored servants for a week at a time she held herself ve\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae\"\n",
      "Epoch 1, Batch 1000, Char 71 Loss: 1.6503751277923584\n",
      "Epoch 1, Batch 1000, Char 71\n",
      "target: r predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself ver\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae \"\n",
      "Epoch 1, Batch 1000, Char 72 Loss: 2.6483054161071777\n",
      "Epoch 1, Batch 1000, Char 72\n",
      "target: y predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae  \"\n",
      "Epoch 1, Batch 1000, Char 73 Loss: 0.1865643858909607\n",
      "Epoch 1, Batch 1000, Char 73\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   \"\n",
      "Epoch 1, Batch 1000, Char 74 Loss: 2.5090880393981934\n",
      "Epoch 1, Batch 1000, Char 74\n",
      "target: s predicted: a\n",
      "History: \"obody but her colored servants for a week at a time she held herself very s\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a\"\n",
      "Epoch 1, Batch 1000, Char 75 Loss: 1.9582481384277344\n",
      "Epoch 1, Batch 1000, Char 75\n",
      "target: t predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very st\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a \"\n",
      "Epoch 1, Batch 1000, Char 76 Loss: 3.511585235595703\n",
      "Epoch 1, Batch 1000, Char 76\n",
      "target: r predicted: h\n",
      "History: \"obody but her colored servants for a week at a time she held herself very str\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h\"\n",
      "Epoch 1, Batch 1000, Char 77 Loss: 2.685652494430542\n",
      "Epoch 1, Batch 1000, Char 77\n",
      "target: a predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very stra\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h \"\n",
      "Epoch 1, Batch 1000, Char 78 Loss: 3.762540102005005\n",
      "Epoch 1, Batch 1000, Char 78\n",
      "target: i predicted: n\n",
      "History: \"obody but her colored servants for a week at a time she held herself very strai\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h n\"\n",
      "Epoch 1, Batch 1000, Char 79 Loss: 2.839449405670166\n",
      "Epoch 1, Batch 1000, Char 79\n",
      "target: g predicted: n\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straig\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn\"\n",
      "Epoch 1, Batch 1000, Char 80 Loss: 1.4312982559204102\n",
      "Epoch 1, Batch 1000, Char 80\n",
      "target: h predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very straigh\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn \"\n",
      "Epoch 1, Batch 1000, Char 81 Loss: 3.1913256645202637\n",
      "Epoch 1, Batch 1000, Char 81\n",
      "target: t predicted: e\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn e\"\n",
      "Epoch 1, Batch 1000, Char 82 Loss: 1.2212700843811035\n",
      "Epoch 1, Batch 1000, Char 82\n",
      "target:   predicted: h\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn eh\"\n",
      "Epoch 1, Batch 1000, Char 83 Loss: 2.4635770320892334\n",
      "Epoch 1, Batch 1000, Char 83\n",
      "target: w predicted: a\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight w\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn eha\"\n",
      "Epoch 1, Batch 1000, Char 84 Loss: 1.4980376958847046\n",
      "Epoch 1, Batch 1000, Char 84\n",
      "target: h predicted: a\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight wh\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaa\"\n",
      "Epoch 1, Batch 1000, Char 85 Loss: 0.6440728902816772\n",
      "Epoch 1, Batch 1000, Char 85\n",
      "target: e predicted: e\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight whe\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae\"\n",
      "Epoch 1, Batch 1000, Char 86 Loss: 2.7977850437164307\n",
      "Epoch 1, Batch 1000, Char 86\n",
      "target: n predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae \"\n",
      "Epoch 1, Batch 1000, Char 87 Loss: 1.3893697261810303\n",
      "Epoch 1, Batch 1000, Char 87\n",
      "target:   predicted: d\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae d\"\n",
      "Epoch 1, Batch 1000, Char 88 Loss: 2.469374656677246\n",
      "Epoch 1, Batch 1000, Char 88\n",
      "target: s predicted: a\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when s\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da\"\n",
      "Epoch 1, Batch 1000, Char 89 Loss: 2.0969595909118652\n",
      "Epoch 1, Batch 1000, Char 89\n",
      "target: e predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when se\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da \"\n",
      "Epoch 1, Batch 1000, Char 90 Loss: 2.535661458969116\n",
      "Epoch 1, Batch 1000, Char 90\n",
      "target: a predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when sea\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  \"\n",
      "Epoch 1, Batch 1000, Char 91 Loss: 2.0825860500335693\n",
      "Epoch 1, Batch 1000, Char 91\n",
      "target: t predicted: n\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when seat\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  n\"\n",
      "Epoch 1, Batch 1000, Char 92 Loss: 2.292309284210205\n",
      "Epoch 1, Batch 1000, Char 92\n",
      "target: e predicted: h\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when seate\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  nh\"\n",
      "Epoch 1, Batch 1000, Char 93 Loss: 2.34124755859375\n",
      "Epoch 1, Batch 1000, Char 93\n",
      "target: d predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when seated\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  nh \"\n",
      "Epoch 1, Batch 1000, Char 94 Loss: 0.31578031182289124\n",
      "Epoch 1, Batch 1000, Char 94\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when seated \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  nh  \"\n",
      "Epoch 1, Batch 1000, Char 95 Loss: 1.992139220237732\n",
      "Epoch 1, Batch 1000, Char 95\n",
      "target: a predicted: a\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when seated a\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  nh  a\"\n",
      "Epoch 1, Batch 1000, Char 96 Loss: 1.400939702987671\n",
      "Epoch 1, Batch 1000, Char 96\n",
      "target: n predicted: n\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when seated an\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  nh  an\"\n",
      "Epoch 1, Batch 1000, Char 97 Loss: 1.3439841270446777\n",
      "Epoch 1, Batch 1000, Char 97\n",
      "target: d predicted: d\n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when seated and\", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  nh  and\"\n",
      "Epoch 1, Batch 1000, Char 98 Loss: 0.3153380751609802\n",
      "Epoch 1, Batch 1000, Char 98\n",
      "target:   predicted:  \n",
      "History: \"obody but her colored servants for a week at a time she held herself very straight when seated and \", Predicted: \"d e   aelhae  ah e    a   endh a   anaa  eanhanahne a e ae e ae    e ae   a h nn ehaae da  nh  and \"\n",
      "Epoch 1, Batch 1100, Char 0 Loss: 2.055222988128662\n",
      "Epoch 1, Batch 1100, Char 1 Loss: 2.7667737007141113\n",
      "Epoch 1, Batch 1100, Char 2 Loss: 8.337617874145508\n",
      "Epoch 1, Batch 1100, Char 3 Loss: 3.97531795501709\n",
      "Epoch 1, Batch 1100, Char 4 Loss: 2.6050450801849365\n",
      "Epoch 1, Batch 1100, Char 5 Loss: 2.1049909591674805\n",
      "Epoch 1, Batch 1100, Char 6 Loss: 1.9413719177246094\n",
      "Epoch 1, Batch 1100, Char 7 Loss: 2.1712729930877686\n",
      "Epoch 1, Batch 1100, Char 8 Loss: 2.1697120666503906\n",
      "Epoch 1, Batch 1100, Char 9 Loss: 4.29734992980957\n",
      "Epoch 1, Batch 1100, Char 10 Loss: 2.8565521240234375\n",
      "Epoch 1, Batch 1100, Char 11 Loss: 2.028000831604004\n",
      "Epoch 1, Batch 1100, Char 12 Loss: 1.3653631210327148\n",
      "Epoch 1, Batch 1100, Char 13 Loss: 2.4921135902404785\n",
      "Epoch 1, Batch 1100, Char 14 Loss: 2.1120681762695312\n",
      "Epoch 1, Batch 1100, Char 15 Loss: 1.355728268623352\n",
      "Epoch 1, Batch 1100, Char 16 Loss: 1.910703420639038\n",
      "Epoch 1, Batch 1100, Char 17 Loss: 1.1756350994110107\n",
      "Epoch 1, Batch 1100, Char 18 Loss: 0.8464769124984741\n",
      "Epoch 1, Batch 1100, Char 19 Loss: 2.5133891105651855\n",
      "Epoch 1, Batch 1100, Char 20 Loss: 1.1557615995407104\n",
      "Epoch 1, Batch 1100, Char 21 Loss: 2.4674339294433594\n",
      "Epoch 1, Batch 1100, Char 22 Loss: 2.5899696350097656\n",
      "Epoch 1, Batch 1100, Char 23 Loss: 1.8775782585144043\n",
      "Epoch 1, Batch 1100, Char 24 Loss: 2.1446666717529297\n",
      "Epoch 1, Batch 1100, Char 25 Loss: 2.9116992950439453\n",
      "Epoch 1, Batch 1100, Char 26 Loss: 4.396376132965088\n",
      "Epoch 1, Batch 1100, Char 27 Loss: 0.6375010013580322\n",
      "Epoch 1, Batch 1100, Char 28 Loss: 2.0370936393737793\n",
      "Epoch 1, Batch 1100, Char 29 Loss: 2.300668478012085\n",
      "Epoch 1, Batch 1100, Char 30 Loss: 3.225573778152466\n",
      "Epoch 1, Batch 1100, Char 31 Loss: 2.480210781097412\n",
      "Epoch 1, Batch 1100, Char 32 Loss: 1.4345663785934448\n",
      "Epoch 1, Batch 1100, Char 33 Loss: 4.825608730316162\n",
      "Epoch 1, Batch 1100, Char 34 Loss: 0.6090802550315857\n",
      "Epoch 1, Batch 1100, Char 35 Loss: 3.1900739669799805\n",
      "Epoch 1, Batch 1100, Char 36 Loss: 3.5799412727355957\n",
      "Epoch 1, Batch 1100, Char 37 Loss: 7.838138103485107\n",
      "Epoch 1, Batch 1100, Char 38 Loss: 1.3054256439208984\n",
      "Epoch 1, Batch 1100, Char 39 Loss: 1.0116822719573975\n",
      "Epoch 1, Batch 1100, Char 40 Loss: 3.0147039890289307\n",
      "Epoch 1, Batch 1100, Char 41 Loss: 1.287543773651123\n",
      "Epoch 1, Batch 1100, Char 42 Loss: 3.4536943435668945\n",
      "Epoch 1, Batch 1100, Char 43 Loss: 5.9639692306518555\n",
      "Epoch 1, Batch 1100, Char 44 Loss: 1.7529224157333374\n",
      "Epoch 1, Batch 1100, Char 45 Loss: 3.6153106689453125\n",
      "Epoch 1, Batch 1100, Char 46 Loss: 2.4638729095458984\n",
      "Epoch 1, Batch 1100, Char 47 Loss: 1.1121689081192017\n",
      "Epoch 1, Batch 1100, Char 48 Loss: 2.6543073654174805\n",
      "Epoch 1, Batch 1100, Char 49 Loss: 1.8421509265899658\n",
      "Epoch 1, Batch 1100, Char 50 Loss: 0.20119577646255493\n",
      "Epoch 1, Batch 1100, Char 51 Loss: 1.8452231884002686\n",
      "Epoch 1, Batch 1100, Char 52 Loss: 1.217585563659668\n",
      "Epoch 1, Batch 1100, Char 53 Loss: 4.588761806488037\n",
      "Epoch 1, Batch 1100, Char 54 Loss: 3.620737075805664\n",
      "Epoch 1, Batch 1100, Char 55 Loss: 3.3875961303710938\n",
      "Epoch 1, Batch 1100, Char 56 Loss: 5.761139869689941\n",
      "Epoch 1, Batch 1100, Char 57 Loss: 2.053227663040161\n",
      "Epoch 1, Batch 1100, Char 58 Loss: 1.5008361339569092\n",
      "Epoch 1, Batch 1100, Char 59 Loss: 1.737033486366272\n",
      "Epoch 1, Batch 1100, Char 60 Loss: 0.39410245418548584\n",
      "Epoch 1, Batch 1100, Char 61 Loss: 3.8766980171203613\n",
      "Epoch 1, Batch 1100, Char 62 Loss: 2.4653944969177246\n",
      "Epoch 1, Batch 1100, Char 63 Loss: 1.383884310722351\n",
      "Epoch 1, Batch 1100, Char 64 Loss: 2.40342116355896\n",
      "Epoch 1, Batch 1100, Char 65 Loss: 2.5004935264587402\n",
      "Epoch 1, Batch 1100, Char 66 Loss: 2.0457911491394043\n",
      "Epoch 1, Batch 1100, Char 67 Loss: 1.1712950468063354\n",
      "Epoch 1, Batch 1100, Char 68 Loss: 2.806016206741333\n",
      "Epoch 1, Batch 1100, Char 69 Loss: 1.24481201171875\n",
      "Epoch 1, Batch 1100, Char 70 Loss: 3.792513370513916\n",
      "Epoch 1, Batch 1100, Char 71 Loss: 0.12415958940982819\n",
      "Epoch 1, Batch 1100, Char 72 Loss: 2.3510775566101074\n",
      "Epoch 1, Batch 1100, Char 73 Loss: 0.387832909822464\n",
      "Epoch 1, Batch 1100, Char 74 Loss: 2.4433467388153076\n",
      "Epoch 1, Batch 1100, Char 75 Loss: 2.0567996501922607\n",
      "Epoch 1, Batch 1100, Char 76 Loss: 1.309483289718628\n",
      "Epoch 1, Batch 1100, Char 77 Loss: 1.8427398204803467\n",
      "Epoch 1, Batch 1100, Char 78 Loss: 1.1950678825378418\n",
      "Epoch 1, Batch 1100, Char 79 Loss: 0.8228393793106079\n",
      "Epoch 1, Batch 1100, Char 80 Loss: 2.8169422149658203\n",
      "Epoch 1, Batch 1100, Char 81 Loss: 1.9615145921707153\n",
      "Epoch 1, Batch 1100, Char 82 Loss: 3.18912935256958\n",
      "Epoch 1, Batch 1100, Char 83 Loss: 2.574215888977051\n",
      "Epoch 1, Batch 1100, Char 84 Loss: 3.005152702331543\n",
      "Epoch 1, Batch 1100, Char 85 Loss: 2.0104684829711914\n",
      "Epoch 1, Batch 1100, Char 86 Loss: 2.586465835571289\n",
      "Epoch 1, Batch 1100, Char 87 Loss: 1.553560495376587\n",
      "Epoch 1, Batch 1100, Char 88 Loss: 2.0646753311157227\n",
      "Epoch 1, Batch 1100, Char 89 Loss: 0.19247931241989136\n",
      "Epoch 1, Batch 1100, Char 90 Loss: 2.0365750789642334\n",
      "Epoch 1, Batch 1100, Char 91 Loss: 4.133185386657715\n",
      "Epoch 1, Batch 1100, Char 92 Loss: 2.3448965549468994\n",
      "Epoch 1, Batch 1100, Char 93 Loss: 3.487614631652832\n",
      "Epoch 1, Batch 1100, Char 94 Loss: 0.12177624553442001\n",
      "Epoch 1, Batch 1100, Char 95 Loss: 1.0536876916885376\n",
      "Epoch 1, Batch 1100, Char 96 Loss: 2.6107492446899414\n",
      "Epoch 1, Batch 1100, Char 97 Loss: 1.7550759315490723\n",
      "Epoch 1, Batch 1100, Char 98 Loss: 0.18987244367599487\n",
      "Epoch 1, Batch 1200, Char 0 Loss: 2.7609846591949463\n",
      "Epoch 1, Batch 1200, Char 1 Loss: 0.8068982362747192\n",
      "Epoch 1, Batch 1200, Char 2 Loss: 4.878591537475586\n",
      "Epoch 1, Batch 1200, Char 3 Loss: 2.1128153800964355\n",
      "Epoch 1, Batch 1200, Char 4 Loss: 16.656482696533203\n",
      "Epoch 1, Batch 1200, Char 5 Loss: 2.4497857093811035\n",
      "Epoch 1, Batch 1200, Char 6 Loss: 3.897681951522827\n",
      "Epoch 1, Batch 1200, Char 7 Loss: 2.084580183029175\n",
      "Epoch 1, Batch 1200, Char 8 Loss: 1.4075371026992798\n",
      "Epoch 1, Batch 1200, Char 9 Loss: 2.5899622440338135\n",
      "Epoch 1, Batch 1200, Char 10 Loss: 3.5440337657928467\n",
      "Epoch 1, Batch 1200, Char 11 Loss: 1.7677384614944458\n",
      "Epoch 1, Batch 1200, Char 12 Loss: 3.120572328567505\n",
      "Epoch 1, Batch 1200, Char 13 Loss: 2.3302910327911377\n",
      "Epoch 1, Batch 1200, Char 14 Loss: 0.346477210521698\n",
      "Epoch 1, Batch 1200, Char 15 Loss: 2.6558470726013184\n",
      "Epoch 1, Batch 1200, Char 16 Loss: 2.9658684730529785\n",
      "Epoch 1, Batch 1200, Char 17 Loss: 1.6655734777450562\n",
      "Epoch 1, Batch 1200, Char 18 Loss: 4.382003307342529\n",
      "Epoch 1, Batch 1200, Char 19 Loss: 1.7151687145233154\n",
      "Epoch 1, Batch 1200, Char 20 Loss: 2.0317564010620117\n",
      "Epoch 1, Batch 1200, Char 21 Loss: 3.912752628326416\n",
      "Epoch 1, Batch 1200, Char 22 Loss: 2.6939685344696045\n",
      "Epoch 1, Batch 1200, Char 23 Loss: 2.1044957637786865\n",
      "Epoch 1, Batch 1200, Char 24 Loss: 2.669715404510498\n",
      "Epoch 1, Batch 1200, Char 25 Loss: 3.388613224029541\n",
      "Epoch 1, Batch 1200, Char 26 Loss: 1.94189453125\n",
      "Epoch 1, Batch 1200, Char 27 Loss: 1.182790994644165\n",
      "Epoch 1, Batch 1200, Char 28 Loss: 2.9068045616149902\n",
      "Epoch 1, Batch 1200, Char 29 Loss: 1.5194772481918335\n",
      "Epoch 1, Batch 1200, Char 30 Loss: 1.499885082244873\n",
      "Epoch 1, Batch 1200, Char 31 Loss: 4.024575233459473\n",
      "Epoch 1, Batch 1200, Char 32 Loss: 3.7481491565704346\n",
      "Epoch 1, Batch 1200, Char 33 Loss: 2.491365909576416\n",
      "Epoch 1, Batch 1200, Char 34 Loss: 1.9252954721450806\n",
      "Epoch 1, Batch 1200, Char 35 Loss: 1.1922119855880737\n",
      "Epoch 1, Batch 1200, Char 36 Loss: 6.258225440979004\n",
      "Epoch 1, Batch 1200, Char 37 Loss: 2.8841607570648193\n",
      "Epoch 1, Batch 1200, Char 38 Loss: 19.48834991455078\n",
      "Epoch 1, Batch 1200, Char 39 Loss: 19.467121124267578\n",
      "Epoch 1, Batch 1200, Char 40 Loss: 2.358945846557617\n",
      "Epoch 1, Batch 1200, Char 41 Loss: 2.8130836486816406\n",
      "Epoch 1, Batch 1200, Char 42 Loss: 1.6371177434921265\n",
      "Epoch 1, Batch 1200, Char 43 Loss: 0.1504562795162201\n",
      "Epoch 1, Batch 1200, Char 44 Loss: 3.071021556854248\n",
      "Epoch 1, Batch 1200, Char 45 Loss: 2.688153028488159\n",
      "Epoch 1, Batch 1200, Char 46 Loss: 3.6878113746643066\n",
      "Epoch 1, Batch 1200, Char 47 Loss: 3.4052257537841797\n",
      "Epoch 1, Batch 1200, Char 48 Loss: 1.9322845935821533\n",
      "Epoch 1, Batch 1200, Char 49 Loss: 1.4086577892303467\n",
      "Epoch 1, Batch 1200, Char 50 Loss: 3.5202105045318604\n",
      "Epoch 1, Batch 1200, Char 51 Loss: 2.2737412452697754\n",
      "Epoch 1, Batch 1200, Char 52 Loss: 3.337874412536621\n",
      "Epoch 1, Batch 1200, Char 53 Loss: 1.053394079208374\n",
      "Epoch 1, Batch 1200, Char 54 Loss: 2.26845383644104\n",
      "Epoch 1, Batch 1200, Char 55 Loss: 1.664937138557434\n",
      "Epoch 1, Batch 1200, Char 56 Loss: 1.5097441673278809\n",
      "Epoch 1, Batch 1200, Char 57 Loss: 0.3500780761241913\n",
      "Epoch 1, Batch 1200, Char 58 Loss: 2.4543144702911377\n",
      "Epoch 1, Batch 1200, Char 59 Loss: 1.703025221824646\n",
      "Epoch 1, Batch 1200, Char 60 Loss: 1.4555490016937256\n",
      "Epoch 1, Batch 1200, Char 61 Loss: 2.0393753051757812\n",
      "Epoch 1, Batch 1200, Char 62 Loss: 1.3831621408462524\n",
      "Epoch 1, Batch 1200, Char 63 Loss: 2.8693931102752686\n",
      "Epoch 1, Batch 1200, Char 64 Loss: 1.8796477317810059\n",
      "Epoch 1, Batch 1200, Char 65 Loss: 3.229823589324951\n",
      "Epoch 1, Batch 1200, Char 66 Loss: 1.5638540983200073\n",
      "Epoch 1, Batch 1200, Char 67 Loss: 1.0379258394241333\n",
      "Epoch 1, Batch 1200, Char 68 Loss: 2.713806629180908\n",
      "Epoch 1, Batch 1200, Char 69 Loss: 1.7517564296722412\n",
      "Epoch 1, Batch 1200, Char 70 Loss: 0.88941490650177\n",
      "Epoch 1, Batch 1200, Char 71 Loss: 2.5587785243988037\n",
      "Epoch 1, Batch 1200, Char 72 Loss: 2.2028086185455322\n",
      "Epoch 1, Batch 1200, Char 73 Loss: 1.3466589450836182\n",
      "Epoch 1, Batch 1200, Char 74 Loss: 2.5458924770355225\n",
      "Epoch 1, Batch 1200, Char 75 Loss: 2.83141827583313\n",
      "Epoch 1, Batch 1200, Char 76 Loss: 1.616072654724121\n",
      "Epoch 1, Batch 1200, Char 77 Loss: 4.214142799377441\n",
      "Epoch 1, Batch 1200, Char 78 Loss: 1.657300591468811\n",
      "Epoch 1, Batch 1200, Char 79 Loss: 1.873795986175537\n",
      "Epoch 1, Batch 1200, Char 80 Loss: 3.7994329929351807\n",
      "Epoch 1, Batch 1200, Char 81 Loss: 2.610640048980713\n",
      "Epoch 1, Batch 1200, Char 82 Loss: 1.9888700246810913\n",
      "Epoch 1, Batch 1200, Char 83 Loss: 2.508477210998535\n",
      "Epoch 1, Batch 1200, Char 84 Loss: 3.3014557361602783\n",
      "Epoch 1, Batch 1200, Char 85 Loss: 1.9667229652404785\n",
      "Epoch 1, Batch 1200, Char 86 Loss: 1.1508049964904785\n",
      "Epoch 1, Batch 1200, Char 87 Loss: 2.9293212890625\n",
      "Epoch 1, Batch 1200, Char 88 Loss: 0.8436877727508545\n",
      "Epoch 1, Batch 1200, Char 89 Loss: 1.0095747709274292\n",
      "Epoch 1, Batch 1200, Char 90 Loss: 2.4735774993896484\n",
      "Epoch 1, Batch 1200, Char 91 Loss: 2.5889766216278076\n",
      "Epoch 1, Batch 1200, Char 92 Loss: 0.8754807114601135\n",
      "Epoch 1, Batch 1200, Char 93 Loss: 2.2482755184173584\n",
      "Epoch 1, Batch 1200, Char 94 Loss: 5.019574165344238\n",
      "Epoch 1, Batch 1200, Char 95 Loss: 3.8791260719299316\n",
      "Epoch 1, Batch 1200, Char 96 Loss: 2.49168062210083\n",
      "Epoch 1, Batch 1200, Char 97 Loss: 1.9046064615249634\n",
      "Epoch 1, Batch 1200, Char 98 Loss: 1.1653224229812622\n",
      "Epoch 1, Batch 1300, Char 0 Loss: 2.502551555633545\n",
      "Epoch 1, Batch 1300, Char 1 Loss: 1.6205952167510986\n",
      "Epoch 1, Batch 1300, Char 2 Loss: 2.6636786460876465\n",
      "Epoch 1, Batch 1300, Char 3 Loss: 0.9952078461647034\n",
      "Epoch 1, Batch 1300, Char 4 Loss: 2.8397774696350098\n",
      "Epoch 1, Batch 1300, Char 5 Loss: 2.768376588821411\n",
      "Epoch 1, Batch 1300, Char 6 Loss: 3.419743061065674\n",
      "Epoch 1, Batch 1300, Char 7 Loss: 2.026477336883545\n",
      "Epoch 1, Batch 1300, Char 8 Loss: 1.093144416809082\n",
      "Epoch 1, Batch 1300, Char 9 Loss: 2.714484214782715\n",
      "Epoch 1, Batch 1300, Char 10 Loss: 2.7629404067993164\n",
      "Epoch 1, Batch 1300, Char 11 Loss: 2.0930819511413574\n",
      "Epoch 1, Batch 1300, Char 12 Loss: 1.4231308698654175\n",
      "Epoch 1, Batch 1300, Char 13 Loss: 2.6342689990997314\n",
      "Epoch 1, Batch 1300, Char 14 Loss: 2.2711427211761475\n",
      "Epoch 1, Batch 1300, Char 15 Loss: 2.462247133255005\n",
      "Epoch 1, Batch 1300, Char 16 Loss: 1.5553463697433472\n",
      "Epoch 1, Batch 1300, Char 17 Loss: 2.5817878246307373\n",
      "Epoch 1, Batch 1300, Char 18 Loss: 0.9730511903762817\n",
      "Epoch 1, Batch 1300, Char 19 Loss: 2.6118435859680176\n",
      "Epoch 1, Batch 1300, Char 20 Loss: 1.2272692918777466\n",
      "Epoch 1, Batch 1300, Char 21 Loss: 1.4469759464263916\n",
      "Epoch 1, Batch 1300, Char 22 Loss: 4.308106422424316\n",
      "Epoch 1, Batch 1300, Char 23 Loss: 2.8909387588500977\n",
      "Epoch 1, Batch 1300, Char 24 Loss: 1.9274441003799438\n",
      "Epoch 1, Batch 1300, Char 25 Loss: 2.7851083278656006\n",
      "Epoch 1, Batch 1300, Char 26 Loss: 4.542815685272217\n",
      "Epoch 1, Batch 1300, Char 27 Loss: 1.953406810760498\n",
      "Epoch 1, Batch 1300, Char 28 Loss: 5.077081680297852\n",
      "Epoch 1, Batch 1300, Char 29 Loss: 3.0630345344543457\n",
      "Epoch 1, Batch 1300, Char 30 Loss: 0.23121388256549835\n",
      "Epoch 1, Batch 1300, Char 31 Loss: 3.3257393836975098\n",
      "Epoch 1, Batch 1300, Char 32 Loss: 1.840054988861084\n",
      "Epoch 1, Batch 1300, Char 33 Loss: 2.165221691131592\n",
      "Epoch 1, Batch 1300, Char 34 Loss: 1.195838451385498\n",
      "Epoch 1, Batch 1300, Char 35 Loss: 2.7475008964538574\n",
      "Epoch 1, Batch 1300, Char 36 Loss: 0.752848207950592\n",
      "Epoch 1, Batch 1300, Char 37 Loss: 1.8819823265075684\n",
      "Epoch 1, Batch 1300, Char 38 Loss: 1.186264991760254\n",
      "Epoch 1, Batch 1300, Char 39 Loss: 1.873984932899475\n",
      "Epoch 1, Batch 1300, Char 40 Loss: 2.21475887298584\n",
      "Epoch 1, Batch 1300, Char 41 Loss: 1.8862299919128418\n",
      "Epoch 1, Batch 1300, Char 42 Loss: 4.026524543762207\n",
      "Epoch 1, Batch 1300, Char 43 Loss: 2.1136951446533203\n",
      "Epoch 1, Batch 1300, Char 44 Loss: 3.620945453643799\n",
      "Epoch 1, Batch 1300, Char 45 Loss: 1.3619886636734009\n",
      "Epoch 1, Batch 1300, Char 46 Loss: 2.421888828277588\n",
      "Epoch 1, Batch 1300, Char 47 Loss: 1.507911205291748\n",
      "Epoch 1, Batch 1300, Char 48 Loss: 3.4811506271362305\n",
      "Epoch 1, Batch 1300, Char 49 Loss: 2.111152172088623\n",
      "Epoch 1, Batch 1300, Char 50 Loss: 2.0547447204589844\n",
      "Epoch 1, Batch 1300, Char 51 Loss: 4.440260410308838\n",
      "Epoch 1, Batch 1300, Char 52 Loss: 2.871276378631592\n",
      "Epoch 1, Batch 1300, Char 53 Loss: 2.802987575531006\n",
      "Epoch 1, Batch 1300, Char 54 Loss: 1.7530325651168823\n",
      "Epoch 1, Batch 1300, Char 55 Loss: 3.069413423538208\n",
      "Epoch 1, Batch 1300, Char 56 Loss: 1.7944302558898926\n",
      "Epoch 1, Batch 1300, Char 57 Loss: 2.0970354080200195\n",
      "Epoch 1, Batch 1300, Char 58 Loss: 1.8548911809921265\n",
      "Epoch 1, Batch 1300, Char 59 Loss: 2.1639556884765625\n",
      "Epoch 1, Batch 1300, Char 60 Loss: 1.8449217081069946\n",
      "Epoch 1, Batch 1300, Char 61 Loss: 1.8449373245239258\n",
      "Epoch 1, Batch 1300, Char 62 Loss: 3.7721989154815674\n",
      "Epoch 1, Batch 1300, Char 63 Loss: 3.1306257247924805\n",
      "Epoch 1, Batch 1300, Char 64 Loss: 3.505059003829956\n",
      "Epoch 1, Batch 1300, Char 65 Loss: 0.08813238143920898\n",
      "Epoch 1, Batch 1300, Char 66 Loss: 3.375809907913208\n",
      "Epoch 1, Batch 1300, Char 67 Loss: 1.998916745185852\n",
      "Epoch 1, Batch 1300, Char 68 Loss: 2.5998849868774414\n",
      "Epoch 1, Batch 1300, Char 69 Loss: 2.1795525550842285\n",
      "Epoch 1, Batch 1300, Char 70 Loss: 2.7070412635803223\n",
      "Epoch 1, Batch 1300, Char 71 Loss: 1.7334293127059937\n",
      "Epoch 1, Batch 1300, Char 72 Loss: 2.8456783294677734\n",
      "Epoch 1, Batch 1300, Char 73 Loss: 0.3414222002029419\n",
      "Epoch 1, Batch 1300, Char 74 Loss: 4.215360164642334\n",
      "Epoch 1, Batch 1300, Char 75 Loss: 2.4811718463897705\n",
      "Epoch 1, Batch 1300, Char 76 Loss: 3.5994319915771484\n",
      "Epoch 1, Batch 1300, Char 77 Loss: 0.08693600445985794\n",
      "Epoch 1, Batch 1300, Char 78 Loss: 1.8331565856933594\n",
      "Epoch 1, Batch 1300, Char 79 Loss: 1.1376640796661377\n",
      "Epoch 1, Batch 1300, Char 80 Loss: 2.6707420349121094\n",
      "Epoch 1, Batch 1300, Char 81 Loss: 1.6956815719604492\n",
      "Epoch 1, Batch 1300, Char 82 Loss: 2.793609857559204\n",
      "Epoch 1, Batch 1300, Char 83 Loss: 0.335416316986084\n",
      "Epoch 1, Batch 1300, Char 84 Loss: 2.1592607498168945\n",
      "Epoch 1, Batch 1300, Char 85 Loss: 1.6866155862808228\n",
      "Epoch 1, Batch 1300, Char 86 Loss: 2.722886562347412\n",
      "Epoch 1, Batch 1300, Char 87 Loss: 2.632004737854004\n",
      "Epoch 1, Batch 1300, Char 88 Loss: 1.0636165142059326\n",
      "Epoch 1, Batch 1300, Char 89 Loss: 0.7778372764587402\n",
      "Epoch 1, Batch 1300, Char 90 Loss: 1.8036863803863525\n",
      "Epoch 1, Batch 1300, Char 91 Loss: 1.1179975271224976\n",
      "Epoch 1, Batch 1300, Char 92 Loss: 5.058734893798828\n",
      "Epoch 1, Batch 1300, Char 93 Loss: 2.822392702102661\n",
      "Epoch 1, Batch 1300, Char 94 Loss: 2.76456356048584\n",
      "Epoch 1, Batch 1300, Char 95 Loss: 2.7419047355651855\n",
      "Epoch 1, Batch 1300, Char 96 Loss: 1.8374329805374146\n",
      "Epoch 1, Batch 1300, Char 97 Loss: 2.1208205223083496\n",
      "Epoch 1, Batch 1300, Char 98 Loss: 2.0982043743133545\n",
      "Epoch 1, Batch 1400, Char 0 Loss: 1.4040759801864624\n",
      "Epoch 1, Batch 1400, Char 1 Loss: 0.3769277334213257\n",
      "Epoch 1, Batch 1400, Char 2 Loss: 3.875545024871826\n",
      "Epoch 1, Batch 1400, Char 3 Loss: 2.7555863857269287\n",
      "Epoch 1, Batch 1400, Char 4 Loss: 2.4432263374328613\n",
      "Epoch 1, Batch 1400, Char 5 Loss: 1.884793996810913\n",
      "Epoch 1, Batch 1400, Char 6 Loss: 1.4514950513839722\n",
      "Epoch 1, Batch 1400, Char 7 Loss: 2.4592230319976807\n",
      "Epoch 1, Batch 1400, Char 8 Loss: 0.3743906021118164\n",
      "Epoch 1, Batch 1400, Char 9 Loss: 2.344993829727173\n",
      "Epoch 1, Batch 1400, Char 10 Loss: 2.8101513385772705\n",
      "Epoch 1, Batch 1400, Char 11 Loss: 2.7006683349609375\n",
      "Epoch 1, Batch 1400, Char 12 Loss: 1.3927048444747925\n",
      "Epoch 1, Batch 1400, Char 13 Loss: 1.217263102531433\n",
      "Epoch 1, Batch 1400, Char 14 Loss: 2.758333683013916\n",
      "Epoch 1, Batch 1400, Char 15 Loss: 2.3620262145996094\n",
      "Epoch 1, Batch 1400, Char 16 Loss: 0.902528703212738\n",
      "Epoch 1, Batch 1400, Char 17 Loss: 2.6639533042907715\n",
      "Epoch 1, Batch 1400, Char 18 Loss: 2.2416043281555176\n",
      "Epoch 1, Batch 1400, Char 19 Loss: 1.488584280014038\n",
      "Epoch 1, Batch 1400, Char 20 Loss: 3.6587836742401123\n",
      "Epoch 1, Batch 1400, Char 21 Loss: 2.248992919921875\n",
      "Epoch 1, Batch 1400, Char 22 Loss: 3.1861062049865723\n",
      "Epoch 1, Batch 1400, Char 23 Loss: 3.7280871868133545\n",
      "Epoch 1, Batch 1400, Char 24 Loss: 2.3142902851104736\n",
      "Epoch 1, Batch 1400, Char 25 Loss: 1.1992243528366089\n",
      "Epoch 1, Batch 1400, Char 26 Loss: 3.1390597820281982\n",
      "Epoch 1, Batch 1400, Char 27 Loss: 2.1312029361724854\n",
      "Epoch 1, Batch 1400, Char 28 Loss: 2.0312821865081787\n",
      "Epoch 1, Batch 1400, Char 29 Loss: 2.64137601852417\n",
      "Epoch 1, Batch 1400, Char 30 Loss: 2.0314955711364746\n",
      "Epoch 1, Batch 1400, Char 31 Loss: 2.7138900756835938\n",
      "Epoch 1, Batch 1400, Char 32 Loss: 0.6870675086975098\n",
      "Epoch 1, Batch 1400, Char 33 Loss: 1.6522868871688843\n",
      "Epoch 1, Batch 1400, Char 34 Loss: 1.3897618055343628\n",
      "Epoch 1, Batch 1400, Char 35 Loss: 2.5367953777313232\n",
      "Epoch 1, Batch 1400, Char 36 Loss: 1.2335224151611328\n",
      "Epoch 1, Batch 1400, Char 37 Loss: 2.5922350883483887\n",
      "Epoch 1, Batch 1400, Char 38 Loss: 4.267407417297363\n",
      "Epoch 1, Batch 1400, Char 39 Loss: 8.418912887573242\n",
      "Epoch 1, Batch 1400, Char 40 Loss: 2.1066884994506836\n",
      "Epoch 1, Batch 1400, Char 41 Loss: 2.4925315380096436\n",
      "Epoch 1, Batch 1400, Char 42 Loss: 2.3572335243225098\n",
      "Epoch 1, Batch 1400, Char 43 Loss: 1.1408770084381104\n",
      "Epoch 1, Batch 1400, Char 44 Loss: 2.153703212738037\n",
      "Epoch 1, Batch 1400, Char 45 Loss: 2.586199998855591\n",
      "Epoch 1, Batch 1400, Char 46 Loss: 2.165951728820801\n",
      "Epoch 1, Batch 1400, Char 47 Loss: 0.964663028717041\n",
      "Epoch 1, Batch 1400, Char 48 Loss: 2.496492385864258\n",
      "Epoch 1, Batch 1400, Char 49 Loss: 6.110073566436768\n",
      "Epoch 1, Batch 1400, Char 50 Loss: 1.9365350008010864\n",
      "Epoch 1, Batch 1400, Char 51 Loss: 4.508068084716797\n",
      "Epoch 1, Batch 1400, Char 52 Loss: 2.4605214595794678\n",
      "Epoch 1, Batch 1400, Char 53 Loss: 2.3401272296905518\n",
      "Epoch 1, Batch 1400, Char 54 Loss: 1.6934796571731567\n",
      "Epoch 1, Batch 1400, Char 55 Loss: 3.4289560317993164\n",
      "Epoch 1, Batch 1400, Char 56 Loss: 2.590130090713501\n",
      "Epoch 1, Batch 1400, Char 57 Loss: 3.508855104446411\n",
      "Epoch 1, Batch 1400, Char 58 Loss: 3.6545987129211426\n",
      "Epoch 1, Batch 1400, Char 59 Loss: 2.4423820972442627\n",
      "Epoch 1, Batch 1400, Char 60 Loss: 0.3484342396259308\n",
      "Epoch 1, Batch 1400, Char 61 Loss: 2.7112438678741455\n",
      "Epoch 1, Batch 1400, Char 62 Loss: 3.9040772914886475\n",
      "Epoch 1, Batch 1400, Char 63 Loss: 0.18591950833797455\n",
      "Epoch 1, Batch 1400, Char 64 Loss: 1.608134150505066\n",
      "Epoch 1, Batch 1400, Char 65 Loss: 1.3437843322753906\n",
      "Epoch 1, Batch 1400, Char 66 Loss: 2.6896579265594482\n",
      "Epoch 1, Batch 1400, Char 67 Loss: 0.6962380409240723\n",
      "Epoch 1, Batch 1400, Char 68 Loss: 1.5914140939712524\n",
      "Epoch 1, Batch 1400, Char 69 Loss: 1.3341610431671143\n",
      "Epoch 1, Batch 1400, Char 70 Loss: 2.110656261444092\n",
      "Epoch 1, Batch 1400, Char 71 Loss: 2.407494068145752\n",
      "Epoch 1, Batch 1400, Char 72 Loss: 4.532644748687744\n",
      "Epoch 1, Batch 1400, Char 73 Loss: 1.913613200187683\n",
      "Epoch 1, Batch 1400, Char 74 Loss: 3.0082616806030273\n",
      "Epoch 1, Batch 1400, Char 75 Loss: 1.984824776649475\n",
      "Epoch 1, Batch 1400, Char 76 Loss: 2.361745834350586\n",
      "Epoch 1, Batch 1400, Char 77 Loss: 1.704116702079773\n",
      "Epoch 1, Batch 1400, Char 78 Loss: 1.1938292980194092\n",
      "Epoch 1, Batch 1400, Char 79 Loss: 2.0871942043304443\n",
      "Epoch 1, Batch 1400, Char 80 Loss: 2.6997499465942383\n",
      "Epoch 1, Batch 1400, Char 81 Loss: 11.01784610748291\n",
      "Epoch 1, Batch 1400, Char 82 Loss: 2.2579309940338135\n",
      "Epoch 1, Batch 1400, Char 83 Loss: 3.6402478218078613\n",
      "Epoch 1, Batch 1400, Char 84 Loss: 1.655540943145752\n",
      "Epoch 1, Batch 1400, Char 85 Loss: 1.4357362985610962\n",
      "Epoch 1, Batch 1400, Char 86 Loss: 2.3582208156585693\n",
      "Epoch 1, Batch 1400, Char 87 Loss: 2.036135196685791\n",
      "Epoch 1, Batch 1400, Char 88 Loss: 2.332146644592285\n",
      "Epoch 1, Batch 1400, Char 89 Loss: 2.344025135040283\n",
      "Epoch 1, Batch 1400, Char 90 Loss: 1.1360561847686768\n",
      "Epoch 1, Batch 1400, Char 91 Loss: 0.6848758459091187\n",
      "Epoch 1, Batch 1400, Char 92 Loss: 1.180956244468689\n",
      "Epoch 1, Batch 1400, Char 93 Loss: 2.340183973312378\n",
      "Epoch 1, Batch 1400, Char 94 Loss: 2.2534291744232178\n",
      "Epoch 1, Batch 1400, Char 95 Loss: 2.1091437339782715\n",
      "Epoch 1, Batch 1400, Char 96 Loss: 1.8512967824935913\n",
      "Epoch 1, Batch 1400, Char 97 Loss: 2.3368914127349854\n",
      "Epoch 1, Batch 1400, Char 98 Loss: 2.5502982139587402\n",
      "Epoch 1, Batch 1500, Char 0 Loss: 1.323447823524475\n",
      "Epoch 1, Batch 1500, Char 1 Loss: 0.750993549823761\n",
      "Epoch 1, Batch 1500, Char 2 Loss: 1.0219721794128418\n",
      "Epoch 1, Batch 1500, Char 3 Loss: 3.3700156211853027\n",
      "Epoch 1, Batch 1500, Char 4 Loss: 3.2318272590637207\n",
      "Epoch 1, Batch 1500, Char 5 Loss: 2.5256261825561523\n",
      "Epoch 1, Batch 1500, Char 6 Loss: 1.9981328248977661\n",
      "Epoch 1, Batch 1500, Char 7 Loss: 1.015472650527954\n",
      "Epoch 1, Batch 1500, Char 8 Loss: 2.2125697135925293\n",
      "Epoch 1, Batch 1500, Char 9 Loss: 1.308627724647522\n",
      "Epoch 1, Batch 1500, Char 10 Loss: 1.8818984031677246\n",
      "Epoch 1, Batch 1500, Char 11 Loss: 2.001736640930176\n",
      "Epoch 1, Batch 1500, Char 12 Loss: 1.3781927824020386\n",
      "Epoch 1, Batch 1500, Char 13 Loss: 3.4675676822662354\n",
      "Epoch 1, Batch 1500, Char 14 Loss: 1.9602738618850708\n",
      "Epoch 1, Batch 1500, Char 15 Loss: 3.915508270263672\n",
      "Epoch 1, Batch 1500, Char 16 Loss: 1.372237205505371\n",
      "Epoch 1, Batch 1500, Char 17 Loss: 4.431183815002441\n",
      "Epoch 1, Batch 1500, Char 18 Loss: 1.8378105163574219\n",
      "Epoch 1, Batch 1500, Char 19 Loss: 1.1432006359100342\n",
      "Epoch 1, Batch 1500, Char 20 Loss: 2.3652093410491943\n",
      "Epoch 1, Batch 1500, Char 21 Loss: 2.0304408073425293\n",
      "Epoch 1, Batch 1500, Char 22 Loss: 1.186094045639038\n",
      "Epoch 1, Batch 1500, Char 23 Loss: 3.931216239929199\n",
      "Epoch 1, Batch 1500, Char 24 Loss: 0.6694156527519226\n",
      "Epoch 1, Batch 1500, Char 25 Loss: 3.9558513164520264\n",
      "Epoch 1, Batch 1500, Char 26 Loss: 3.7773666381835938\n",
      "Epoch 1, Batch 1500, Char 27 Loss: 3.287754535675049\n",
      "Epoch 1, Batch 1500, Char 28 Loss: 3.11763334274292\n",
      "Epoch 1, Batch 1500, Char 29 Loss: 1.4312775135040283\n",
      "Epoch 1, Batch 1500, Char 30 Loss: 4.394695281982422\n",
      "Epoch 1, Batch 1500, Char 31 Loss: 4.023715496063232\n",
      "Epoch 1, Batch 1500, Char 32 Loss: 0.24652518332004547\n",
      "Epoch 1, Batch 1500, Char 33 Loss: 1.9272751808166504\n",
      "Epoch 1, Batch 1500, Char 34 Loss: 1.456809401512146\n",
      "Epoch 1, Batch 1500, Char 35 Loss: 2.3265292644500732\n",
      "Epoch 1, Batch 1500, Char 36 Loss: 2.9846153259277344\n",
      "Epoch 1, Batch 1500, Char 37 Loss: 2.2397027015686035\n",
      "Epoch 1, Batch 1500, Char 38 Loss: 3.2333500385284424\n",
      "Epoch 1, Batch 1500, Char 39 Loss: 2.42855167388916\n",
      "Epoch 1, Batch 1500, Char 40 Loss: 2.0430097579956055\n",
      "Epoch 1, Batch 1500, Char 41 Loss: 1.433614730834961\n",
      "Epoch 1, Batch 1500, Char 42 Loss: 2.23770809173584\n",
      "Epoch 1, Batch 1500, Char 43 Loss: 1.5449764728546143\n",
      "Epoch 1, Batch 1500, Char 44 Loss: 1.5582859516143799\n",
      "Epoch 1, Batch 1500, Char 45 Loss: 0.26034480333328247\n",
      "Epoch 1, Batch 1500, Char 46 Loss: 3.3135013580322266\n",
      "Epoch 1, Batch 1500, Char 47 Loss: 2.5334291458129883\n",
      "Epoch 1, Batch 1500, Char 48 Loss: 1.7785447835922241\n",
      "Epoch 1, Batch 1500, Char 49 Loss: 2.0366578102111816\n",
      "Epoch 1, Batch 1500, Char 50 Loss: 2.251469135284424\n",
      "Epoch 1, Batch 1500, Char 51 Loss: 1.152300238609314\n",
      "Epoch 1, Batch 1500, Char 52 Loss: 1.8309569358825684\n",
      "Epoch 1, Batch 1500, Char 53 Loss: 0.7866169214248657\n",
      "Epoch 1, Batch 1500, Char 54 Loss: 2.2002129554748535\n",
      "Epoch 1, Batch 1500, Char 55 Loss: 1.3173054456710815\n",
      "Epoch 1, Batch 1500, Char 56 Loss: 0.7488993406295776\n",
      "Epoch 1, Batch 1500, Char 57 Loss: 1.000058889389038\n",
      "Epoch 1, Batch 1500, Char 58 Loss: 3.169139862060547\n",
      "Epoch 1, Batch 1500, Char 59 Loss: 2.275930643081665\n",
      "Epoch 1, Batch 1500, Char 60 Loss: 2.075161933898926\n",
      "Epoch 1, Batch 1500, Char 61 Loss: 1.5076583623886108\n",
      "Epoch 1, Batch 1500, Char 62 Loss: 3.779189109802246\n",
      "Epoch 1, Batch 1500, Char 63 Loss: 1.4935331344604492\n",
      "Epoch 1, Batch 1500, Char 64 Loss: 3.8404510021209717\n",
      "Epoch 1, Batch 1500, Char 65 Loss: 4.179851055145264\n",
      "Epoch 1, Batch 1500, Char 66 Loss: 1.123236060142517\n",
      "Epoch 1, Batch 1500, Char 67 Loss: 2.7733898162841797\n",
      "Epoch 1, Batch 1500, Char 68 Loss: 4.009513854980469\n",
      "Epoch 1, Batch 1500, Char 69 Loss: 0.23187927901744843\n",
      "Epoch 1, Batch 1500, Char 70 Loss: 1.8996295928955078\n",
      "Epoch 1, Batch 1500, Char 71 Loss: 1.3753833770751953\n",
      "Epoch 1, Batch 1500, Char 72 Loss: 4.3087687492370605\n",
      "Epoch 1, Batch 1500, Char 73 Loss: 1.7855778932571411\n",
      "Epoch 1, Batch 1500, Char 74 Loss: 1.1098603010177612\n",
      "Epoch 1, Batch 1500, Char 75 Loss: 3.266845464706421\n",
      "Epoch 1, Batch 1500, Char 76 Loss: 1.9014551639556885\n",
      "Epoch 1, Batch 1500, Char 77 Loss: 4.193861484527588\n",
      "Epoch 1, Batch 1500, Char 78 Loss: 1.7427489757537842\n",
      "Epoch 1, Batch 1500, Char 79 Loss: 1.1157855987548828\n",
      "Epoch 1, Batch 1500, Char 80 Loss: 1.7775578498840332\n",
      "Epoch 1, Batch 1500, Char 81 Loss: 0.7571669220924377\n",
      "Epoch 1, Batch 1500, Char 82 Loss: 3.0905544757843018\n",
      "Epoch 1, Batch 1500, Char 83 Loss: 1.209667444229126\n",
      "Epoch 1, Batch 1500, Char 84 Loss: 3.097259759902954\n",
      "Epoch 1, Batch 1500, Char 85 Loss: 2.2039928436279297\n",
      "Epoch 1, Batch 1500, Char 86 Loss: 3.672874927520752\n",
      "Epoch 1, Batch 1500, Char 87 Loss: 3.832937717437744\n",
      "Epoch 1, Batch 1500, Char 88 Loss: 0.22163264453411102\n",
      "Epoch 1, Batch 1500, Char 89 Loss: 1.008341670036316\n",
      "Epoch 1, Batch 1500, Char 90 Loss: 2.1882011890411377\n",
      "Epoch 1, Batch 1500, Char 91 Loss: 1.2932653427124023\n",
      "Epoch 1, Batch 1500, Char 92 Loss: 1.8423197269439697\n",
      "Epoch 1, Batch 1500, Char 93 Loss: 1.9844237565994263\n",
      "Epoch 1, Batch 1500, Char 94 Loss: 1.3313076496124268\n",
      "Epoch 1, Batch 1500, Char 95 Loss: 2.626983642578125\n",
      "Epoch 1, Batch 1500, Char 96 Loss: 1.8665831089019775\n",
      "Epoch 1, Batch 1500, Char 97 Loss: 1.003540277481079\n",
      "Epoch 1, Batch 1500, Char 98 Loss: 2.6086270809173584\n",
      "Epoch 1, Batch 1600, Char 0 Loss: 1.340333104133606\n",
      "Epoch 1, Batch 1600, Char 1 Loss: 1.7602955102920532\n",
      "Epoch 1, Batch 1600, Char 2 Loss: 0.7684134840965271\n",
      "Epoch 1, Batch 1600, Char 3 Loss: 2.345109224319458\n",
      "Epoch 1, Batch 1600, Char 4 Loss: 1.328355073928833\n",
      "Epoch 1, Batch 1600, Char 5 Loss: 3.2800991535186768\n",
      "Epoch 1, Batch 1600, Char 6 Loss: 1.6381984949111938\n",
      "Epoch 1, Batch 1600, Char 7 Loss: 2.083181858062744\n",
      "Epoch 1, Batch 1600, Char 8 Loss: 3.2412400245666504\n",
      "Epoch 1, Batch 1600, Char 9 Loss: 2.055262565612793\n",
      "Epoch 1, Batch 1600, Char 10 Loss: 4.373652458190918\n",
      "Epoch 1, Batch 1600, Char 11 Loss: 3.955490827560425\n",
      "Epoch 1, Batch 1600, Char 12 Loss: 4.42840576171875\n",
      "Epoch 1, Batch 1600, Char 13 Loss: 3.6323630809783936\n",
      "Epoch 1, Batch 1600, Char 14 Loss: 2.1498560905456543\n",
      "Epoch 1, Batch 1600, Char 15 Loss: 1.8204597234725952\n",
      "Epoch 1, Batch 1600, Char 16 Loss: 0.9699527025222778\n",
      "Epoch 1, Batch 1600, Char 17 Loss: 2.9461135864257812\n",
      "Epoch 1, Batch 1600, Char 18 Loss: 2.618706464767456\n",
      "Epoch 1, Batch 1600, Char 19 Loss: 0.3258430063724518\n",
      "Epoch 1, Batch 1600, Char 20 Loss: 2.2257142066955566\n",
      "Epoch 1, Batch 1600, Char 21 Loss: 0.8599227666854858\n",
      "Epoch 1, Batch 1600, Char 22 Loss: 0.7871472835540771\n",
      "Epoch 1, Batch 1600, Char 23 Loss: 0.9617307186126709\n",
      "Epoch 1, Batch 1600, Char 24 Loss: 2.7317006587982178\n",
      "Epoch 1, Batch 1600, Char 25 Loss: 4.003246307373047\n",
      "Epoch 1, Batch 1600, Char 26 Loss: 3.673654317855835\n",
      "Epoch 1, Batch 1600, Char 27 Loss: 2.019378662109375\n",
      "Epoch 1, Batch 1600, Char 28 Loss: 1.6370415687561035\n",
      "Epoch 1, Batch 1600, Char 29 Loss: 1.447313666343689\n",
      "Epoch 1, Batch 1600, Char 30 Loss: 2.3226842880249023\n",
      "Epoch 1, Batch 1600, Char 31 Loss: 3.6556921005249023\n",
      "Epoch 1, Batch 1600, Char 32 Loss: 1.9807509183883667\n",
      "Epoch 1, Batch 1600, Char 33 Loss: 4.714334011077881\n",
      "Epoch 1, Batch 1600, Char 34 Loss: 2.627668857574463\n",
      "Epoch 1, Batch 1600, Char 35 Loss: 1.2641876935958862\n",
      "Epoch 1, Batch 1600, Char 36 Loss: 4.091684341430664\n",
      "Epoch 1, Batch 1600, Char 37 Loss: 2.398761749267578\n",
      "Epoch 1, Batch 1600, Char 38 Loss: 2.6248912811279297\n",
      "Epoch 1, Batch 1600, Char 39 Loss: 3.240522623062134\n",
      "Epoch 1, Batch 1600, Char 40 Loss: 2.0017337799072266\n",
      "Epoch 1, Batch 1600, Char 41 Loss: 1.4238256216049194\n",
      "Epoch 1, Batch 1600, Char 42 Loss: 2.6878304481506348\n",
      "Epoch 1, Batch 1600, Char 43 Loss: 0.7909912467002869\n",
      "Epoch 1, Batch 1600, Char 44 Loss: 0.936235249042511\n",
      "Epoch 1, Batch 1600, Char 45 Loss: 2.9110748767852783\n",
      "Epoch 1, Batch 1600, Char 46 Loss: 1.4729273319244385\n",
      "Epoch 1, Batch 1600, Char 47 Loss: 1.6560328006744385\n",
      "Epoch 1, Batch 1600, Char 48 Loss: 2.20613956451416\n",
      "Epoch 1, Batch 1600, Char 49 Loss: 2.267754077911377\n",
      "Epoch 1, Batch 1600, Char 50 Loss: 2.628044843673706\n",
      "Epoch 1, Batch 1600, Char 51 Loss: 0.3361079692840576\n",
      "Epoch 1, Batch 1600, Char 52 Loss: 2.642179012298584\n",
      "Epoch 1, Batch 1600, Char 53 Loss: 2.0342843532562256\n",
      "Epoch 1, Batch 1600, Char 54 Loss: 3.5204131603240967\n",
      "Epoch 1, Batch 1600, Char 55 Loss: 4.098536014556885\n",
      "Epoch 1, Batch 1600, Char 56 Loss: 2.2332282066345215\n",
      "Epoch 1, Batch 1600, Char 57 Loss: 3.6281516551971436\n",
      "Epoch 1, Batch 1600, Char 58 Loss: 4.5041093826293945\n",
      "Epoch 1, Batch 1600, Char 59 Loss: 0.9500332474708557\n",
      "Epoch 1, Batch 1600, Char 60 Loss: 3.259561777114868\n",
      "Epoch 1, Batch 1600, Char 61 Loss: 2.0774707794189453\n",
      "Epoch 1, Batch 1600, Char 62 Loss: 2.7141337394714355\n",
      "Epoch 1, Batch 1600, Char 63 Loss: 1.6039037704467773\n",
      "Epoch 1, Batch 1600, Char 64 Loss: 2.7344846725463867\n",
      "Epoch 1, Batch 1600, Char 65 Loss: 2.5454652309417725\n",
      "Epoch 1, Batch 1600, Char 66 Loss: 3.4363338947296143\n",
      "Epoch 1, Batch 1600, Char 67 Loss: 1.7267963886260986\n",
      "Epoch 1, Batch 1600, Char 68 Loss: 2.5827975273132324\n",
      "Epoch 1, Batch 1600, Char 69 Loss: 1.7740600109100342\n",
      "Epoch 1, Batch 1600, Char 70 Loss: 2.3615965843200684\n",
      "Epoch 1, Batch 1600, Char 71 Loss: 0.31544312834739685\n",
      "Epoch 1, Batch 1600, Char 72 Loss: 1.9771449565887451\n",
      "Epoch 1, Batch 1600, Char 73 Loss: 2.059985637664795\n",
      "Epoch 1, Batch 1600, Char 74 Loss: 2.199313163757324\n",
      "Epoch 1, Batch 1600, Char 75 Loss: 1.5022001266479492\n",
      "Epoch 1, Batch 1600, Char 76 Loss: 2.0333213806152344\n",
      "Epoch 1, Batch 1600, Char 77 Loss: 1.6280934810638428\n",
      "Epoch 1, Batch 1600, Char 78 Loss: 0.33753952383995056\n",
      "Epoch 1, Batch 1600, Char 79 Loss: 2.242830753326416\n",
      "Epoch 1, Batch 1600, Char 80 Loss: 0.8194408416748047\n",
      "Epoch 1, Batch 1600, Char 81 Loss: 0.8135790824890137\n",
      "Epoch 1, Batch 1600, Char 82 Loss: 0.9566704630851746\n",
      "Epoch 1, Batch 1600, Char 83 Loss: 2.844202756881714\n",
      "Epoch 1, Batch 1600, Char 84 Loss: 1.6490429639816284\n",
      "Epoch 1, Batch 1600, Char 85 Loss: 4.619971752166748\n",
      "Epoch 1, Batch 1600, Char 86 Loss: 2.8682548999786377\n",
      "Epoch 1, Batch 1600, Char 87 Loss: 0.9511303305625916\n",
      "Epoch 1, Batch 1600, Char 88 Loss: 4.133482933044434\n",
      "Epoch 1, Batch 1600, Char 89 Loss: 1.6720771789550781\n",
      "Epoch 1, Batch 1600, Char 90 Loss: 10.989029884338379\n",
      "Epoch 1, Batch 1600, Char 91 Loss: 3.2475390434265137\n",
      "Epoch 1, Batch 1600, Char 92 Loss: 2.0187556743621826\n",
      "Epoch 1, Batch 1600, Char 93 Loss: 4.217113018035889\n",
      "Epoch 1, Batch 1600, Char 94 Loss: 5.142401218414307\n",
      "Epoch 1, Batch 1600, Char 95 Loss: 1.8000987768173218\n",
      "Epoch 1, Batch 1600, Char 96 Loss: 2.621269464492798\n",
      "Epoch 1, Batch 1600, Char 97 Loss: 3.200061798095703\n",
      "Epoch 1, Batch 1600, Char 98 Loss: 0.9286868572235107\n",
      "Epoch 1, Batch 1700, Char 0 Loss: 3.236941337585449\n",
      "Epoch 1, Batch 1700, Char 1 Loss: 0.9908668994903564\n",
      "Epoch 1, Batch 1700, Char 2 Loss: 4.339678764343262\n",
      "Epoch 1, Batch 1700, Char 3 Loss: 2.0342302322387695\n",
      "Epoch 1, Batch 1700, Char 4 Loss: 3.039491653442383\n",
      "Epoch 1, Batch 1700, Char 5 Loss: 4.002800464630127\n",
      "Epoch 1, Batch 1700, Char 6 Loss: 0.23836186528205872\n",
      "Epoch 1, Batch 1700, Char 7 Loss: 0.9008394479751587\n",
      "Epoch 1, Batch 1700, Char 8 Loss: 3.193091869354248\n",
      "Epoch 1, Batch 1700, Char 9 Loss: 2.1154379844665527\n",
      "Epoch 1, Batch 1700, Char 10 Loss: 2.2562336921691895\n",
      "Epoch 1, Batch 1700, Char 11 Loss: 2.8931989669799805\n",
      "Epoch 1, Batch 1700, Char 12 Loss: 2.4971351623535156\n",
      "Epoch 1, Batch 1700, Char 13 Loss: 3.8804409503936768\n",
      "Epoch 1, Batch 1700, Char 14 Loss: 1.3409870862960815\n",
      "Epoch 1, Batch 1700, Char 15 Loss: 2.2872869968414307\n",
      "Epoch 1, Batch 1700, Char 16 Loss: 1.5217233896255493\n",
      "Epoch 1, Batch 1700, Char 17 Loss: 2.94472074508667\n",
      "Epoch 1, Batch 1700, Char 18 Loss: 1.8676302433013916\n",
      "Epoch 1, Batch 1700, Char 19 Loss: 3.7966527938842773\n",
      "Epoch 1, Batch 1700, Char 20 Loss: 0.2344481647014618\n",
      "Epoch 1, Batch 1700, Char 21 Loss: 0.9050412178039551\n",
      "Epoch 1, Batch 1700, Char 22 Loss: 3.047288417816162\n",
      "Epoch 1, Batch 1700, Char 23 Loss: 2.3130574226379395\n",
      "Epoch 1, Batch 1700, Char 24 Loss: 2.4001951217651367\n",
      "Epoch 1, Batch 1700, Char 25 Loss: 1.3188440799713135\n",
      "Epoch 1, Batch 1700, Char 26 Loss: 0.9009506702423096\n",
      "Epoch 1, Batch 1700, Char 27 Loss: 2.7981488704681396\n",
      "Epoch 1, Batch 1700, Char 28 Loss: 2.3953816890716553\n",
      "Epoch 1, Batch 1700, Char 29 Loss: 4.854434967041016\n",
      "Epoch 1, Batch 1700, Char 30 Loss: 3.710099220275879\n",
      "Epoch 1, Batch 1700, Char 31 Loss: 1.2921704053878784\n",
      "Epoch 1, Batch 1700, Char 32 Loss: 3.545719623565674\n",
      "Epoch 1, Batch 1700, Char 33 Loss: 4.3316144943237305\n",
      "Epoch 1, Batch 1700, Char 34 Loss: 0.7278147339820862\n",
      "Epoch 1, Batch 1700, Char 35 Loss: 1.8674029111862183\n",
      "Epoch 1, Batch 1700, Char 36 Loss: 2.175175666809082\n",
      "Epoch 1, Batch 1700, Char 37 Loss: 1.9800339937210083\n",
      "Epoch 1, Batch 1700, Char 38 Loss: 3.8879642486572266\n",
      "Epoch 1, Batch 1700, Char 39 Loss: 7.09935188293457\n",
      "Epoch 1, Batch 1700, Char 40 Loss: 3.5420472621917725\n",
      "Epoch 1, Batch 1700, Char 41 Loss: 2.5644237995147705\n",
      "Epoch 1, Batch 1700, Char 42 Loss: 3.480602264404297\n",
      "Epoch 1, Batch 1700, Char 43 Loss: 0.4367372393608093\n",
      "Epoch 1, Batch 1700, Char 44 Loss: 1.8478317260742188\n",
      "Epoch 1, Batch 1700, Char 45 Loss: 1.0029689073562622\n",
      "Epoch 1, Batch 1700, Char 46 Loss: 1.816742181777954\n",
      "Epoch 1, Batch 1700, Char 47 Loss: 1.4437568187713623\n",
      "Epoch 1, Batch 1700, Char 48 Loss: 1.5006412267684937\n",
      "Epoch 1, Batch 1700, Char 49 Loss: 2.1738288402557373\n",
      "Epoch 1, Batch 1700, Char 50 Loss: 2.791870594024658\n",
      "Epoch 1, Batch 1700, Char 51 Loss: 6.582528591156006\n",
      "Epoch 1, Batch 1700, Char 52 Loss: 1.8690356016159058\n",
      "Epoch 1, Batch 1700, Char 53 Loss: 4.98961067199707\n",
      "Epoch 1, Batch 1700, Char 54 Loss: 4.235469818115234\n",
      "Epoch 1, Batch 1700, Char 55 Loss: 0.7234230637550354\n",
      "Epoch 1, Batch 1700, Char 56 Loss: 4.1298675537109375\n",
      "Epoch 1, Batch 1700, Char 57 Loss: 4.397027969360352\n",
      "Epoch 1, Batch 1700, Char 58 Loss: 0.21504725515842438\n",
      "Epoch 1, Batch 1700, Char 59 Loss: 2.067322015762329\n",
      "Epoch 1, Batch 1700, Char 60 Loss: 1.6073187589645386\n",
      "Epoch 1, Batch 1700, Char 61 Loss: 1.816418170928955\n",
      "Epoch 1, Batch 1700, Char 62 Loss: 0.9950637221336365\n",
      "Epoch 1, Batch 1700, Char 63 Loss: 2.200472116470337\n",
      "Epoch 1, Batch 1700, Char 64 Loss: 0.9922761917114258\n",
      "Epoch 1, Batch 1700, Char 65 Loss: 4.58172607421875\n",
      "Epoch 1, Batch 1700, Char 66 Loss: 0.8636810183525085\n",
      "Epoch 1, Batch 1700, Char 67 Loss: 2.8786802291870117\n",
      "Epoch 1, Batch 1700, Char 68 Loss: 1.9687448740005493\n",
      "Epoch 1, Batch 1700, Char 69 Loss: 0.9392015337944031\n",
      "Epoch 1, Batch 1700, Char 70 Loss: 2.913275718688965\n",
      "Epoch 1, Batch 1700, Char 71 Loss: 1.7328248023986816\n",
      "Epoch 1, Batch 1700, Char 72 Loss: 2.4944968223571777\n",
      "Epoch 1, Batch 1700, Char 73 Loss: 8.146885871887207\n",
      "Epoch 1, Batch 1700, Char 74 Loss: 2.605710506439209\n",
      "Epoch 1, Batch 1700, Char 75 Loss: 2.304354429244995\n",
      "Epoch 1, Batch 1700, Char 76 Loss: 2.57498836517334\n",
      "Epoch 1, Batch 1700, Char 77 Loss: 0.9742984771728516\n",
      "Epoch 1, Batch 1700, Char 78 Loss: 1.9016085863113403\n",
      "Epoch 1, Batch 1700, Char 79 Loss: 0.9282474517822266\n",
      "Epoch 1, Batch 1700, Char 80 Loss: 2.682903528213501\n",
      "Epoch 1, Batch 1700, Char 81 Loss: 2.5940685272216797\n",
      "Epoch 1, Batch 1700, Char 82 Loss: 1.8029175996780396\n",
      "Epoch 1, Batch 1700, Char 83 Loss: 2.136613130569458\n",
      "Epoch 1, Batch 1700, Char 84 Loss: 3.1780295372009277\n",
      "Epoch 1, Batch 1700, Char 85 Loss: 4.182399749755859\n",
      "Epoch 1, Batch 1700, Char 86 Loss: 0.791942298412323\n",
      "Epoch 1, Batch 1700, Char 87 Loss: 2.9274954795837402\n",
      "Epoch 1, Batch 1700, Char 88 Loss: 2.102849245071411\n",
      "Epoch 1, Batch 1700, Char 89 Loss: 2.998239517211914\n",
      "Epoch 1, Batch 1700, Char 90 Loss: 1.4401483535766602\n",
      "Epoch 1, Batch 1700, Char 91 Loss: 4.363137722015381\n",
      "Epoch 1, Batch 1700, Char 92 Loss: 0.7593012452125549\n",
      "Epoch 1, Batch 1700, Char 93 Loss: 1.7924669981002808\n",
      "Epoch 1, Batch 1700, Char 94 Loss: 2.0782976150512695\n",
      "Epoch 1, Batch 1700, Char 95 Loss: 1.9636965990066528\n",
      "Epoch 1, Batch 1700, Char 96 Loss: 3.0527853965759277\n",
      "Epoch 1, Batch 1700, Char 97 Loss: 1.194137454032898\n",
      "Epoch 1, Batch 1700, Char 98 Loss: 2.424440622329712\n",
      "Epoch 1, Batch 1800, Char 0 Loss: 1.960557460784912\n",
      "Epoch 1, Batch 1800, Char 1 Loss: 1.3754115104675293\n",
      "Epoch 1, Batch 1800, Char 2 Loss: 4.040355682373047\n",
      "Epoch 1, Batch 1800, Char 3 Loss: 3.2287399768829346\n",
      "Epoch 1, Batch 1800, Char 4 Loss: 3.595726728439331\n",
      "Epoch 1, Batch 1800, Char 5 Loss: 4.101425647735596\n",
      "Epoch 1, Batch 1800, Char 6 Loss: 2.854417324066162\n",
      "Epoch 1, Batch 1800, Char 7 Loss: 2.757768392562866\n",
      "Epoch 1, Batch 1800, Char 8 Loss: 2.721769332885742\n",
      "Epoch 1, Batch 1800, Char 9 Loss: 1.8437349796295166\n",
      "Epoch 1, Batch 1800, Char 10 Loss: 2.8565168380737305\n",
      "Epoch 1, Batch 1800, Char 11 Loss: 0.3721127212047577\n",
      "Epoch 1, Batch 1800, Char 12 Loss: 3.810530424118042\n",
      "Epoch 1, Batch 1800, Char 13 Loss: 2.9868507385253906\n",
      "Epoch 1, Batch 1800, Char 14 Loss: 2.0088915824890137\n",
      "Epoch 1, Batch 1800, Char 15 Loss: 2.558370590209961\n",
      "Epoch 1, Batch 1800, Char 16 Loss: 1.0188757181167603\n",
      "Epoch 1, Batch 1800, Char 17 Loss: 4.480490207672119\n",
      "Epoch 1, Batch 1800, Char 18 Loss: 2.567324161529541\n",
      "Epoch 1, Batch 1800, Char 19 Loss: 5.715451240539551\n",
      "Epoch 1, Batch 1800, Char 20 Loss: 1.7775119543075562\n",
      "Epoch 1, Batch 1800, Char 21 Loss: 3.321890354156494\n",
      "Epoch 1, Batch 1800, Char 22 Loss: 3.087757110595703\n",
      "Epoch 1, Batch 1800, Char 23 Loss: 3.1830782890319824\n",
      "Epoch 1, Batch 1800, Char 24 Loss: 3.076289653778076\n",
      "Epoch 1, Batch 1800, Char 25 Loss: 0.9514562487602234\n",
      "Epoch 1, Batch 1800, Char 26 Loss: 1.998516321182251\n",
      "Epoch 1, Batch 1800, Char 27 Loss: 2.1543128490448\n",
      "Epoch 1, Batch 1800, Char 28 Loss: 1.3644856214523315\n",
      "Epoch 1, Batch 1800, Char 29 Loss: 1.8622863292694092\n",
      "Epoch 1, Batch 1800, Char 30 Loss: 1.0215017795562744\n",
      "Epoch 1, Batch 1800, Char 31 Loss: 0.6117390990257263\n",
      "Epoch 1, Batch 1800, Char 32 Loss: 1.010870099067688\n",
      "Epoch 1, Batch 1800, Char 33 Loss: 2.8543291091918945\n",
      "Epoch 1, Batch 1800, Char 34 Loss: 2.7517378330230713\n",
      "Epoch 1, Batch 1800, Char 35 Loss: 3.756150245666504\n",
      "Epoch 1, Batch 1800, Char 36 Loss: 1.3614290952682495\n",
      "Epoch 1, Batch 1800, Char 37 Loss: 1.0020023584365845\n",
      "Epoch 1, Batch 1800, Char 38 Loss: 1.8434046506881714\n",
      "Epoch 1, Batch 1800, Char 39 Loss: 2.8858485221862793\n",
      "Epoch 1, Batch 1800, Char 40 Loss: 3.298389434814453\n",
      "Epoch 1, Batch 1800, Char 41 Loss: 1.337201476097107\n",
      "Epoch 1, Batch 1800, Char 42 Loss: 0.9896453619003296\n",
      "Epoch 1, Batch 1800, Char 43 Loss: 2.677999973297119\n",
      "Epoch 1, Batch 1800, Char 44 Loss: 2.372382640838623\n",
      "Epoch 1, Batch 1800, Char 45 Loss: 2.816105842590332\n",
      "Epoch 1, Batch 1800, Char 46 Loss: 2.691879987716675\n",
      "Epoch 1, Batch 1800, Char 47 Loss: 4.840250015258789\n",
      "Epoch 1, Batch 1800, Char 48 Loss: 2.6733345985412598\n",
      "Epoch 1, Batch 1800, Char 49 Loss: 2.719675064086914\n",
      "Epoch 1, Batch 1800, Char 50 Loss: 0.6011672616004944\n",
      "Epoch 1, Batch 1800, Char 51 Loss: 1.9998830556869507\n",
      "Epoch 1, Batch 1800, Char 52 Loss: 1.5774105787277222\n",
      "Epoch 1, Batch 1800, Char 53 Loss: 3.8353991508483887\n",
      "Epoch 1, Batch 1800, Char 54 Loss: 2.3344602584838867\n",
      "Epoch 1, Batch 1800, Char 55 Loss: 4.0394062995910645\n",
      "Epoch 1, Batch 1800, Char 56 Loss: 1.258430004119873\n",
      "Epoch 1, Batch 1800, Char 57 Loss: 2.0977864265441895\n",
      "Epoch 1, Batch 1800, Char 58 Loss: 1.1345194578170776\n",
      "Epoch 1, Batch 1800, Char 59 Loss: 4.383638381958008\n",
      "Epoch 1, Batch 1800, Char 60 Loss: 2.4494872093200684\n",
      "Epoch 1, Batch 1800, Char 61 Loss: 2.0291707515716553\n",
      "Epoch 1, Batch 1800, Char 62 Loss: 1.9947640895843506\n",
      "Epoch 1, Batch 1800, Char 63 Loss: 1.109885334968567\n",
      "Epoch 1, Batch 1800, Char 64 Loss: 1.840234637260437\n",
      "Epoch 1, Batch 1800, Char 65 Loss: 1.0377410650253296\n",
      "Epoch 1, Batch 1800, Char 66 Loss: 0.5924192070960999\n",
      "Epoch 1, Batch 1800, Char 67 Loss: 0.9534158706665039\n",
      "Epoch 1, Batch 1800, Char 68 Loss: 3.575247287750244\n",
      "Epoch 1, Batch 1800, Char 69 Loss: 2.269753932952881\n",
      "Epoch 1, Batch 1800, Char 70 Loss: 2.880817174911499\n",
      "Epoch 1, Batch 1800, Char 71 Loss: 3.155648946762085\n",
      "Epoch 1, Batch 1800, Char 72 Loss: 1.3383365869522095\n",
      "Epoch 1, Batch 1800, Char 73 Loss: 2.756277561187744\n",
      "Epoch 1, Batch 1800, Char 74 Loss: 1.9856407642364502\n",
      "Epoch 1, Batch 1800, Char 75 Loss: 0.8178336024284363\n",
      "Epoch 1, Batch 1800, Char 76 Loss: 2.7342700958251953\n",
      "Epoch 1, Batch 1800, Char 77 Loss: 2.019735813140869\n",
      "Epoch 1, Batch 1800, Char 78 Loss: 2.394522190093994\n",
      "Epoch 1, Batch 1800, Char 79 Loss: 1.5180238485336304\n",
      "Epoch 1, Batch 1800, Char 80 Loss: 3.314283847808838\n",
      "Epoch 1, Batch 1800, Char 81 Loss: 0.6638010740280151\n",
      "Epoch 1, Batch 1800, Char 82 Loss: 2.4671647548675537\n",
      "Epoch 1, Batch 1800, Char 83 Loss: 0.35465970635414124\n",
      "Epoch 1, Batch 1800, Char 84 Loss: 2.035245895385742\n",
      "Epoch 1, Batch 1800, Char 85 Loss: 5.001294136047363\n",
      "Epoch 1, Batch 1800, Char 86 Loss: 3.7871689796447754\n",
      "Epoch 1, Batch 1800, Char 87 Loss: 2.7043776512145996\n",
      "Epoch 1, Batch 1800, Char 88 Loss: 1.985569953918457\n",
      "Epoch 1, Batch 1800, Char 89 Loss: 1.483331322669983\n",
      "Epoch 1, Batch 1800, Char 90 Loss: 2.7915525436401367\n",
      "Epoch 1, Batch 1800, Char 91 Loss: 3.6318717002868652\n",
      "Epoch 1, Batch 1800, Char 92 Loss: 2.342545986175537\n",
      "Epoch 1, Batch 1800, Char 93 Loss: 2.7004053592681885\n",
      "Epoch 1, Batch 1800, Char 94 Loss: 1.6574077606201172\n",
      "Epoch 1, Batch 1800, Char 95 Loss: 1.958991527557373\n",
      "Epoch 1, Batch 1800, Char 96 Loss: 1.4562451839447021\n",
      "Epoch 1, Batch 1800, Char 97 Loss: 2.704693078994751\n",
      "Epoch 1, Batch 1800, Char 98 Loss: 0.574808657169342\n",
      "Epoch 1, Batch 1900, Char 0 Loss: 1.9094356298446655\n",
      "Epoch 1, Batch 1900, Char 1 Loss: 3.0201244354248047\n",
      "Epoch 1, Batch 1900, Char 2 Loss: 2.707906723022461\n",
      "Epoch 1, Batch 1900, Char 3 Loss: 0.3995876908302307\n",
      "Epoch 1, Batch 1900, Char 4 Loss: 3.958996295928955\n",
      "Epoch 1, Batch 1900, Char 5 Loss: 1.2511138916015625\n",
      "Epoch 1, Batch 1900, Char 6 Loss: 2.6871235370635986\n",
      "Epoch 1, Batch 1900, Char 7 Loss: 2.2487871646881104\n",
      "Epoch 1, Batch 1900, Char 8 Loss: 1.829939842224121\n",
      "Epoch 1, Batch 1900, Char 9 Loss: 9.081238746643066\n",
      "Epoch 1, Batch 1900, Char 10 Loss: 0.4194621443748474\n",
      "Epoch 1, Batch 1900, Char 11 Loss: 1.0009675025939941\n",
      "Epoch 1, Batch 1900, Char 12 Loss: 2.767587900161743\n",
      "Epoch 1, Batch 1900, Char 13 Loss: 1.9907292127609253\n",
      "Epoch 1, Batch 1900, Char 14 Loss: 1.4239720106124878\n",
      "Epoch 1, Batch 1900, Char 15 Loss: 2.5115771293640137\n",
      "Epoch 1, Batch 1900, Char 16 Loss: 3.865420341491699\n",
      "Epoch 1, Batch 1900, Char 17 Loss: 4.086054801940918\n",
      "Epoch 1, Batch 1900, Char 18 Loss: 16.437822341918945\n",
      "Epoch 1, Batch 1900, Char 19 Loss: 6.66415548324585\n",
      "Epoch 1, Batch 1900, Char 20 Loss: 3.9503555297851562\n",
      "Epoch 1, Batch 1900, Char 21 Loss: 3.4279284477233887\n",
      "Epoch 1, Batch 1900, Char 22 Loss: 3.9994091987609863\n",
      "Epoch 1, Batch 1900, Char 23 Loss: 0.8545917868614197\n",
      "Epoch 1, Batch 1900, Char 24 Loss: 2.710026264190674\n",
      "Epoch 1, Batch 1900, Char 25 Loss: 1.7598285675048828\n",
      "Epoch 1, Batch 1900, Char 26 Loss: 1.8648154735565186\n",
      "Epoch 1, Batch 1900, Char 27 Loss: 3.380824089050293\n",
      "Epoch 1, Batch 1900, Char 28 Loss: 2.2024083137512207\n",
      "Epoch 1, Batch 1900, Char 29 Loss: 2.998340129852295\n",
      "Epoch 1, Batch 1900, Char 30 Loss: 2.846259355545044\n",
      "Epoch 1, Batch 1900, Char 31 Loss: 2.0003154277801514\n",
      "Epoch 1, Batch 1900, Char 32 Loss: 4.4718337059021\n",
      "Epoch 1, Batch 1900, Char 33 Loss: 0.39667704701423645\n",
      "Epoch 1, Batch 1900, Char 34 Loss: 1.004287838935852\n",
      "Epoch 1, Batch 1900, Char 35 Loss: 4.204065322875977\n",
      "Epoch 1, Batch 1900, Char 36 Loss: 2.334326982498169\n",
      "Epoch 1, Batch 1900, Char 37 Loss: 1.9474579095840454\n",
      "Epoch 1, Batch 1900, Char 38 Loss: 2.386164426803589\n",
      "Epoch 1, Batch 1900, Char 39 Loss: 0.9949633479118347\n",
      "Epoch 1, Batch 1900, Char 40 Loss: 5.0780839920043945\n",
      "Epoch 1, Batch 1900, Char 41 Loss: 0.3854595422744751\n",
      "Epoch 1, Batch 1900, Char 42 Loss: 1.8108891248703003\n",
      "Epoch 1, Batch 1900, Char 43 Loss: 2.982842445373535\n",
      "Epoch 1, Batch 1900, Char 44 Loss: 0.3101976811885834\n",
      "Epoch 1, Batch 1900, Char 45 Loss: 3.3353214263916016\n",
      "Epoch 1, Batch 1900, Char 46 Loss: 2.777505397796631\n",
      "Epoch 1, Batch 1900, Char 47 Loss: 2.2295684814453125\n",
      "Epoch 1, Batch 1900, Char 48 Loss: 1.4072757959365845\n",
      "Epoch 1, Batch 1900, Char 49 Loss: 2.594849109649658\n",
      "Epoch 1, Batch 1900, Char 50 Loss: 1.1081998348236084\n",
      "Epoch 1, Batch 1900, Char 51 Loss: 2.750246286392212\n",
      "Epoch 1, Batch 1900, Char 52 Loss: 2.1003522872924805\n",
      "Epoch 1, Batch 1900, Char 53 Loss: 1.9561792612075806\n",
      "Epoch 1, Batch 1900, Char 54 Loss: 1.9777917861938477\n",
      "Epoch 1, Batch 1900, Char 55 Loss: 1.0213932991027832\n",
      "Epoch 1, Batch 1900, Char 56 Loss: 0.6299600005149841\n",
      "Epoch 1, Batch 1900, Char 57 Loss: 0.9811908602714539\n",
      "Epoch 1, Batch 1900, Char 58 Loss: 3.67325496673584\n",
      "Epoch 1, Batch 1900, Char 59 Loss: 2.2437236309051514\n",
      "Epoch 1, Batch 1900, Char 60 Loss: 3.318702220916748\n",
      "Epoch 1, Batch 1900, Char 61 Loss: 4.970820426940918\n",
      "Epoch 1, Batch 1900, Char 62 Loss: 4.981480598449707\n",
      "Epoch 1, Batch 1900, Char 63 Loss: 0.832785427570343\n",
      "Epoch 1, Batch 1900, Char 64 Loss: 2.749169111251831\n",
      "Epoch 1, Batch 1900, Char 65 Loss: 2.2876601219177246\n",
      "Epoch 1, Batch 1900, Char 66 Loss: 1.0783289670944214\n",
      "Epoch 1, Batch 1900, Char 67 Loss: 2.214094638824463\n",
      "Epoch 1, Batch 1900, Char 68 Loss: 2.8141136169433594\n",
      "Epoch 1, Batch 1900, Char 69 Loss: 2.9175009727478027\n",
      "Epoch 1, Batch 1900, Char 70 Loss: 1.0344432592391968\n",
      "Epoch 1, Batch 1900, Char 71 Loss: 1.905524730682373\n",
      "Epoch 1, Batch 1900, Char 72 Loss: 3.4660394191741943\n",
      "Epoch 1, Batch 1900, Char 73 Loss: 4.945680141448975\n",
      "Epoch 1, Batch 1900, Char 74 Loss: 2.814133644104004\n",
      "Epoch 1, Batch 1900, Char 75 Loss: 3.4153261184692383\n",
      "Epoch 1, Batch 1900, Char 76 Loss: 2.0401721000671387\n",
      "Epoch 1, Batch 1900, Char 77 Loss: 2.0534491539001465\n",
      "Epoch 1, Batch 1900, Char 78 Loss: 6.595716953277588\n",
      "Epoch 1, Batch 1900, Char 79 Loss: 2.3050003051757812\n",
      "Epoch 1, Batch 1900, Char 80 Loss: 2.6349945068359375\n",
      "Epoch 1, Batch 1900, Char 81 Loss: 3.309479236602783\n",
      "Epoch 1, Batch 1900, Char 82 Loss: 0.8259807825088501\n",
      "Epoch 1, Batch 1900, Char 83 Loss: 1.9792951345443726\n",
      "Epoch 1, Batch 1900, Char 84 Loss: 1.016974687576294\n",
      "Epoch 1, Batch 1900, Char 85 Loss: 1.95753812789917\n",
      "Epoch 1, Batch 1900, Char 86 Loss: 2.136826276779175\n",
      "Epoch 1, Batch 1900, Char 87 Loss: 1.5512481927871704\n",
      "Epoch 1, Batch 1900, Char 88 Loss: 3.910870313644409\n",
      "Epoch 1, Batch 1900, Char 89 Loss: 2.2565648555755615\n",
      "Epoch 1, Batch 1900, Char 90 Loss: 4.000141143798828\n",
      "Epoch 1, Batch 1900, Char 91 Loss: 0.351772665977478\n",
      "Epoch 1, Batch 1900, Char 92 Loss: 1.823014259338379\n",
      "Epoch 1, Batch 1900, Char 93 Loss: 1.3855712413787842\n",
      "Epoch 1, Batch 1900, Char 94 Loss: 4.127143859863281\n",
      "Epoch 1, Batch 1900, Char 95 Loss: 1.4683401584625244\n",
      "Epoch 1, Batch 1900, Char 96 Loss: 3.8770647048950195\n",
      "Epoch 1, Batch 1900, Char 97 Loss: 3.851191997528076\n",
      "Epoch 1, Batch 1900, Char 98 Loss: 0.8318274617195129\n",
      "Epoch 1, Batch 2000, Char 0 Loss: 3.529841661453247\n",
      "Epoch 1, Batch 2000, Char 0\n",
      "target: v predicted: n\n",
      "History: \"v\", Predicted: \"n\"\n",
      "Epoch 1, Batch 2000, Char 1 Loss: 0.23307541012763977\n",
      "Epoch 1, Batch 2000, Char 1\n",
      "target: e predicted: e\n",
      "History: \"ve\", Predicted: \"ne\"\n",
      "Epoch 1, Batch 2000, Char 2 Loss: 2.5664758682250977\n",
      "Epoch 1, Batch 2000, Char 2\n",
      "target: d predicted:  \n",
      "History: \"ved\", Predicted: \"ne \"\n",
      "Epoch 1, Batch 2000, Char 3 Loss: 0.3348589539527893\n",
      "Epoch 1, Batch 2000, Char 3\n",
      "target:   predicted:  \n",
      "History: \"ved \", Predicted: \"ne  \"\n",
      "Epoch 1, Batch 2000, Char 4 Loss: 1.95428466796875\n",
      "Epoch 1, Batch 2000, Char 4\n",
      "target: a predicted: a\n",
      "History: \"ved a\", Predicted: \"ne  a\"\n",
      "Epoch 1, Batch 2000, Char 5 Loss: 1.9373997449874878\n",
      "Epoch 1, Batch 2000, Char 5\n",
      "target: t predicted: n\n",
      "History: \"ved at\", Predicted: \"ne  an\"\n",
      "Epoch 1, Batch 2000, Char 6 Loss: 1.0885162353515625\n",
      "Epoch 1, Batch 2000, Char 6\n",
      "target:   predicted:  \n",
      "History: \"ved at \", Predicted: \"ne  an \"\n",
      "Epoch 1, Batch 2000, Char 7 Loss: 3.879133701324463\n",
      "Epoch 1, Batch 2000, Char 7\n",
      "target: l predicted: a\n",
      "History: \"ved at l\", Predicted: \"ne  an a\"\n",
      "Epoch 1, Batch 2000, Char 8 Loss: 1.7576581239700317\n",
      "Epoch 1, Batch 2000, Char 8\n",
      "target: e predicted: e\n",
      "History: \"ved at le\", Predicted: \"ne  an ae\"\n",
      "Epoch 1, Batch 2000, Char 9 Loss: 3.070590019226074\n",
      "Epoch 1, Batch 2000, Char 9\n",
      "target: a predicted:  \n",
      "History: \"ved at lea\", Predicted: \"ne  an ae \"\n",
      "Epoch 1, Batch 2000, Char 10 Loss: 2.4391093254089355\n",
      "Epoch 1, Batch 2000, Char 10\n",
      "target: s predicted: n\n",
      "History: \"ved at leas\", Predicted: \"ne  an ae n\"\n",
      "Epoch 1, Batch 2000, Char 11 Loss: 2.444610595703125\n",
      "Epoch 1, Batch 2000, Char 11\n",
      "target: t predicted:  \n",
      "History: \"ved at least\", Predicted: \"ne  an ae n \"\n",
      "Epoch 1, Batch 2000, Char 12 Loss: 1.0742088556289673\n",
      "Epoch 1, Batch 2000, Char 12\n",
      "target:   predicted:  \n",
      "History: \"ved at least \", Predicted: \"ne  an ae n  \"\n",
      "Epoch 1, Batch 2000, Char 13 Loss: 3.3730006217956543\n",
      "Epoch 1, Batch 2000, Char 13\n",
      "target: f predicted: a\n",
      "History: \"ved at least f\", Predicted: \"ne  an ae n  a\"\n",
      "Epoch 1, Batch 2000, Char 14 Loss: 2.1354429721832275\n",
      "Epoch 1, Batch 2000, Char 14\n",
      "target: o predicted:  \n",
      "History: \"ved at least fo\", Predicted: \"ne  an ae n  a \"\n",
      "Epoch 1, Batch 2000, Char 15 Loss: 1.791700839996338\n",
      "Epoch 1, Batch 2000, Char 15\n",
      "target: u predicted: u\n",
      "History: \"ved at least fou\", Predicted: \"ne  an ae n  a u\"\n",
      "Epoch 1, Batch 2000, Char 16 Loss: 1.951781988143921\n",
      "Epoch 1, Batch 2000, Char 16\n",
      "target: r predicted: n\n",
      "History: \"ved at least four\", Predicted: \"ne  an ae n  a un\"\n",
      "Epoch 1, Batch 2000, Char 17 Loss: 1.3067666292190552\n",
      "Epoch 1, Batch 2000, Char 17\n",
      "target:   predicted:  \n",
      "History: \"ved at least four \", Predicted: \"ne  an ae n  a un \"\n",
      "Epoch 1, Batch 2000, Char 18 Loss: 3.8269035816192627\n",
      "Epoch 1, Batch 2000, Char 18\n",
      "target: l predicted: a\n",
      "History: \"ved at least four l\", Predicted: \"ne  an ae n  a un a\"\n",
      "Epoch 1, Batch 2000, Char 19 Loss: 2.6041932106018066\n",
      "Epoch 1, Batch 2000, Char 19\n",
      "target: i predicted: e\n",
      "History: \"ved at least four li\", Predicted: \"ne  an ae n  a un ae\"\n",
      "Epoch 1, Batch 2000, Char 20 Loss: 3.737462282180786\n",
      "Epoch 1, Batch 2000, Char 20\n",
      "target: v predicted: n\n",
      "History: \"ved at least four liv\", Predicted: \"ne  an ae n  a un aen\"\n",
      "Epoch 1, Batch 2000, Char 21 Loss: 0.21952985227108002\n",
      "Epoch 1, Batch 2000, Char 21\n",
      "target: e predicted: e\n",
      "History: \"ved at least four live\", Predicted: \"ne  an ae n  a un aene\"\n",
      "Epoch 1, Batch 2000, Char 22 Loss: 3.218416690826416\n",
      "Epoch 1, Batch 2000, Char 22\n",
      "target: s predicted:  \n",
      "History: \"ved at least four lives\", Predicted: \"ne  an ae n  a un aene \"\n",
      "Epoch 1, Batch 2000, Char 23 Loss: 0.8660917282104492\n",
      "Epoch 1, Batch 2000, Char 23\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives \", Predicted: \"ne  an ae n  a un aene  \"\n",
      "Epoch 1, Batch 2000, Char 24 Loss: 3.197195529937744\n",
      "Epoch 1, Batch 2000, Char 24\n",
      "target: c predicted: a\n",
      "History: \"ved at least four lives c\", Predicted: \"ne  an ae n  a un aene  a\"\n",
      "Epoch 1, Batch 2000, Char 25 Loss: 1.573021411895752\n",
      "Epoch 1, Batch 2000, Char 25\n",
      "target: o predicted: h\n",
      "History: \"ved at least four lives co\", Predicted: \"ne  an ae n  a un aene  ah\"\n",
      "Epoch 1, Batch 2000, Char 26 Loss: 1.7481523752212524\n",
      "Epoch 1, Batch 2000, Char 26\n",
      "target: u predicted: u\n",
      "History: \"ved at least four lives cou\", Predicted: \"ne  an ae n  a un aene  ahu\"\n",
      "Epoch 1, Batch 2000, Char 27 Loss: 2.3321664333343506\n",
      "Epoch 1, Batch 2000, Char 27\n",
      "target: s predicted: n\n",
      "History: \"ved at least four lives cous\", Predicted: \"ne  an ae n  a un aene  ahun\"\n",
      "Epoch 1, Batch 2000, Char 28 Loss: 2.8098855018615723\n",
      "Epoch 1, Batch 2000, Char 28\n",
      "target: i predicted:  \n",
      "History: \"ved at least four lives cousi\", Predicted: \"ne  an ae n  a un aene  ahun \"\n",
      "Epoch 1, Batch 2000, Char 29 Loss: 1.3053805828094482\n",
      "Epoch 1, Batch 2000, Char 29\n",
      "target: n predicted: n\n",
      "History: \"ved at least four lives cousin\", Predicted: \"ne  an ae n  a un aene  ahun n\"\n",
      "Epoch 1, Batch 2000, Char 30 Loss: 1.072102427482605\n",
      "Epoch 1, Batch 2000, Char 30\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin \", Predicted: \"ne  an ae n  a un aene  ahun n \"\n",
      "Epoch 1, Batch 2000, Char 31 Loss: 2.763810157775879\n",
      "Epoch 1, Batch 2000, Char 31\n",
      "target: m predicted: a\n",
      "History: \"ved at least four lives cousin m\", Predicted: \"ne  an ae n  a un aene  ahun n a\"\n",
      "Epoch 1, Batch 2000, Char 32 Loss: 2.522613525390625\n",
      "Epoch 1, Batch 2000, Char 32\n",
      "target: o predicted: e\n",
      "History: \"ved at least four lives cousin mo\", Predicted: \"ne  an ae n  a un aene  ahun n ae\"\n",
      "Epoch 1, Batch 2000, Char 33 Loss: 3.191742420196533\n",
      "Epoch 1, Batch 2000, Char 33\n",
      "target: l predicted: u\n",
      "History: \"ved at least four lives cousin mol\", Predicted: \"ne  an ae n  a un aene  ahun n aeu\"\n",
      "Epoch 1, Batch 2000, Char 34 Loss: 2.259103298187256\n",
      "Epoch 1, Batch 2000, Char 34\n",
      "target: l predicted: e\n",
      "History: \"ved at least four lives cousin moll\", Predicted: \"ne  an ae n  a un aene  ahun n aeue\"\n",
      "Epoch 1, Batch 2000, Char 35 Loss: 2.107851505279541\n",
      "Epoch 1, Batch 2000, Char 35\n",
      "target: y predicted: e\n",
      "History: \"ved at least four lives cousin molly\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee\"\n",
      "Epoch 1, Batch 2000, Char 36 Loss: 0.2343253791332245\n",
      "Epoch 1, Batch 2000, Char 36\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee \"\n",
      "Epoch 1, Batch 2000, Char 37 Loss: 3.265803337097168\n",
      "Epoch 1, Batch 2000, Char 37\n",
      "target: b predicted: a\n",
      "History: \"ved at least four lives cousin molly b\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee a\"\n",
      "Epoch 1, Batch 2000, Char 38 Loss: 0.9498957395553589\n",
      "Epoch 1, Batch 2000, Char 38\n",
      "target: e predicted: e\n",
      "History: \"ved at least four lives cousin molly be\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae\"\n",
      "Epoch 1, Batch 2000, Char 39 Loss: 3.6987624168395996\n",
      "Epoch 1, Batch 2000, Char 39\n",
      "target: l predicted:  \n",
      "History: \"ved at least four lives cousin molly bel\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae \"\n",
      "Epoch 1, Batch 2000, Char 40 Loss: 2.2004542350769043\n",
      "Epoch 1, Batch 2000, Char 40\n",
      "target: l predicted: e\n",
      "History: \"ved at least four lives cousin molly bell\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae e\"\n",
      "Epoch 1, Batch 2000, Char 41 Loss: 1.7315267324447632\n",
      "Epoch 1, Batch 2000, Char 41\n",
      "target: e predicted: e\n",
      "History: \"ved at least four lives cousin molly belle\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee\"\n",
      "Epoch 1, Batch 2000, Char 42 Loss: 0.8861530423164368\n",
      "Epoch 1, Batch 2000, Char 42\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee \"\n",
      "Epoch 1, Batch 2000, Char 43 Loss: 1.945109248161316\n",
      "Epoch 1, Batch 2000, Char 43\n",
      "target: a predicted: a\n",
      "History: \"ved at least four lives cousin molly belle a\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee a\"\n",
      "Epoch 1, Batch 2000, Char 44 Loss: 1.5419560670852661\n",
      "Epoch 1, Batch 2000, Char 44\n",
      "target: n predicted: n\n",
      "History: \"ved at least four lives cousin molly belle an\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an\"\n",
      "Epoch 1, Batch 2000, Char 45 Loss: 1.7841787338256836\n",
      "Epoch 1, Batch 2000, Char 45\n",
      "target: d predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an \"\n",
      "Epoch 1, Batch 2000, Char 46 Loss: 0.3333670198917389\n",
      "Epoch 1, Batch 2000, Char 46\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  \"\n",
      "Epoch 1, Batch 2000, Char 47 Loss: 3.2024340629577637\n",
      "Epoch 1, Batch 2000, Char 47\n",
      "target: b predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and b\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  a\"\n",
      "Epoch 1, Batch 2000, Char 48 Loss: 2.5012974739074707\n",
      "Epoch 1, Batch 2000, Char 48\n",
      "target: a predicted: e\n",
      "History: \"ved at least four lives cousin molly belle and ba\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  ae\"\n",
      "Epoch 1, Batch 2000, Char 49 Loss: 4.3312225341796875\n",
      "Epoch 1, Batch 2000, Char 49\n",
      "target: b predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and bab\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aen\"\n",
      "Epoch 1, Batch 2000, Char 50 Loss: 2.5098514556884766\n",
      "Epoch 1, Batch 2000, Char 50\n",
      "target: y predicted: e\n",
      "History: \"ved at least four lives cousin molly belle and baby\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene\"\n",
      "Epoch 1, Batch 2000, Char 51 Loss: 0.23033086955547333\n",
      "Epoch 1, Batch 2000, Char 51\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene \"\n",
      "Epoch 1, Batch 2000, Char 52 Loss: 1.9262092113494873\n",
      "Epoch 1, Batch 2000, Char 52\n",
      "target: a predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby a\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene a\"\n",
      "Epoch 1, Batch 2000, Char 53 Loss: 1.517609715461731\n",
      "Epoch 1, Batch 2000, Char 53\n",
      "target: n predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby an\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an\"\n",
      "Epoch 1, Batch 2000, Char 54 Loss: 1.7510915994644165\n",
      "Epoch 1, Batch 2000, Char 54\n",
      "target: d predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an \"\n",
      "Epoch 1, Batch 2000, Char 55 Loss: 0.32924884557724\n",
      "Epoch 1, Batch 2000, Char 55\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  \"\n",
      "Epoch 1, Batch 2000, Char 56 Loss: 3.0237483978271484\n",
      "Epoch 1, Batch 2000, Char 56\n",
      "target: s predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and s\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a\"\n",
      "Epoch 1, Batch 2000, Char 57 Loss: 7.14064884185791\n",
      "Epoch 1, Batch 2000, Char 57\n",
      "target: n predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and sn\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a \"\n",
      "Epoch 1, Batch 2000, Char 58 Loss: 3.676270008087158\n",
      "Epoch 1, Batch 2000, Char 58\n",
      "target: a predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and sna\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  \"\n",
      "Epoch 1, Batch 2000, Char 59 Loss: 4.113700866699219\n",
      "Epoch 1, Batch 2000, Char 59\n",
      "target: p predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  n\"\n",
      "Epoch 1, Batch 2000, Char 60 Loss: 2.2362313270568848\n",
      "Epoch 1, Batch 2000, Char 60\n",
      "target:   predicted: o\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  no\"\n",
      "Epoch 1, Batch 2000, Char 61 Loss: 1.8902549743652344\n",
      "Epoch 1, Batch 2000, Char 61\n",
      "target: a predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap a\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noa\"\n",
      "Epoch 1, Batch 2000, Char 62 Loss: 1.4786654710769653\n",
      "Epoch 1, Batch 2000, Char 62\n",
      "target: n predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap an\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan\"\n",
      "Epoch 1, Batch 2000, Char 63 Loss: 1.7024041414260864\n",
      "Epoch 1, Batch 2000, Char 63\n",
      "target: d predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan \"\n",
      "Epoch 1, Batch 2000, Char 64 Loss: 0.32389166951179504\n",
      "Epoch 1, Batch 2000, Char 64\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  \"\n",
      "Epoch 1, Batch 2000, Char 65 Loss: 2.7318267822265625\n",
      "Epoch 1, Batch 2000, Char 65\n",
      "target: m predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and m\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  a\"\n",
      "Epoch 1, Batch 2000, Char 66 Loss: 2.1426966190338135\n",
      "Epoch 1, Batch 2000, Char 66\n",
      "target: i predicted: e\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mi\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  ae\"\n",
      "Epoch 1, Batch 2000, Char 67 Loss: 1.2465590238571167\n",
      "Epoch 1, Batch 2000, Char 67\n",
      "target: n predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and min\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen\"\n",
      "Epoch 1, Batch 2000, Char 68 Loss: 2.464958667755127\n",
      "Epoch 1, Batch 2000, Char 68\n",
      "target: e predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen \"\n",
      "Epoch 1, Batch 2000, Char 69 Loss: 0.8833137154579163\n",
      "Epoch 1, Batch 2000, Char 69\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  \"\n",
      "Epoch 1, Batch 2000, Char 70 Loss: 2.615802526473999\n",
      "Epoch 1, Batch 2000, Char 70\n",
      "target: i predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  a\"\n",
      "Epoch 1, Batch 2000, Char 71 Loss: 2.1044692993164062\n",
      "Epoch 1, Batch 2000, Char 71\n",
      "target:   predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  an\"\n",
      "Epoch 1, Batch 2000, Char 72 Loss: 3.330084800720215\n",
      "Epoch 1, Batch 2000, Char 72\n",
      "target: f predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i f\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana\"\n",
      "Epoch 1, Batch 2000, Char 73 Loss: 2.9184083938598633\n",
      "Epoch 1, Batch 2000, Char 73\n",
      "target: e predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i fe\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana \"\n",
      "Epoch 1, Batch 2000, Char 74 Loss: 3.5627217292785645\n",
      "Epoch 1, Batch 2000, Char 74\n",
      "target: l predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i fel\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  \"\n",
      "Epoch 1, Batch 2000, Char 75 Loss: 4.258695125579834\n",
      "Epoch 1, Batch 2000, Char 75\n",
      "target: t predicted: e\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e\"\n",
      "Epoch 1, Batch 2000, Char 76 Loss: 1.0260194540023804\n",
      "Epoch 1, Batch 2000, Char 76\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e \"\n",
      "Epoch 1, Batch 2000, Char 77 Loss: 2.107835054397583\n",
      "Epoch 1, Batch 2000, Char 77\n",
      "target: t predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt t\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a\"\n",
      "Epoch 1, Batch 2000, Char 78 Loss: 1.2703320980072021\n",
      "Epoch 1, Batch 2000, Char 78\n",
      "target: h predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt th\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a \"\n",
      "Epoch 1, Batch 2000, Char 79 Loss: 1.5169609785079956\n",
      "Epoch 1, Batch 2000, Char 79\n",
      "target: a predicted: e\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt tha\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a e\"\n",
      "Epoch 1, Batch 2000, Char 80 Loss: 1.996877908706665\n",
      "Epoch 1, Batch 2000, Char 80\n",
      "target: t predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en\"\n",
      "Epoch 1, Batch 2000, Char 81 Loss: 1.0206637382507324\n",
      "Epoch 1, Batch 2000, Char 81\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en \"\n",
      "Epoch 1, Batch 2000, Char 82 Loss: 2.8169164657592773\n",
      "Epoch 1, Batch 2000, Char 82\n",
      "target: h predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that h\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en a\"\n",
      "Epoch 1, Batch 2000, Char 83 Loss: 0.8430173993110657\n",
      "Epoch 1, Batch 2000, Char 83\n",
      "target: e predicted: e\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae\"\n",
      "Epoch 1, Batch 2000, Char 84 Loss: 0.8713771104812622\n",
      "Epoch 1, Batch 2000, Char 84\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae \"\n",
      "Epoch 1, Batch 2000, Char 85 Loss: 2.6047542095184326\n",
      "Epoch 1, Batch 2000, Char 85\n",
      "target: w predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he w\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae a\"\n",
      "Epoch 1, Batch 2000, Char 86 Loss: 1.1391446590423584\n",
      "Epoch 1, Batch 2000, Char 86\n",
      "target: a predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he wa\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aa\"\n",
      "Epoch 1, Batch 2000, Char 87 Loss: 2.382056713104248\n",
      "Epoch 1, Batch 2000, Char 87\n",
      "target: s predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan\"\n",
      "Epoch 1, Batch 2000, Char 88 Loss: 0.8653047680854797\n",
      "Epoch 1, Batch 2000, Char 88\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan \"\n",
      "Epoch 1, Batch 2000, Char 89 Loss: 3.4244964122772217\n",
      "Epoch 1, Batch 2000, Char 89\n",
      "target: n predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was n\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a\"\n",
      "Epoch 1, Batch 2000, Char 90 Loss: 2.7122817039489746\n",
      "Epoch 1, Batch 2000, Char 90\n",
      "target: o predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was no\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a \"\n",
      "Epoch 1, Batch 2000, Char 91 Loss: 2.7603769302368164\n",
      "Epoch 1, Batch 2000, Char 91\n",
      "target: t predicted: u\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was not\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a u\"\n",
      "Epoch 1, Batch 2000, Char 92 Loss: 1.001335620880127\n",
      "Epoch 1, Batch 2000, Char 92\n",
      "target:   predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was not \", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a u \"\n",
      "Epoch 1, Batch 2000, Char 93 Loss: 3.67136812210083\n",
      "Epoch 1, Batch 2000, Char 93\n",
      "target: l predicted: a\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was not l\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a u a\"\n",
      "Epoch 1, Batch 2000, Char 94 Loss: 2.104583263397217\n",
      "Epoch 1, Batch 2000, Char 94\n",
      "target: a predicted: e\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was not la\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a u ae\"\n",
      "Epoch 1, Batch 2000, Char 95 Loss: 3.7328667640686035\n",
      "Epoch 1, Batch 2000, Char 95\n",
      "target: u predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was not lau\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a u aen\"\n",
      "Epoch 1, Batch 2000, Char 96 Loss: 2.8385066986083984\n",
      "Epoch 1, Batch 2000, Char 96\n",
      "target: g predicted: n\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was not laug\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a u aenn\"\n",
      "Epoch 1, Batch 2000, Char 97 Loss: 1.9242167472839355\n",
      "Epoch 1, Batch 2000, Char 97\n",
      "target: h predicted:  \n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was not laugh\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a u aenn \"\n",
      "Epoch 1, Batch 2000, Char 98 Loss: 2.1406707763671875\n",
      "Epoch 1, Batch 2000, Char 98\n",
      "target: i predicted: e\n",
      "History: \"ved at least four lives cousin molly belle and baby and snap and mine i felt that he was not laughi\", Predicted: \"ne  an ae n  a un aene  ahun n aeuee ae ee an  aene an  a  noan  aen  ana  e a en ae aan a u aenn e\"\n",
      "Epoch 1, Batch 2100, Char 0 Loss: 0.37114930152893066\n",
      "Epoch 1, Batch 2100, Char 1 Loss: 3.628211498260498\n",
      "Epoch 1, Batch 2100, Char 2 Loss: 2.26473331451416\n",
      "Epoch 1, Batch 2100, Char 3 Loss: 3.6889352798461914\n",
      "Epoch 1, Batch 2100, Char 4 Loss: 0.1259525567293167\n",
      "Epoch 1, Batch 2100, Char 5 Loss: 2.8693456649780273\n",
      "Epoch 1, Batch 2100, Char 6 Loss: 1.3389315605163574\n",
      "Epoch 1, Batch 2100, Char 7 Loss: 3.9817845821380615\n",
      "Epoch 1, Batch 2100, Char 8 Loss: 2.273038387298584\n",
      "Epoch 1, Batch 2100, Char 9 Loss: 2.5564708709716797\n",
      "Epoch 1, Batch 2100, Char 10 Loss: 3.7282910346984863\n",
      "Epoch 1, Batch 2100, Char 11 Loss: 2.390078067779541\n",
      "Epoch 1, Batch 2100, Char 12 Loss: 0.9472997784614563\n",
      "Epoch 1, Batch 2100, Char 13 Loss: 1.7913256883621216\n",
      "Epoch 1, Batch 2100, Char 14 Loss: 2.3521203994750977\n",
      "Epoch 1, Batch 2100, Char 15 Loss: 1.9008220434188843\n",
      "Epoch 1, Batch 2100, Char 16 Loss: 2.0824379920959473\n",
      "Epoch 1, Batch 2100, Char 17 Loss: 2.7044835090637207\n",
      "Epoch 1, Batch 2100, Char 18 Loss: 3.3188600540161133\n",
      "Epoch 1, Batch 2100, Char 19 Loss: 1.6897461414337158\n",
      "Epoch 1, Batch 2100, Char 20 Loss: 3.2266526222229004\n",
      "Epoch 1, Batch 2100, Char 21 Loss: 2.149846315383911\n",
      "Epoch 1, Batch 2100, Char 22 Loss: 0.3582790195941925\n",
      "Epoch 1, Batch 2100, Char 23 Loss: 2.753769874572754\n",
      "Epoch 1, Batch 2100, Char 24 Loss: 5.125870704650879\n",
      "Epoch 1, Batch 2100, Char 25 Loss: 2.0978219509124756\n",
      "Epoch 1, Batch 2100, Char 26 Loss: 3.434835910797119\n",
      "Epoch 1, Batch 2100, Char 27 Loss: 4.854284286499023\n",
      "Epoch 1, Batch 2100, Char 28 Loss: 2.7729451656341553\n",
      "Epoch 1, Batch 2100, Char 29 Loss: 1.369810700416565\n",
      "Epoch 1, Batch 2100, Char 30 Loss: 1.9433848857879639\n",
      "Epoch 1, Batch 2100, Char 31 Loss: 1.0877183675765991\n",
      "Epoch 1, Batch 2100, Char 32 Loss: 3.9805502891540527\n",
      "Epoch 1, Batch 2100, Char 33 Loss: 2.850698471069336\n",
      "Epoch 1, Batch 2100, Char 34 Loss: 3.6155223846435547\n",
      "Epoch 1, Batch 2100, Char 35 Loss: 1.350694179534912\n",
      "Epoch 1, Batch 2100, Char 36 Loss: 1.2925364971160889\n",
      "Epoch 1, Batch 2100, Char 37 Loss: 2.653076648712158\n",
      "Epoch 1, Batch 2100, Char 38 Loss: 2.545119047164917\n",
      "Epoch 1, Batch 2100, Char 39 Loss: 2.7194714546203613\n",
      "Epoch 1, Batch 2100, Char 40 Loss: 4.067490577697754\n",
      "Epoch 1, Batch 2100, Char 41 Loss: 1.9340698719024658\n",
      "Epoch 1, Batch 2100, Char 42 Loss: 3.4733128547668457\n",
      "Epoch 1, Batch 2100, Char 43 Loss: 2.762388229370117\n",
      "Epoch 1, Batch 2100, Char 44 Loss: 1.8193975687026978\n",
      "Epoch 1, Batch 2100, Char 45 Loss: 1.9558303356170654\n",
      "Epoch 1, Batch 2100, Char 46 Loss: 1.3821359872817993\n",
      "Epoch 1, Batch 2100, Char 47 Loss: 2.313915729522705\n",
      "Epoch 1, Batch 2100, Char 48 Loss: 0.3534882366657257\n",
      "Epoch 1, Batch 2100, Char 49 Loss: 2.045513868331909\n",
      "Epoch 1, Batch 2100, Char 50 Loss: 2.1384897232055664\n",
      "Epoch 1, Batch 2100, Char 51 Loss: 0.8472740054130554\n",
      "Epoch 1, Batch 2100, Char 52 Loss: 3.4967894554138184\n",
      "Epoch 1, Batch 2100, Char 53 Loss: 2.235557794570923\n",
      "Epoch 1, Batch 2100, Char 54 Loss: 2.114964485168457\n",
      "Epoch 1, Batch 2100, Char 55 Loss: 2.0274510383605957\n",
      "Epoch 1, Batch 2100, Char 56 Loss: 1.3614364862442017\n",
      "Epoch 1, Batch 2100, Char 57 Loss: 2.0181379318237305\n",
      "Epoch 1, Batch 2100, Char 58 Loss: 2.0773253440856934\n",
      "Epoch 1, Batch 2100, Char 59 Loss: 0.8444836139678955\n",
      "Epoch 1, Batch 2100, Char 60 Loss: 2.6088650226593018\n",
      "Epoch 1, Batch 2100, Char 61 Loss: 2.482767105102539\n",
      "Epoch 1, Batch 2100, Char 62 Loss: 3.2176265716552734\n",
      "Epoch 1, Batch 2100, Char 63 Loss: 1.6420453786849976\n",
      "Epoch 1, Batch 2100, Char 64 Loss: 1.6331297159194946\n",
      "Epoch 1, Batch 2100, Char 65 Loss: 2.438706636428833\n",
      "Epoch 1, Batch 2100, Char 66 Loss: 2.085465431213379\n",
      "Epoch 1, Batch 2100, Char 67 Loss: 0.3467981815338135\n",
      "Epoch 1, Batch 2100, Char 68 Loss: 1.9801520109176636\n",
      "Epoch 1, Batch 2100, Char 69 Loss: 3.5316624641418457\n",
      "Epoch 1, Batch 2100, Char 70 Loss: 3.1230502128601074\n",
      "Epoch 1, Batch 2100, Char 71 Loss: 2.5585336685180664\n",
      "Epoch 1, Batch 2100, Char 72 Loss: 4.136241436004639\n",
      "Epoch 1, Batch 2100, Char 73 Loss: 3.351668119430542\n",
      "Epoch 1, Batch 2100, Char 74 Loss: 0.8250102996826172\n",
      "Epoch 1, Batch 2100, Char 75 Loss: 1.892690658569336\n",
      "Epoch 1, Batch 2100, Char 76 Loss: 1.123716115951538\n",
      "Epoch 1, Batch 2100, Char 77 Loss: 0.7385218739509583\n",
      "Epoch 1, Batch 2100, Char 78 Loss: 0.9521927237510681\n",
      "Epoch 1, Batch 2100, Char 79 Loss: 4.119914531707764\n",
      "Epoch 1, Batch 2100, Char 80 Loss: 8.669696807861328\n",
      "Epoch 1, Batch 2100, Char 81 Loss: 2.1725809574127197\n",
      "Epoch 1, Batch 2100, Char 82 Loss: 3.645023822784424\n",
      "Epoch 1, Batch 2100, Char 83 Loss: 0.3401224613189697\n",
      "Epoch 1, Batch 2100, Char 84 Loss: 1.8848553895950317\n",
      "Epoch 1, Batch 2100, Char 85 Loss: 2.2984061241149902\n",
      "Epoch 1, Batch 2100, Char 86 Loss: 1.8359860181808472\n",
      "Epoch 1, Batch 2100, Char 87 Loss: 2.687567949295044\n",
      "Epoch 1, Batch 2100, Char 88 Loss: 1.965386986732483\n",
      "Epoch 1, Batch 2100, Char 89 Loss: 0.31262701749801636\n",
      "Epoch 1, Batch 2100, Char 90 Loss: 3.1271963119506836\n",
      "Epoch 1, Batch 2100, Char 91 Loss: 1.8186535835266113\n",
      "Epoch 1, Batch 2100, Char 92 Loss: 1.941433310508728\n",
      "Epoch 1, Batch 2100, Char 93 Loss: 2.028491735458374\n",
      "Epoch 1, Batch 2100, Char 94 Loss: 3.6645264625549316\n",
      "Epoch 1, Batch 2100, Char 95 Loss: 1.5139223337173462\n",
      "Epoch 1, Batch 2100, Char 96 Loss: 0.9448449015617371\n",
      "Epoch 1, Batch 2100, Char 97 Loss: 2.654362201690674\n",
      "Epoch 1, Batch 2100, Char 98 Loss: 1.8893744945526123\n",
      "Epoch 1, Batch 2200, Char 0 Loss: 2.58705472946167\n",
      "Epoch 1, Batch 2200, Char 1 Loss: 0.8925533294677734\n",
      "Epoch 1, Batch 2200, Char 2 Loss: 1.730132818222046\n",
      "Epoch 1, Batch 2200, Char 3 Loss: 2.964962959289551\n",
      "Epoch 1, Batch 2200, Char 4 Loss: 3.819951057434082\n",
      "Epoch 1, Batch 2200, Char 5 Loss: 1.121949315071106\n",
      "Epoch 1, Batch 2200, Char 6 Loss: 0.9278674721717834\n",
      "Epoch 1, Batch 2200, Char 7 Loss: 2.7785134315490723\n",
      "Epoch 1, Batch 2200, Char 8 Loss: 1.9981236457824707\n",
      "Epoch 1, Batch 2200, Char 9 Loss: 1.2561894655227661\n",
      "Epoch 1, Batch 2200, Char 10 Loss: 2.425102710723877\n",
      "Epoch 1, Batch 2200, Char 11 Loss: 3.415468692779541\n",
      "Epoch 1, Batch 2200, Char 12 Loss: 3.3364410400390625\n",
      "Epoch 1, Batch 2200, Char 13 Loss: 9.505033493041992\n",
      "Epoch 1, Batch 2200, Char 14 Loss: 3.889988422393799\n",
      "Epoch 1, Batch 2200, Char 15 Loss: 4.301154136657715\n",
      "Epoch 1, Batch 2200, Char 16 Loss: 2.8490631580352783\n",
      "Epoch 1, Batch 2200, Char 17 Loss: 4.123232841491699\n",
      "Epoch 1, Batch 2200, Char 18 Loss: 2.7070746421813965\n",
      "Epoch 1, Batch 2200, Char 19 Loss: 0.26061004400253296\n",
      "Epoch 1, Batch 2200, Char 20 Loss: 2.8901329040527344\n",
      "Epoch 1, Batch 2200, Char 21 Loss: 1.67899751663208\n",
      "Epoch 1, Batch 2200, Char 22 Loss: 1.8315402269363403\n",
      "Epoch 1, Batch 2200, Char 23 Loss: 2.166045665740967\n",
      "Epoch 1, Batch 2200, Char 24 Loss: 1.2112162113189697\n",
      "Epoch 1, Batch 2200, Char 25 Loss: 3.004439353942871\n",
      "Epoch 1, Batch 2200, Char 26 Loss: 2.2556238174438477\n",
      "Epoch 1, Batch 2200, Char 27 Loss: 1.0774691104888916\n",
      "Epoch 1, Batch 2200, Char 28 Loss: 1.7327371835708618\n",
      "Epoch 1, Batch 2200, Char 29 Loss: 1.2879658937454224\n",
      "Epoch 1, Batch 2200, Char 30 Loss: 0.920190691947937\n",
      "Epoch 1, Batch 2200, Char 31 Loss: 0.9186722636222839\n",
      "Epoch 1, Batch 2200, Char 32 Loss: 5.005235195159912\n",
      "Epoch 1, Batch 2200, Char 33 Loss: 0.09571575373411179\n",
      "Epoch 1, Batch 2200, Char 34 Loss: 1.9902434349060059\n",
      "Epoch 1, Batch 2200, Char 35 Loss: 3.2872300148010254\n",
      "Epoch 1, Batch 2200, Char 36 Loss: 0.3915769159793854\n",
      "Epoch 1, Batch 2200, Char 37 Loss: 2.88447642326355\n",
      "Epoch 1, Batch 2200, Char 38 Loss: 1.5062413215637207\n",
      "Epoch 1, Batch 2200, Char 39 Loss: 3.6488711833953857\n",
      "Epoch 1, Batch 2200, Char 40 Loss: 8.90742301940918\n",
      "Epoch 1, Batch 2200, Char 41 Loss: 2.7599308490753174\n",
      "Epoch 1, Batch 2200, Char 42 Loss: 0.8854067921638489\n",
      "Epoch 1, Batch 2200, Char 43 Loss: 2.962169647216797\n",
      "Epoch 1, Batch 2200, Char 44 Loss: 2.1729135513305664\n",
      "Epoch 1, Batch 2200, Char 45 Loss: 1.0651236772537231\n",
      "Epoch 1, Batch 2200, Char 46 Loss: 2.90781307220459\n",
      "Epoch 1, Batch 2200, Char 47 Loss: 2.227537155151367\n",
      "Epoch 1, Batch 2200, Char 48 Loss: 0.3850022256374359\n",
      "Epoch 1, Batch 2200, Char 49 Loss: 4.004639625549316\n",
      "Epoch 1, Batch 2200, Char 50 Loss: 2.1470422744750977\n",
      "Epoch 1, Batch 2200, Char 51 Loss: 4.024166107177734\n",
      "Epoch 1, Batch 2200, Char 52 Loss: 2.5112950801849365\n",
      "Epoch 1, Batch 2200, Char 53 Loss: 0.9257605671882629\n",
      "Epoch 1, Batch 2200, Char 54 Loss: 2.1593527793884277\n",
      "Epoch 1, Batch 2200, Char 55 Loss: 1.5179816484451294\n",
      "Epoch 1, Batch 2200, Char 56 Loss: 1.5563693046569824\n",
      "Epoch 1, Batch 2200, Char 57 Loss: 0.2596648037433624\n",
      "Epoch 1, Batch 2200, Char 58 Loss: 1.7522622346878052\n",
      "Epoch 1, Batch 2200, Char 59 Loss: 4.598532676696777\n",
      "Epoch 1, Batch 2200, Char 60 Loss: 2.157329797744751\n",
      "Epoch 1, Batch 2200, Char 61 Loss: 1.7839481830596924\n",
      "Epoch 1, Batch 2200, Char 62 Loss: 2.146162271499634\n",
      "Epoch 1, Batch 2200, Char 63 Loss: 3.0369367599487305\n",
      "Epoch 1, Batch 2200, Char 64 Loss: 9.15188217163086\n",
      "Epoch 1, Batch 2200, Char 65 Loss: 0.09044298529624939\n",
      "Epoch 1, Batch 2200, Char 66 Loss: 2.36124587059021\n",
      "Epoch 1, Batch 2200, Char 67 Loss: 2.2686195373535156\n",
      "Epoch 1, Batch 2200, Char 68 Loss: 3.943948745727539\n",
      "Epoch 1, Batch 2200, Char 69 Loss: 2.073293447494507\n",
      "Epoch 1, Batch 2200, Char 70 Loss: 1.7014273405075073\n",
      "Epoch 1, Batch 2200, Char 71 Loss: 3.2893424034118652\n",
      "Epoch 1, Batch 2200, Char 72 Loss: 0.86905837059021\n",
      "Epoch 1, Batch 2200, Char 73 Loss: 1.7421941757202148\n",
      "Epoch 1, Batch 2200, Char 74 Loss: 1.2771532535552979\n",
      "Epoch 1, Batch 2200, Char 75 Loss: 0.8703960180282593\n",
      "Epoch 1, Batch 2200, Char 76 Loss: 0.9358285069465637\n",
      "Epoch 1, Batch 2200, Char 77 Loss: 3.2173004150390625\n",
      "Epoch 1, Batch 2200, Char 78 Loss: 2.0513341426849365\n",
      "Epoch 1, Batch 2200, Char 79 Loss: 1.6782281398773193\n",
      "Epoch 1, Batch 2200, Char 80 Loss: 3.2300620079040527\n",
      "Epoch 1, Batch 2200, Char 81 Loss: 3.1156225204467773\n",
      "Epoch 1, Batch 2200, Char 82 Loss: 2.829658269882202\n",
      "Epoch 1, Batch 2200, Char 83 Loss: 2.952119827270508\n",
      "Epoch 1, Batch 2200, Char 84 Loss: 0.2683219611644745\n",
      "Epoch 1, Batch 2200, Char 85 Loss: 3.9830994606018066\n",
      "Epoch 1, Batch 2200, Char 86 Loss: 2.4943037033081055\n",
      "Epoch 1, Batch 2200, Char 87 Loss: 2.534086227416992\n",
      "Epoch 1, Batch 2200, Char 88 Loss: 1.4955734014511108\n",
      "Epoch 1, Batch 2200, Char 89 Loss: 1.4903686046600342\n",
      "Epoch 1, Batch 2200, Char 90 Loss: 19.400400161743164\n",
      "Epoch 1, Batch 2200, Char 91 Loss: 1.5346953868865967\n",
      "Epoch 1, Batch 2200, Char 92 Loss: 2.1931312084198\n",
      "Epoch 1, Batch 2200, Char 93 Loss: 3.1055126190185547\n",
      "Epoch 1, Batch 2200, Char 94 Loss: 2.13969087600708\n",
      "Epoch 1, Batch 2200, Char 95 Loss: 4.384711742401123\n",
      "Epoch 1, Batch 2200, Char 96 Loss: 1.6422570943832397\n",
      "Epoch 1, Batch 2200, Char 97 Loss: 2.3065876960754395\n",
      "Epoch 1, Batch 2200, Char 98 Loss: 1.3773797750473022\n",
      "Epoch 1, Batch 2300, Char 0 Loss: 2.572585105895996\n",
      "Epoch 1, Batch 2300, Char 1 Loss: 1.4310798645019531\n",
      "Epoch 1, Batch 2300, Char 2 Loss: 0.19831761717796326\n",
      "Epoch 1, Batch 2300, Char 3 Loss: 2.5521087646484375\n",
      "Epoch 1, Batch 2300, Char 4 Loss: 1.9522343873977661\n",
      "Epoch 1, Batch 2300, Char 5 Loss: 2.777292251586914\n",
      "Epoch 1, Batch 2300, Char 6 Loss: 1.2059743404388428\n",
      "Epoch 1, Batch 2300, Char 7 Loss: 0.8163037896156311\n",
      "Epoch 1, Batch 2300, Char 8 Loss: 1.8347411155700684\n",
      "Epoch 1, Batch 2300, Char 9 Loss: 1.2873154878616333\n",
      "Epoch 1, Batch 2300, Char 10 Loss: 3.0472910404205322\n",
      "Epoch 1, Batch 2300, Char 11 Loss: 1.5991557836532593\n",
      "Epoch 1, Batch 2300, Char 12 Loss: 3.2296488285064697\n",
      "Epoch 1, Batch 2300, Char 13 Loss: 0.25260698795318604\n",
      "Epoch 1, Batch 2300, Char 14 Loss: 3.077169418334961\n",
      "Epoch 1, Batch 2300, Char 15 Loss: 5.739405632019043\n",
      "Epoch 1, Batch 2300, Char 16 Loss: 4.223135471343994\n",
      "Epoch 1, Batch 2300, Char 17 Loss: 5.467050552368164\n",
      "Epoch 1, Batch 2300, Char 18 Loss: 2.4438629150390625\n",
      "Epoch 1, Batch 2300, Char 19 Loss: 3.25978422164917\n",
      "Epoch 1, Batch 2300, Char 20 Loss: 3.5587403774261475\n",
      "Epoch 1, Batch 2300, Char 21 Loss: 2.501878023147583\n",
      "Epoch 1, Batch 2300, Char 22 Loss: 0.24872049689292908\n",
      "Epoch 1, Batch 2300, Char 23 Loss: 2.728280544281006\n",
      "Epoch 1, Batch 2300, Char 24 Loss: 1.9673891067504883\n",
      "Epoch 1, Batch 2300, Char 25 Loss: 2.116486072540283\n",
      "Epoch 1, Batch 2300, Char 26 Loss: 3.1580638885498047\n",
      "Epoch 1, Batch 2300, Char 27 Loss: 1.8885667324066162\n",
      "Epoch 1, Batch 2300, Char 28 Loss: 0.9878543615341187\n",
      "Epoch 1, Batch 2300, Char 29 Loss: 2.881145477294922\n",
      "Epoch 1, Batch 2300, Char 30 Loss: 2.4258368015289307\n",
      "Epoch 1, Batch 2300, Char 31 Loss: 0.823513388633728\n",
      "Epoch 1, Batch 2300, Char 32 Loss: 0.983649492263794\n",
      "Epoch 1, Batch 2300, Char 33 Loss: 2.971219539642334\n",
      "Epoch 1, Batch 2300, Char 34 Loss: 0.8171762824058533\n",
      "Epoch 1, Batch 2300, Char 35 Loss: 3.4400014877319336\n",
      "Epoch 1, Batch 2300, Char 36 Loss: 2.3296608924865723\n",
      "Epoch 1, Batch 2300, Char 37 Loss: 0.24466755986213684\n",
      "Epoch 1, Batch 2300, Char 38 Loss: 3.69223952293396\n",
      "Epoch 1, Batch 2300, Char 39 Loss: 3.504390001296997\n",
      "Epoch 1, Batch 2300, Char 40 Loss: 3.7410192489624023\n",
      "Epoch 1, Batch 2300, Char 41 Loss: 5.365469932556152\n",
      "Epoch 1, Batch 2300, Char 42 Loss: 0.190640389919281\n",
      "Epoch 1, Batch 2300, Char 43 Loss: 3.0443496704101562\n",
      "Epoch 1, Batch 2300, Char 44 Loss: 2.0962038040161133\n",
      "Epoch 1, Batch 2300, Char 45 Loss: 1.102569341659546\n",
      "Epoch 1, Batch 2300, Char 46 Loss: 2.899296760559082\n",
      "Epoch 1, Batch 2300, Char 47 Loss: 0.7856820225715637\n",
      "Epoch 1, Batch 2300, Char 48 Loss: 1.8406496047973633\n",
      "Epoch 1, Batch 2300, Char 49 Loss: 1.2516895532608032\n",
      "Epoch 1, Batch 2300, Char 50 Loss: 3.615910768508911\n",
      "Epoch 1, Batch 2300, Char 51 Loss: 2.3035945892333984\n",
      "Epoch 1, Batch 2300, Char 52 Loss: 3.5650720596313477\n",
      "Epoch 1, Batch 2300, Char 53 Loss: 2.4477133750915527\n",
      "Epoch 1, Batch 2300, Char 54 Loss: 3.6535186767578125\n",
      "Epoch 1, Batch 2300, Char 55 Loss: 2.6536340713500977\n",
      "Epoch 1, Batch 2300, Char 56 Loss: 3.0995233058929443\n",
      "Epoch 1, Batch 2300, Char 57 Loss: 2.6199259757995605\n",
      "Epoch 1, Batch 2300, Char 58 Loss: 2.435673713684082\n",
      "Epoch 1, Batch 2300, Char 59 Loss: 0.24045434594154358\n",
      "Epoch 1, Batch 2300, Char 60 Loss: 2.862431049346924\n",
      "Epoch 1, Batch 2300, Char 61 Loss: 1.711965799331665\n",
      "Epoch 1, Batch 2300, Char 62 Loss: 2.617257595062256\n",
      "Epoch 1, Batch 2300, Char 63 Loss: 2.2013049125671387\n",
      "Epoch 1, Batch 2300, Char 64 Loss: 2.589552640914917\n",
      "Epoch 1, Batch 2300, Char 65 Loss: 1.4639972448349\n",
      "Epoch 1, Batch 2300, Char 66 Loss: 2.258591890335083\n",
      "Epoch 1, Batch 2300, Char 67 Loss: 2.8176465034484863\n",
      "Epoch 1, Batch 2300, Char 68 Loss: 1.4514164924621582\n",
      "Epoch 1, Batch 2300, Char 69 Loss: 4.0901031494140625\n",
      "Epoch 1, Batch 2300, Char 70 Loss: 0.32107144594192505\n",
      "Epoch 1, Batch 2300, Char 71 Loss: 1.82224702835083\n",
      "Epoch 1, Batch 2300, Char 72 Loss: 1.7855687141418457\n",
      "Epoch 1, Batch 2300, Char 73 Loss: 2.3666253089904785\n",
      "Epoch 1, Batch 2300, Char 74 Loss: 0.2483847737312317\n",
      "Epoch 1, Batch 2300, Char 75 Loss: 2.16909122467041\n",
      "Epoch 1, Batch 2300, Char 76 Loss: 1.112549066543579\n",
      "Epoch 1, Batch 2300, Char 77 Loss: 0.7513933181762695\n",
      "Epoch 1, Batch 2300, Char 78 Loss: 0.9996324181556702\n",
      "Epoch 1, Batch 2300, Char 79 Loss: 2.8700623512268066\n",
      "Epoch 1, Batch 2300, Char 80 Loss: 2.333876848220825\n",
      "Epoch 1, Batch 2300, Char 81 Loss: 2.817686080932617\n",
      "Epoch 1, Batch 2300, Char 82 Loss: 2.4031260013580322\n",
      "Epoch 1, Batch 2300, Char 83 Loss: 1.334923505783081\n",
      "Epoch 1, Batch 2300, Char 84 Loss: 1.8506214618682861\n",
      "Epoch 1, Batch 2300, Char 85 Loss: 2.3274335861206055\n",
      "Epoch 1, Batch 2300, Char 86 Loss: 4.77400016784668\n",
      "Epoch 1, Batch 2300, Char 87 Loss: 3.62813663482666\n",
      "Epoch 1, Batch 2300, Char 88 Loss: 2.5550241470336914\n",
      "Epoch 1, Batch 2300, Char 89 Loss: 2.232469081878662\n",
      "Epoch 1, Batch 2300, Char 90 Loss: 1.7298911809921265\n",
      "Epoch 1, Batch 2300, Char 91 Loss: 3.259105682373047\n",
      "Epoch 1, Batch 2300, Char 92 Loss: 2.2432494163513184\n",
      "Epoch 1, Batch 2300, Char 93 Loss: 1.0020357370376587\n",
      "Epoch 1, Batch 2300, Char 94 Loss: 3.011794090270996\n",
      "Epoch 1, Batch 2300, Char 95 Loss: 2.491405725479126\n",
      "Epoch 1, Batch 2300, Char 96 Loss: 0.7840241193771362\n",
      "Epoch 1, Batch 2300, Char 97 Loss: 2.1492228507995605\n",
      "Epoch 1, Batch 2300, Char 98 Loss: 1.1096813678741455\n",
      "Epoch 1, Batch 2400, Char 0 Loss: 5.337248802185059\n",
      "Epoch 1, Batch 2400, Char 1 Loss: 1.2460417747497559\n",
      "Epoch 1, Batch 2400, Char 2 Loss: 3.828054428100586\n",
      "Epoch 1, Batch 2400, Char 3 Loss: 5.175111770629883\n",
      "Epoch 1, Batch 2400, Char 4 Loss: 2.5968103408813477\n",
      "Epoch 1, Batch 2400, Char 5 Loss: 1.8211510181427002\n",
      "Epoch 1, Batch 2400, Char 6 Loss: 0.9188205003738403\n",
      "Epoch 1, Batch 2400, Char 7 Loss: 0.7032983899116516\n",
      "Epoch 1, Batch 2400, Char 8 Loss: 4.278540134429932\n",
      "Epoch 1, Batch 2400, Char 9 Loss: 1.847121000289917\n",
      "Epoch 1, Batch 2400, Char 10 Loss: 2.872683048248291\n",
      "Epoch 1, Batch 2400, Char 11 Loss: 1.6022846698760986\n",
      "Epoch 1, Batch 2400, Char 12 Loss: 1.2737901210784912\n",
      "Epoch 1, Batch 2400, Char 13 Loss: 2.856658458709717\n",
      "Epoch 1, Batch 2400, Char 14 Loss: 2.317528247833252\n",
      "Epoch 1, Batch 2400, Char 15 Loss: 3.580336570739746\n",
      "Epoch 1, Batch 2400, Char 16 Loss: 3.426295280456543\n",
      "Epoch 1, Batch 2400, Char 17 Loss: 2.425816774368286\n",
      "Epoch 1, Batch 2400, Char 18 Loss: 0.2471916377544403\n",
      "Epoch 1, Batch 2400, Char 19 Loss: 2.1893129348754883\n",
      "Epoch 1, Batch 2400, Char 20 Loss: 1.6323740482330322\n",
      "Epoch 1, Batch 2400, Char 21 Loss: 1.7381030321121216\n",
      "Epoch 1, Batch 2400, Char 22 Loss: 0.24530895054340363\n",
      "Epoch 1, Batch 2400, Char 23 Loss: 2.483116626739502\n",
      "Epoch 1, Batch 2400, Char 24 Loss: 1.191988468170166\n",
      "Epoch 1, Batch 2400, Char 25 Loss: 1.7709972858428955\n",
      "Epoch 1, Batch 2400, Char 26 Loss: 2.5282211303710938\n",
      "Epoch 1, Batch 2400, Char 27 Loss: 1.9565589427947998\n",
      "Epoch 1, Batch 2400, Char 28 Loss: 1.4714077711105347\n",
      "Epoch 1, Batch 2400, Char 29 Loss: 2.364671468734741\n",
      "Epoch 1, Batch 2400, Char 30 Loss: 0.24189919233322144\n",
      "Epoch 1, Batch 2400, Char 31 Loss: 1.8240348100662231\n",
      "Epoch 1, Batch 2400, Char 32 Loss: 0.9218894839286804\n",
      "Epoch 1, Batch 2400, Char 33 Loss: 0.6773236393928528\n",
      "Epoch 1, Batch 2400, Char 34 Loss: 4.161168098449707\n",
      "Epoch 1, Batch 2400, Char 35 Loss: 1.7914841175079346\n",
      "Epoch 1, Batch 2400, Char 36 Loss: 2.4461143016815186\n",
      "Epoch 1, Batch 2400, Char 37 Loss: 1.154242992401123\n",
      "Epoch 1, Batch 2400, Char 38 Loss: 2.6392946243286133\n",
      "Epoch 1, Batch 2400, Char 39 Loss: 2.670077323913574\n",
      "Epoch 1, Batch 2400, Char 40 Loss: 2.857417106628418\n",
      "Epoch 1, Batch 2400, Char 41 Loss: 2.7270336151123047\n",
      "Epoch 1, Batch 2400, Char 42 Loss: 0.17852453887462616\n",
      "Epoch 1, Batch 2400, Char 43 Loss: 2.1615476608276367\n",
      "Epoch 1, Batch 2400, Char 44 Loss: 1.605112075805664\n",
      "Epoch 1, Batch 2400, Char 45 Loss: 1.6569675207138062\n",
      "Epoch 1, Batch 2400, Char 46 Loss: 0.23888640105724335\n",
      "Epoch 1, Batch 2400, Char 47 Loss: 2.325111150741577\n",
      "Epoch 1, Batch 2400, Char 48 Loss: 3.377002239227295\n",
      "Epoch 1, Batch 2400, Char 49 Loss: 3.475403308868408\n",
      "Epoch 1, Batch 2400, Char 50 Loss: 4.445896148681641\n",
      "Epoch 1, Batch 2400, Char 51 Loss: 1.8042646646499634\n",
      "Epoch 1, Batch 2400, Char 52 Loss: 2.746162176132202\n",
      "Epoch 1, Batch 2400, Char 53 Loss: 3.268604040145874\n",
      "Epoch 1, Batch 2400, Char 54 Loss: 14.026335716247559\n",
      "Epoch 1, Batch 2400, Char 55 Loss: 3.054638385772705\n",
      "Epoch 1, Batch 2400, Char 56 Loss: 2.118955612182617\n",
      "Epoch 1, Batch 2400, Char 57 Loss: 1.7472695112228394\n",
      "Epoch 1, Batch 2400, Char 58 Loss: 2.636315107345581\n",
      "Epoch 1, Batch 2400, Char 59 Loss: 0.1764085590839386\n",
      "Epoch 1, Batch 2400, Char 60 Loss: 3.48598575592041\n",
      "Epoch 1, Batch 2400, Char 61 Loss: 1.7279187440872192\n",
      "Epoch 1, Batch 2400, Char 62 Loss: 2.3216307163238525\n",
      "Epoch 1, Batch 2400, Char 63 Loss: 1.2897518873214722\n",
      "Epoch 1, Batch 2400, Char 64 Loss: 2.1352038383483887\n",
      "Epoch 1, Batch 2400, Char 65 Loss: 3.0253217220306396\n",
      "Epoch 1, Batch 2400, Char 66 Loss: 3.4647860527038574\n",
      "Epoch 1, Batch 2400, Char 67 Loss: 1.1781920194625854\n",
      "Epoch 1, Batch 2400, Char 68 Loss: 1.7382574081420898\n",
      "Epoch 1, Batch 2400, Char 69 Loss: 2.3186168670654297\n",
      "Epoch 1, Batch 2400, Char 70 Loss: 2.430311918258667\n",
      "Epoch 1, Batch 2400, Char 71 Loss: 1.7289600372314453\n",
      "Epoch 1, Batch 2400, Char 72 Loss: 0.9581693410873413\n",
      "Epoch 1, Batch 2400, Char 73 Loss: 2.8296914100646973\n",
      "Epoch 1, Batch 2400, Char 74 Loss: 2.2531137466430664\n",
      "Epoch 1, Batch 2400, Char 75 Loss: 0.8207536339759827\n",
      "Epoch 1, Batch 2400, Char 76 Loss: 4.0041069984436035\n",
      "Epoch 1, Batch 2400, Char 77 Loss: 4.226927757263184\n",
      "Epoch 1, Batch 2400, Char 78 Loss: 3.4019596576690674\n",
      "Epoch 1, Batch 2400, Char 79 Loss: 3.253350257873535\n",
      "Epoch 1, Batch 2400, Char 80 Loss: 0.9851292371749878\n",
      "Epoch 1, Batch 2400, Char 81 Loss: 3.392610549926758\n",
      "Epoch 1, Batch 2400, Char 82 Loss: 2.7563228607177734\n",
      "Epoch 1, Batch 2400, Char 83 Loss: 0.1783469170331955\n",
      "Epoch 1, Batch 2400, Char 84 Loss: 3.7393436431884766\n",
      "Epoch 1, Batch 2400, Char 85 Loss: 2.0780997276306152\n",
      "Epoch 1, Batch 2400, Char 86 Loss: 4.037372589111328\n",
      "Epoch 1, Batch 2400, Char 87 Loss: 5.339696407318115\n",
      "Epoch 1, Batch 2400, Char 88 Loss: 2.965364456176758\n",
      "Epoch 1, Batch 2400, Char 89 Loss: 1.585953950881958\n",
      "Epoch 1, Batch 2400, Char 90 Loss: 2.18302059173584\n",
      "Epoch 1, Batch 2400, Char 91 Loss: 1.3783479928970337\n",
      "Epoch 1, Batch 2400, Char 92 Loss: 1.8902682065963745\n",
      "Epoch 1, Batch 2400, Char 93 Loss: 0.9239715933799744\n",
      "Epoch 1, Batch 2400, Char 94 Loss: 0.6628165245056152\n",
      "Epoch 1, Batch 2400, Char 95 Loss: 0.947350800037384\n",
      "Epoch 1, Batch 2400, Char 96 Loss: 3.922276496887207\n",
      "Epoch 1, Batch 2400, Char 97 Loss: 3.6817145347595215\n",
      "Epoch 1, Batch 2400, Char 98 Loss: 3.2174103260040283\n",
      "Epoch 1, Batch 2500, Char 0 Loss: 2.7680301666259766\n",
      "Epoch 1, Batch 2500, Char 1 Loss: 1.7080967426300049\n",
      "Epoch 1, Batch 2500, Char 2 Loss: 3.1595547199249268\n",
      "Epoch 1, Batch 2500, Char 3 Loss: 4.288847923278809\n",
      "Epoch 1, Batch 2500, Char 4 Loss: 0.7564268112182617\n",
      "Epoch 1, Batch 2500, Char 5 Loss: 2.9046502113342285\n",
      "Epoch 1, Batch 2500, Char 6 Loss: 2.136704206466675\n",
      "Epoch 1, Batch 2500, Char 7 Loss: 2.3744349479675293\n",
      "Epoch 1, Batch 2500, Char 8 Loss: 2.5680012702941895\n",
      "Epoch 1, Batch 2500, Char 9 Loss: 2.9084534645080566\n",
      "Epoch 1, Batch 2500, Char 10 Loss: 5.353058815002441\n",
      "Epoch 1, Batch 2500, Char 11 Loss: 0.15294502675533295\n",
      "Epoch 1, Batch 2500, Char 12 Loss: 3.2679715156555176\n",
      "Epoch 1, Batch 2500, Char 13 Loss: 1.7231935262680054\n",
      "Epoch 1, Batch 2500, Char 14 Loss: 1.9024635553359985\n",
      "Epoch 1, Batch 2500, Char 15 Loss: 1.8660191297531128\n",
      "Epoch 1, Batch 2500, Char 16 Loss: 2.9685449600219727\n",
      "Epoch 1, Batch 2500, Char 17 Loss: 1.279920220375061\n",
      "Epoch 1, Batch 2500, Char 18 Loss: 1.469046711921692\n",
      "Epoch 1, Batch 2500, Char 19 Loss: 2.7393627166748047\n",
      "Epoch 1, Batch 2500, Char 20 Loss: 2.1957337856292725\n",
      "Epoch 1, Batch 2500, Char 21 Loss: 4.121345520019531\n",
      "Epoch 1, Batch 2500, Char 22 Loss: 1.958430528640747\n",
      "Epoch 1, Batch 2500, Char 23 Loss: 2.2551920413970947\n",
      "Epoch 1, Batch 2500, Char 24 Loss: 0.15304915606975555\n",
      "Epoch 1, Batch 2500, Char 25 Loss: 2.99806547164917\n",
      "Epoch 1, Batch 2500, Char 26 Loss: 0.8242129683494568\n",
      "Epoch 1, Batch 2500, Char 27 Loss: 3.368229389190674\n",
      "Epoch 1, Batch 2500, Char 28 Loss: 1.914932131767273\n",
      "Epoch 1, Batch 2500, Char 29 Loss: 1.7290186882019043\n",
      "Epoch 1, Batch 2500, Char 30 Loss: 1.0865859985351562\n",
      "Epoch 1, Batch 2500, Char 31 Loss: 3.577049970626831\n",
      "Epoch 1, Batch 2500, Char 32 Loss: 3.661465644836426\n",
      "Epoch 1, Batch 2500, Char 33 Loss: 1.783756971359253\n",
      "Epoch 1, Batch 2500, Char 34 Loss: 1.437730073928833\n",
      "Epoch 1, Batch 2500, Char 35 Loss: 2.8466615676879883\n",
      "Epoch 1, Batch 2500, Char 36 Loss: 0.6163103580474854\n",
      "Epoch 1, Batch 2500, Char 37 Loss: 1.8648042678833008\n",
      "Epoch 1, Batch 2500, Char 38 Loss: 1.1582348346710205\n",
      "Epoch 1, Batch 2500, Char 39 Loss: 1.9085216522216797\n",
      "Epoch 1, Batch 2500, Char 40 Loss: 2.14045786857605\n",
      "Epoch 1, Batch 2500, Char 41 Loss: 4.89094877243042\n",
      "Epoch 1, Batch 2500, Char 42 Loss: 4.789607048034668\n",
      "Epoch 1, Batch 2500, Char 43 Loss: 0.7824814915657043\n",
      "Epoch 1, Batch 2500, Char 44 Loss: 1.9002374410629272\n",
      "Epoch 1, Batch 2500, Char 45 Loss: 2.1096394062042236\n",
      "Epoch 1, Batch 2500, Char 46 Loss: 2.912224531173706\n",
      "Epoch 1, Batch 2500, Char 47 Loss: 1.8255606889724731\n",
      "Epoch 1, Batch 2500, Char 48 Loss: 2.033267021179199\n",
      "Epoch 1, Batch 2500, Char 49 Loss: 1.401336669921875\n",
      "Epoch 1, Batch 2500, Char 50 Loss: 0.3440546691417694\n",
      "Epoch 1, Batch 2500, Char 51 Loss: 2.00616717338562\n",
      "Epoch 1, Batch 2500, Char 52 Loss: 1.2763030529022217\n",
      "Epoch 1, Batch 2500, Char 53 Loss: 0.605807363986969\n",
      "Epoch 1, Batch 2500, Char 54 Loss: 1.0835649967193604\n",
      "Epoch 1, Batch 2500, Char 55 Loss: 2.938572645187378\n",
      "Epoch 1, Batch 2500, Char 56 Loss: 1.496292233467102\n",
      "Epoch 1, Batch 2500, Char 57 Loss: 3.558805465698242\n",
      "Epoch 1, Batch 2500, Char 58 Loss: 1.077924370765686\n",
      "Epoch 1, Batch 2500, Char 59 Loss: 2.7092204093933105\n",
      "Epoch 1, Batch 2500, Char 60 Loss: 3.927645206451416\n",
      "Epoch 1, Batch 2500, Char 61 Loss: 2.4224979877471924\n",
      "Epoch 1, Batch 2500, Char 62 Loss: 0.33867430686950684\n",
      "Epoch 1, Batch 2500, Char 63 Loss: 3.7310800552368164\n",
      "Epoch 1, Batch 2500, Char 64 Loss: 2.690659523010254\n",
      "Epoch 1, Batch 2500, Char 65 Loss: 2.826206684112549\n",
      "Epoch 1, Batch 2500, Char 66 Loss: 5.142140865325928\n",
      "Epoch 1, Batch 2500, Char 67 Loss: 0.1478639543056488\n",
      "Epoch 1, Batch 2500, Char 68 Loss: 1.8791208267211914\n",
      "Epoch 1, Batch 2500, Char 69 Loss: 1.4441490173339844\n",
      "Epoch 1, Batch 2500, Char 70 Loss: 1.349877953529358\n",
      "Epoch 1, Batch 2500, Char 71 Loss: 0.33798912167549133\n",
      "Epoch 1, Batch 2500, Char 72 Loss: 2.802736759185791\n",
      "Epoch 1, Batch 2500, Char 73 Loss: 4.990523338317871\n",
      "Epoch 1, Batch 2500, Char 74 Loss: 2.686105728149414\n",
      "Epoch 1, Batch 2500, Char 75 Loss: 3.9242310523986816\n",
      "Epoch 1, Batch 2500, Char 76 Loss: 1.895614504814148\n",
      "Epoch 1, Batch 2500, Char 77 Loss: 2.3046088218688965\n",
      "Epoch 1, Batch 2500, Char 78 Loss: 0.3365302085876465\n",
      "Epoch 1, Batch 2500, Char 79 Loss: 2.772704601287842\n",
      "Epoch 1, Batch 2500, Char 80 Loss: 0.5843039155006409\n",
      "Epoch 1, Batch 2500, Char 81 Loss: 1.8463008403778076\n",
      "Epoch 1, Batch 2500, Char 82 Loss: 1.1430808305740356\n",
      "Epoch 1, Batch 2500, Char 83 Loss: 2.741792917251587\n",
      "Epoch 1, Batch 2500, Char 84 Loss: 1.9123297929763794\n",
      "Epoch 1, Batch 2500, Char 85 Loss: 2.0382771492004395\n",
      "Epoch 1, Batch 2500, Char 86 Loss: 4.031740188598633\n",
      "Epoch 1, Batch 2500, Char 87 Loss: 0.3324587345123291\n",
      "Epoch 1, Batch 2500, Char 88 Loss: 2.031043767929077\n",
      "Epoch 1, Batch 2500, Char 89 Loss: 1.2136504650115967\n",
      "Epoch 1, Batch 2500, Char 90 Loss: 0.5863966941833496\n",
      "Epoch 1, Batch 2500, Char 91 Loss: 1.086092472076416\n",
      "Epoch 1, Batch 2500, Char 92 Loss: 2.6649088859558105\n",
      "Epoch 1, Batch 2500, Char 93 Loss: 3.1416406631469727\n",
      "Epoch 1, Batch 2500, Char 94 Loss: 1.9768414497375488\n",
      "Epoch 1, Batch 2500, Char 95 Loss: 2.464515209197998\n",
      "Epoch 1, Batch 2500, Char 96 Loss: 5.308771133422852\n",
      "Epoch 1, Batch 2500, Char 97 Loss: 3.419874668121338\n",
      "Epoch 1, Batch 2500, Char 98 Loss: 3.6122539043426514\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train and calculate loss for each character\u001b[39;00m\n\u001b[1;32m     14\u001b[0m target_char \u001b[38;5;241m=\u001b[39m input_line_tensor[char_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m output, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhot_input_char_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_char\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Char \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchar_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(input_line_tensor, target_char_tensor)\u001b[0m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(chars)), target_char_tensor)\n\u001b[1;32m     13\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 14\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/optim/adam.py:432\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    434\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, 101):\n",
    "    for batch_idx, (inputs) in enumerate(dataloader):\n",
    "        # Initialize variables to store the history and predicted characters for each batch\n",
    "        history = []\n",
    "        predicted_chars = []\n",
    "\n",
    "        input_line_tensor = inputs[0]  # Get the first character\n",
    "        for char_idx in range(input_line_tensor.shape[0] - 1):\n",
    "            # Convert to one-hot encoding for each character\n",
    "            hot_input_char_tensor = torch.nn.functional.one_hot(input_line_tensor[char_idx], num_classes=n_characters).type(torch.float)\n",
    "            \n",
    "            # Train and calculate loss for each character\n",
    "            target_char = input_line_tensor[char_idx + 1].unsqueeze(0)\n",
    "            output, loss = train(hot_input_char_tensor.unsqueeze(0), target_char)\n",
    "           \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Char {char_idx} Loss: {loss}')\n",
    "\n",
    "            if batch_idx % 1000 == 0:\n",
    "                # Use the output to generate a character prediction\n",
    "                topv, topi = output.topk(1, dim=1)  # Change dim to 1\n",
    "                predicted_char = text_dataset.idx_to_char[topi[0, 0].item()]\n",
    "                target_char = text_dataset.idx_to_char[target_char.item()]\n",
    "\n",
    "                # Append the current character and prediction to their respective lists\n",
    "                history.append(target_char)\n",
    "                predicted_chars.append(predicted_char)\n",
    "\n",
    "                # Display the summarized history\n",
    "                history_str = ''.join(history)\n",
    "                predicted_str = ''.join(predicted_chars)\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}, Char {char_idx}\\ntarget: {target_char} predicted: {predicted_char}\\nHistory: \"{history_str}\", Predicted: \"{predicted_str}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
