
@inproceedings{mikolovRecurrentNeuralNetwork2010,
	title = {Recurrent neural network based language model},
	url = {https://www.isca-archive.org/interspeech_2010/mikolov10_interspeech.html},
	doi = {10.21437/Interspeech.2010-343},
	abstract = {A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50\% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18\% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5\% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition},
	language = {en},
	urldate = {2025-03-04},
	booktitle = {Interspeech 2010},
	publisher = {ISCA},
	author = {Mikolov, Tomáš and Karafiát, Martin and Burget, Lukáš and Černocký, Jan and Khudanpur, Sanjeev},
	month = sep,
	year = {2010},
	pages = {1045--1048},
}

@misc{khoslaSurveyMemoryAugmentedNeural2023,
	title = {Survey on {Memory}-{Augmented} {Neural} {Networks}: {Cognitive} {Insights} to {AI} {Applications}},
	shorttitle = {Survey on {Memory}-{Augmented} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2312.06141},
	doi = {10.48550/arXiv.2312.06141},
	abstract = {This paper explores Memory-Augmented Neural Networks (MANNs), delving into how they blend human-like memory processes into AI. It covers different memory types, like sensory, short-term, and long-term memory, linking psychological theories with AI applications. The study investigates advanced architectures such as Hopfield Networks, Neural Turing Machines, Correlation Matrix Memories, Memformer, and Neural Attention Memory, explaining how they work and where they excel. It dives into real-world uses of MANNs across Natural Language Processing, Computer Vision, Multimodal Learning, and Retrieval Models, showing how memory boosters enhance accuracy, efficiency, and reliability in AI tasks. Overall, this survey provides a comprehensive view of MANNs, offering insights for future research in memory-based AI systems.},
	urldate = {2025-03-04},
	publisher = {arXiv},
	author = {Khosla, Savya and Zhu, Zhen and He, Yifei},
	month = dec,
	year = {2023},
	note = {arXiv:2312.06141 [cs]
version: 2},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/YN9MIJX9/Khosla et al. - 2023 - Survey on Memory-Augmented Neural Networks Cognitive Insights to AI Applications.pdf:application/pdf;Snapshot:/home/jaden/snap/zotero-snap/common/Zotero/storage/XDATIDNX/2312.html:text/html},
}

@misc{applegarthExploringSynapticResonance2025,
	title = {Exploring {Synaptic} {Resonance} in {Large} {Language} {Models}: {A} {Novel} {Approach} to {Contextual} {Memory} {Integration}},
	shorttitle = {Exploring {Synaptic} {Resonance} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2502.10699},
	doi = {10.48550/arXiv.2502.10699},
	abstract = {Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence over extended sequences. Traditional approaches, such as self-attention mechanisms and memory-augmented architectures, often prioritize short-term dependencies, leading to fragmentation and inconsistency in long-range contextual understanding. Inspired by principles of synaptic plasticity observed in biological neural systems, a novel mechanism, Synaptic Resonance, is introduced to dynamically reinforce relevant memory pathways during training and inference. Unlike static memory representations, this mechanism continuously adjusts synaptic weight matrices based on contextual relevance, allowing for improved information retention without excessive computational overhead. Evaluations conducted on an open-source language model demonstrate reductions in perplexity, enhancements in contextual coherence, and increased robustness against input noise, highlighting the effectiveness of reinforcement-driven memory modulation. Comparative analysis against baseline models further reveals that the proposed approach achieves higher memory retention efficiency while maintaining computational feasibility. The architectural modifications integrate seamlessly into existing transformer-based frameworks, ensuring stable convergence and efficient inference without sacrificing scalability. Applications benefiting from improved long-term contextual consistency, such as dialogue systems and document summarization, stand to gain from this approach. Empirical findings suggest that dynamically reinforced memory pathways offer a promising alternative to conventional memory mechanisms, addressing longstanding limitations in extended sequence modeling.},
	urldate = {2025-03-04},
	publisher = {arXiv},
	author = {Applegarth, George and Weatherstone, Christian and Hollingsworth, Maximilian and Middlebrook, Henry and Irvin, Marcus},
	month = feb,
	year = {2025},
	note = {arXiv:2502.10699 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/24BRHVMB/Applegarth et al. - 2025 - Exploring Synaptic Resonance in Large Language Models A Novel Approach to Contextual Memory Integra.pdf:application/pdf;Snapshot:/home/jaden/snap/zotero-snap/common/Zotero/storage/XLYJGR2T/2502.html:text/html},
}

@article{westonMemoryNetworks2014,
	title = {Memory {Networks}},
	url = {https://www.semanticscholar.org/paper/Memory-Networks-Weston-Chopra/71ae756c75ac89e2d731c9c79649562b5768ff39},
	abstract = {Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	urldate = {2025-03-04},
	journal = {CoRR},
	author = {Weston, J. and Chopra, S. and Bordes, Antoine},
	month = oct,
	year = {2014},
	file = {Full Text PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/JN3LRJQR/Weston et al. - 2014 - Memory Networks.pdf:application/pdf},
}

@inproceedings{santoroMetaLearningMemoryAugmentedNeural2016,
	title = {Meta-{Learning} with {Memory}-{Augmented} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v48/santoro16.html},
	abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
	language = {en},
	urldate = {2025-03-04},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {1842--1850},
	file = {Full Text PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/7AYFANM8/Santoro et al. - 2016 - Meta-Learning with Memory-Augmented Neural Networks.pdf:application/pdf},
}

@inproceedings{wuMemorizingTransformers2022,
	title = {Memorizing transformers},
	url = {https://openreview.net/forum?id=TrjbxzRcnf-},
	booktitle = {The tenth international conference on learning representations, {ICLR} 2022, virtual event, april 25-29, 2022},
	publisher = {OpenReview.net},
	author = {Wu, Yuhuai and Rabe, Markus Norman and Hutchins, DeLesley and Szegedy, Christian},
	year = {2022},
	note = {tex.bibsource: dblp computer science bibliography, https://dblp.org
tex.timestamp: Sat, 20 Aug 2022 01:15:42 +0200},
}

@article{Tack2024OnlineAO,
	title = {Online adaptation of language models with a memory of amortized contexts},
	volume = {abs/2403.04317},
	url = {https://api.semanticscholar.org/CorpusID:268264809},
	journal = {ArXiv},
	author = {Tack, Jihoon and Kim, Jaehyung and Mitchell, Eric and Shin, Jinwoo and Teh, Yee Whye and Schwarz, Jonathan Richard},
	year = {2024},
	file = {PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/JBUUKG84/Tack et al. - Online Adaptation of Language Models with a Memory of Amortized Contexts.pdf:application/pdf},
}

@article{LongMem,
	title = {Augmenting language models with long-term memory},
	journal = {arXiv preprint arXiv:2306.07174},
	author = {Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
	year = {2023},
}

@inproceedings{miconiDifferentiablePlasticityTraining2018a,
	title = {Differentiable plasticity: training plastic neural networks with backpropagation},
	shorttitle = {Differentiable plasticity},
	url = {https://proceedings.mlr.press/v80/miconi18a.html},
	abstract = {How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains: synaptic plasticity, carefully tuned by evolution to produce efficient lifelong learning. We show that plasticity, just like connection weights, can be optimized by gradient descent in large (millions of parameters) recurrent networks with Hebbian plastic connections. First, recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel, high-dimensional (1000+ pixels) natural images not seen during training. Crucially, traditional non-plastic recurrent networks fail to solve this task. Furthermore, trained plastic networks can also solve generic meta-learning tasks such as the Omniglot task, with competitive results and little parameter overhead. Finally, in reinforcement learning settings, plastic networks outperform non-plastic equivalent in a maze exploration task. We conclude that differentiable plasticity may provide a powerful novel approach to the learning-to-learn problem.},
	language = {en},
	urldate = {2025-03-04},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Miconi, Thomas and Stanley, Kenneth and Clune, Jeff},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {3559--3568},
	file = {Full Text PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/NZJ93QHA/Miconi et al. - 2018 - Differentiable plasticity training plastic neural networks with backpropagation.pdf:application/pdf;Supplementary PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/SLAUE23H/Miconi et al. - 2018 - Differentiable plasticity training plastic neural networks with backpropagation.pdf:application/pdf},
}

@inproceedings{clark-etal-2022-meta,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Meta-learning fast weight language models},
	url = {https://aclanthology.org/2022.emnlp-main.661/},
	doi = {10.18653/v1/2022.emnlp-main.661},
	abstract = {Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates. FWLs can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.},
	booktitle = {Proceedings of the 2022 conference on empirical methods in natural language processing},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Kevin and Guu, Kelvin and Chang, Ming-Wei and Pasupat, Panupong and Hinton, Geoffrey and Norouzi, Mohammad},
	editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
	month = dec,
	year = {2022},
	pages = {9751--9757},
	file = {PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/ZAWUDU35/Clark et al. - 2022 - Meta-Learning Fast Weight Language Models.pdf:application/pdf},
}

@inproceedings{finnModelagnosticMetalearningFast2017,
	series = {{ICML}'17},
	title = {Model-agnostic meta-learning for fast adaptation of deep networks},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	booktitle = {Proceedings of the 34th international conference on machine learning - volume 70},
	publisher = {JMLR.org},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	year = {2017},
	note = {Place: Sydney, NSW, Australia
Number of pages: 10},
	pages = {1126--1135},
}

@inproceedings{shinContinualLearningDeep2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Continual learning with deep generative replay},
	isbn = {978-1-5108-6096-4},
	abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
	booktitle = {Proceedings of the 31st international conference on neural information processing systems},
	publisher = {Curran Associates Inc.},
	author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
	year = {2017},
	note = {Number of pages: 10
Place: Long Beach, California, USA},
	pages = {2994--3003},
}

@article{mnihHumanlevelControlDeep2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2025-03-04},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science},
	pages = {529--533},
}

@article{hintonUsingFastWeights1987,
	title = {Using {Fast} {Weights} to {Deblur} {Old} {Memories}},
	volume = {9},
	url = {https://escholarship.org/uc/item/0570j1dp},
	abstract = {Connectionist models usually have a single weight on each connection. Some interesting newproperties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associationsare "blurred" by subsequent learning, all the original associations can be "deblurred" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel outthe changes in the slow weights caused by the subsequent learning.},
	language = {en},
	number = {0},
	urldate = {2025-03-04},
	journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Hinton, Geoffrey E. and Plaut, David C.},
	year = {1987},
	file = {Full Text PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/YZLP9MLD/Hinton and Plaut - 1987 - Using Fast Weights to Deblur Old Memories.pdf:application/pdf},
}

@misc{baUsingFastWeights2016,
	title = {Using {Fast} {Weights} to {Attend} to the {Recent} {Past}},
	url = {http://arxiv.org/abs/1610.06258},
	doi = {10.48550/arXiv.1610.06258},
	abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
	urldate = {2025-03-04},
	publisher = {arXiv},
	author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
	month = dec,
	year = {2016},
	note = {arXiv:1610.06258 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Preprint PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/HQGVLZPL/Ba et al. - 2016 - Using Fast Weights to Attend to the Recent Past.pdf:application/pdf;Snapshot:/home/jaden/snap/zotero-snap/common/Zotero/storage/8L5RBMV9/1610.html:text/html},
}

@article{Forgetting_Survey_2024,
	title = {A comprehensive survey of forgetting in deep learning beyond continual learning},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Wang, Zhenyi and Yang, Enneng and Shen, Li and Huang, Heng},
	year = {2024},
	note = {Publisher: IEEE},
}

@article{Graves2014NeuralTM,
	title = {Neural turing machines},
	volume = {abs/1410.5401},
	url = {https://api.semanticscholar.org/CorpusID:15299054},
	journal = {ArXiv},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	year = {2014},
}

@inproceedings{wangAugmentingLanguageModels2023,
	address = {Red Hook, NY, USA},
	series = {Nips '23},
	title = {Augmenting language models with long-term memory},
	abstract = {Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LONGMEM), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LONGMEM can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LONGMEM can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.},
	booktitle = {Proceedings of the 37th international conference on neural information processing systems},
	publisher = {Curran Associates Inc.},
	author = {Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
	year = {2023},
	note = {Number of pages: 14
Place: New Orleans, LA, USA
tex.articleno: 3259},
}

@article{schmidhuberLearningControlFastweight1992,
	title = {Learning to control fast-weight memories: {An} alternative to dynamic recurrent networks},
	volume = {4},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1992.4.1.131},
	doi = {10.1162/neco.1992.4.1.131},
	abstract = {Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding.},
	number = {1},
	journal = {Neural Computation},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {1992},
	note = {tex.eprint: https://direct.mit.edu/neco/article-pdf/4/1/131/812242/neco.1992.4.1.131.pdf},
	pages = {131--139},
}

@inproceedings{irieGoingLinearTransformers2021a,
	address = {Red Hook, NY, USA},
	series = {Nips '21},
	title = {Going beyond linear transformers with recurrent fast weight programmers},
	isbn = {978-1-7138-4539-3},
	abstract = {Transformers with linearised attention ("linear Transformers") have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight Programmers (FWPs) from the '90s. However, the original FWP formulation is more general than the one of linear Transformers: a slow neural network (NN) continually reprograms the weights of a fast NN with arbitrary architecture. In existing linear Transformers, both NNs are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of Transformers and RNNs. In the reinforcement learning setting, we report large improvements over LSTM in several Atari games. Our code is public.},
	booktitle = {Proceedings of the 35th international conference on neural information processing systems},
	publisher = {Curran Associates Inc.},
	author = {Irie, Kazuki and Schlag, Imanol and Csordás, Róbert and Schmidhuber, Jürgen},
	year = {2021},
	note = {Number of pages: 15
tex.articleno: 590},
	file = {PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/PG7B6CZZ/Irie et al. - 2021 - Going Beyond Linear Transformers with Recurrent Fast Weight Programmers.pdf:application/pdf},
}

@inproceedings{pmlr-v80-krause18a,
	series = {Proceedings of machine learning research},
	title = {Dynamic evaluation of neural sequence models},
	volume = {80},
	url = {https://proceedings.mlr.press/v80/krause18a.html},
	abstract = {We explore dynamic evaluation, where sequence models are adapted to the recent sequence history using gradient descent, assigning higher probabilities to re-occurring sequential patterns. We develop a dynamic evaluation approach that outperforms existing adaptation approaches in our comparisons. We apply dynamic evaluation to outperform all previous word-level perplexities on the Penn Treebank and WikiText-2 datasets (achieving 51.1 and 44.3 respectively) and all previous character-level cross-entropies on the text8 and Hutter Prize datasets (achieving 1.19 bits/char and 1.08 bits/char respectively).},
	booktitle = {Proceedings of the 35th international conference on machine learning},
	publisher = {PMLR},
	author = {Krause, Ben and Kahembwe, Emmanuel and Murray, Iain and Renals, Steve},
	editor = {Dy, Jennifer and Krause, Andreas},
	month = jul,
	year = {2018},
	pages = {2766--2775},
	file = {PDF:/home/jaden/snap/zotero-snap/common/Zotero/storage/J6XHV2IG/Krause et al. - Dynamic Evaluation of Neural Sequence Models.pdf:application/pdf},
}