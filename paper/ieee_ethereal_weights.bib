@misc{applegarthExploringSynapticResonance2025,
  title = {Exploring {{Synaptic Resonance}} in {{Large Language Models}}: {{A Novel Approach}} to {{Contextual Memory Integration}}},
  shorttitle = {Exploring {{Synaptic Resonance}} in {{Large Language Models}}},
  author = {Applegarth, George and Weatherstone, Christian and Hollingsworth, Maximilian and Middlebrook, Henry and Irvin, Marcus},
  year = 2025,
  month = feb,
  number = {arXiv:2502.10699},
  eprint = {2502.10699},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.10699},
  urldate = {2025-03-04},
  abstract = {Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence over extended sequences. Traditional approaches, such as self-attention mechanisms and memory-augmented architectures, often prioritize short-term dependencies, leading to fragmentation and inconsistency in long-range contextual understanding. Inspired by principles of synaptic plasticity observed in biological neural systems, a novel mechanism, Synaptic Resonance, is introduced to dynamically reinforce relevant memory pathways during training and inference. Unlike static memory representations, this mechanism continuously adjusts synaptic weight matrices based on contextual relevance, allowing for improved information retention without excessive computational overhead. Evaluations conducted on an open-source language model demonstrate reductions in perplexity, enhancements in contextual coherence, and increased robustness against input noise, highlighting the effectiveness of reinforcement-driven memory modulation. Comparative analysis against baseline models further reveals that the proposed approach achieves higher memory retention efficiency while maintaining computational feasibility. The architectural modifications integrate seamlessly into existing transformer-based frameworks, ensuring stable convergence and efficient inference without sacrificing scalability. Applications benefiting from improved long-term contextual consistency, such as dialogue systems and document summarization, stand to gain from this approach. Empirical findings suggest that dynamically reinforced memory pathways offer a promising alternative to conventional memory mechanisms, addressing longstanding limitations in extended sequence modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/24BRHVMB/Applegarth et al. - 2025 - Exploring Synaptic Resonance in Large Language Models A Novel Approach to Contextual Memory Integra.pdf;/home/jaden/snap/zotero-snap/common/Zotero/storage/XLYJGR2T/2502.html}
}

@misc{baUsingFastWeights2016,
  title = {Using {{Fast Weights}} to {{Attend}} to the {{Recent Past}}},
  author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
  year = 2016,
  month = dec,
  number = {arXiv:1610.06258},
  eprint = {1610.06258},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.06258},
  urldate = {2025-03-04},
  abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/HQGVLZPL/Ba et al. - 2016 - Using Fast Weights to Attend to the Recent Past.pdf;/home/jaden/snap/zotero-snap/common/Zotero/storage/8L5RBMV9/1610.html}
}

@article{behrouzNestedLearningIllusiona,
  title = {Nested {{Learning}}: {{The Illusion}} of {{Deep Learning Architectures}}},
  author = {Behrouz, Ali and Razaviyayn, Meisam and Zhong, Peiling and Mirrokni, Vahab},
  abstract = {Over the last decades, developing more powerful neural architectures and simultaneously designing optimization algorithms to effectively train them have been the core of research efforts to enhance the capability of machine learning models. Despite the recent progresses, particularly in developing Language Models (LMs), there are fundamental challenges and unanswered questions about how such models can continually learn/memorize, self-improved, and find ``effective solutions,''. In this paper, we present a new learning paradigm, called Nested Learning (NL), that coherently represents a model with a set of nested, multi-level, and/or parallel optimization problems, each of which with its own ``context flow''. NL reveals that existing deep learning methods learns from data through compressing their own context flow, and explain how in-context learning emerges in large models. NL suggests a path (a new dimension to deep learning) to design more expressive learning algorithms with more ``levels'', resulting in higher-order in-context learning abilities. In addition to its neuroscientifically plausible and mathematically white-box nature, we advocate for its importance by presenting three core contributions: (1) Deep Optimizers: Based on NL, we show that well-known gradient-based optimizers (e.g., Adam, SGD with Momentum, etc.) are in fact associative memory modules that aim to compress the gradients with gradient descent. Building on this insight, we present a set of more expressive optimizers with deep memory and/or more powerful learning rules; (2) Self-Modifying Titans: Taking advantage of NL's insights on learning algorithms, we present a novel sequence model that learns how to modify itself by learning its own update algorithm; and (3) Continuum Memory System: We present a new formulation for memory system that generalizes the traditional viewpoint of ``long-term/short-term memory''. Combining our self-modifying sequence model with the continuum memory system, we present a learning module, called HOPE, showing promising results in language modeling, continual learning, and long-context reasoning tasks.},
  langid = {english},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/YRQASQK9/Behrouz et al. - Nested Learning The Illusion of Deep Learning Architectures.pdf}
}

@inproceedings{clark-etal-2022-meta,
  title = {Meta-Learning Fast Weight Language Models},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  author = {Clark, Kevin and Guu, Kelvin and Chang, Ming-Wei and Pasupat, Panupong and Hinton, Geoffrey and Norouzi, Mohammad},
  editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
  year = 2022,
  month = dec,
  pages = {9751--9757},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.emnlp-main.661},
  abstract = {Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates. FWLs can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/ZAWUDU35/Clark et al. - 2022 - Meta-Learning Fast Weight Language Models.pdf}
}

@inproceedings{finnModelagnosticMetalearningFast2017,
  title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = 2017,
  series = {{{ICML}}'17},
  pages = {1126--1135},
  publisher = {JMLR.org},
  address = {Sydney, NSW, Australia},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/YH47YUWF/Finn et al. - 2017 - Model-agnostic meta-learning for fast adaptation of deep networks.pdf}
}

@article{Forgetting_Survey_2024,
  title = {A Comprehensive Survey of Forgetting in Deep Learning beyond Continual Learning},
  author = {Wang, Zhenyi and Yang, Enneng and Shen, Li and Huang, Heng},
  year = 2024,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/AL3DBBNR/Wang et al. - 2024 - A comprehensive survey of forgetting in deep learning beyond continual learning.pdf}
}

@article{Graves2014NeuralTM,
  title = {Neural Turing Machines},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = 2014,
  journal = {ArXiv},
  volume = {abs/1410.5401},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/6TJGNIBQ/Graves et al. - 2014 - Neural turing machines.pdf}
}

@article{hintonUsingFastWeights1987,
  title = {Using {{Fast Weights}} to {{Deblur Old Memories}}},
  author = {Hinton, Geoffrey E. and Plaut, David C.},
  year = 1987,
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {9},
  number = {0},
  urldate = {2025-03-04},
  abstract = {Connectionist models usually have a single weight on each connection. Some interesting newproperties emerge if each connection has two weights: A slowly changing, plastic weight which stores long-term knowledge and a fast-changing, elastic weight which stores temporary knowledge and spontaneously decays towards zero. If a network learns a set of associations and then these associationsare "blurred" by subsequent learning, all the original associations can be "deblurred" by rehearsing on just a few of them. The rehearsal allows the fast weights to take on values that temporarily cancel outthe changes in the slow weights caused by the subsequent learning.},
  langid = {english},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/YZLP9MLD/Hinton and Plaut - 1987 - Using Fast Weights to Deblur Old Memories.pdf}
}

@inproceedings{irieGoingLinearTransformers2021a,
  title = {Going beyond Linear Transformers with Recurrent Fast Weight Programmers},
  booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
  author = {Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  year = 2021,
  series = {Nips '21},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {Transformers with linearised attention ("linear Transformers") have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight Programmers (FWPs) from the '90s. However, the original FWP formulation is more general than the one of linear Transformers: a slow neural network (NN) continually reprograms the weights of a fast NN with arbitrary architecture. In existing linear Transformers, both NNs are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of Transformers and RNNs. In the reinforcement learning setting, we report large improvements over LSTM in several Atari games. Our code is public.},
  articleno = {590},
  isbn = {978-1-7138-4539-3},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/PG7B6CZZ/Irie et al. - 2021 - Going Beyond Linear Transformers with Recurrent Fast Weight Programmers.pdf}
}

@misc{khoslaSurveyMemoryAugmentedNeural2023,
  title = {Survey on {{Memory-Augmented Neural Networks}}: {{Cognitive Insights}} to {{AI Applications}}},
  shorttitle = {Survey on {{Memory-Augmented Neural Networks}}},
  author = {Khosla, Savya and Zhu, Zhen and He, Yifei},
  year = 2023,
  month = dec,
  number = {arXiv:2312.06141},
  eprint = {2312.06141},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.06141},
  urldate = {2025-03-04},
  abstract = {This paper explores Memory-Augmented Neural Networks (MANNs), delving into how they blend human-like memory processes into AI. It covers different memory types, like sensory, short-term, and long-term memory, linking psychological theories with AI applications. The study investigates advanced architectures such as Hopfield Networks, Neural Turing Machines, Correlation Matrix Memories, Memformer, and Neural Attention Memory, explaining how they work and where they excel. It dives into real-world uses of MANNs across Natural Language Processing, Computer Vision, Multimodal Learning, and Retrieval Models, showing how memory boosters enhance accuracy, efficiency, and reliability in AI tasks. Overall, this survey provides a comprehensive view of MANNs, offering insights for future research in memory-based AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/YN9MIJX9/Khosla et al. - 2023 - Survey on Memory-Augmented Neural Networks Cognitive Insights to AI Applications.pdf;/home/jaden/snap/zotero-snap/common/Zotero/storage/XDATIDNX/2312.html}
}

@article{LongMem,
  title = {Augmenting Language Models with Long-Term Memory},
  author = {Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  year = 2023,
  journal = {arXiv preprint arXiv:2306.07174},
  eprint = {2306.07174},
  archiveprefix = {arXiv},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/24U32BNF/Wang et al. - 2023 - Augmenting language models with long-term memory.pdf}
}

@article{lynchLongtermPotentiationMemory2004,
  title = {Long-Term Potentiation and Memory},
  author = {Lynch, M. A.},
  year = 2004,
  month = jan,
  journal = {Physiological Reviews},
  volume = {84},
  number = {1},
  pages = {87--136},
  issn = {0031-9333},
  doi = {10.1152/physrev.00014.2003},
  abstract = {One of the most significant challenges in neuroscience is to identify the cellular and molecular processes that underlie learning and memory formation. The past decade has seen remarkable progress in understanding changes that accompany certain forms of acquisition and recall, particularly those forms which require activation of afferent pathways in the hippocampus. This progress can be attributed to a number of factors including well-characterized animal models, well-defined probes for analysis of cell signaling events and changes in gene transcription, and technology which has allowed gene knockout and overexpression in cells and animals. Of the several animal models used in identifying the changes which accompany plasticity in synaptic connections, long-term potentiation (LTP) has received most attention, and although it is not yet clear whether the changes that underlie maintenance of LTP also underlie memory consolidation, significant advances have been made in understanding cell signaling events that contribute to this form of synaptic plasticity. In this review, emphasis is focused on analysis of changes that occur after learning, especially spatial learning, and LTP and the value of assessing these changes in parallel is discussed. The effect of different stressors on spatial learning/memory and LTP is emphasized, and the review concludes with a brief analysis of the contribution of studies, in which transgenic animals were used, to the literature on memory/learning and LTP.},
  langid = {english},
  pmid = {14715912},
  keywords = {Animals,Brain,Humans,Long-Term Potentiation,Memory,Nerve Growth Factors,Receptors N-Methyl-D-Aspartate,Signal Transduction,Stress Physiological,Synaptic Transmission}
}

@article{martinSynapticPlasticityMemory2000,
  title = {Synaptic Plasticity and Memory: An Evaluation of the Hypothesis},
  shorttitle = {Synaptic Plasticity and Memory},
  author = {Martin, S. J. and Grimwood, P. D. and Morris, R. G.},
  year = 2000,
  journal = {Annual Review of Neuroscience},
  volume = {23},
  pages = {649--711},
  issn = {0147-006X},
  doi = {10.1146/annurev.neuro.23.1.649},
  abstract = {Changing the strength of connections between neurons is widely assumed to be the mechanism by which memory traces are encoded and stored in the central nervous system. In its most general form, the synaptic plasticity and memory hypothesis states that "activity-dependent synaptic plasticity is induced at appropriate synapses during memory formation and is both necessary and sufficient for the information storage underlying the type of memory mediated by the brain area in which that plasticity is observed." We outline a set of criteria by which this hypothesis can be judged and describe a range of experimental strategies used to investigate it. We review both classical and newly discovered properties of synaptic plasticity and stress the importance of the neural architecture and synaptic learning rules of the network in which it is embedded. The greater part of the article focuses on types of memory mediated by the hippocampus, amygdala, and cortex. We conclude that a wealth of data supports the notion that synaptic plasticity is necessary for learning and memory, but that little data currently supports the notion of sufficiency.},
  langid = {english},
  pmid = {10845078},
  keywords = {Amygdala,Animals,Evaluation Studies as Topic,Hippocampus,Humans,Learning,Long-Term Potentiation,Memory,Models Neurological,Neural Pathways,Neuronal Plasticity,Synapses}
}

@inproceedings{miconiDifferentiablePlasticityTraining2018a,
  title = {Differentiable Plasticity: Training Plastic Neural Networks with Backpropagation},
  shorttitle = {Differentiable Plasticity},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Miconi, Thomas and Stanley, Kenneth and Clune, Jeff},
  year = 2018,
  month = jul,
  pages = {3559--3568},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-03-04},
  abstract = {How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains: synaptic plasticity, carefully tuned by evolution to produce efficient lifelong learning. We show that plasticity, just like connection weights, can be optimized by gradient descent in large (millions of parameters) recurrent networks with Hebbian plastic connections. First, recurrent plastic networks with more than two million parameters can be trained to memorize and reconstruct sets of novel, high-dimensional (1000+ pixels) natural images not seen during training. Crucially, traditional non-plastic recurrent networks fail to solve this task. Furthermore, trained plastic networks can also solve generic meta-learning tasks such as the Omniglot task, with competitive results and little parameter overhead. Finally, in reinforcement learning settings, plastic networks outperform non-plastic equivalent in a maze exploration task. We conclude that differentiable plasticity may provide a powerful novel approach to the learning-to-learn problem.},
  langid = {english},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/NZJ93QHA/Miconi et al. - 2018 - Differentiable plasticity training plastic neural networks with backpropagation.pdf;/home/jaden/snap/zotero-snap/common/Zotero/storage/SLAUE23H/Miconi et al. - 2018 - Differentiable plasticity training plastic neural networks with backpropagation.pdf}
}

@inproceedings{mikolovRecurrentNeuralNetwork2010,
  title = {Recurrent Neural Network Based Language Model},
  booktitle = {Interspeech 2010},
  author = {Mikolov, Tom{\'a}{\v s} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v s} and {\v C}ernock{\'y}, Jan and Khudanpur, Sanjeev},
  year = 2010,
  month = sep,
  pages = {1045--1048},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2010-343},
  urldate = {2025-03-04},
  abstract = {A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50\% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18\% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5\% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. Index Terms: language modeling, recurrent neural networks, speech recognition},
  langid = {english},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/STG3FEXR/Mikolov et al. - 2010 - Recurrent neural network based language model.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = 2015,
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2025-03-04},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/RS54YCNC/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf}
}

@article{noklandDirectFeedbackAlignment,
  title = {Direct {{Feedback Alignment Provides Learning}} in {{Deep Neural Networks}}},
  author = {N{\o}kland, Arild},
  abstract = {Artificial neural networks are most commonly trained with the back-propagation algorithm, where the gradient for learning is provided by back-propagating the error, layer by layer, from the output layer to the hidden layers. A recently discovered method called feedback-alignment shows that the weights used for propagating the error backward don't have to be symmetric with the weights used for propagation the activation forward. In fact, random feedback weights work evenly well, because the network learns how to make the feedback useful. In this work, the feedback alignment principle is used for training hidden layers more independently from the rest of the network, and from a zero initial condition. The error is propagated through fixed random feedback connections directly from the output layer to each hidden layer. This simple method is able to achieve zero training error even in convolutional networks and very deep networks, completely without error backpropagation. The method is a step towards biologically plausible machine learning because the error signal is almost local, and no symmetric or reciprocal weights are required. Experiments show that the test performance on MNIST and CIFAR is almost as good as those obtained with back-propagation for fully connected networks. If combined with dropout, the method achieves 1.45\% error on the permutation invariant MNIST task.},
  langid = {english},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/BYJWJWGC/Nøkland - Direct Feedback Alignment Provides Learning in Deep Neural Networks.pdf}
}

@article{ohDiscoveringStateoftheartReinforcement2025,
  title = {Discovering State-of-the-Art Reinforcement Learning Algorithms},
  author = {Oh, Junhyuk and Farquhar, Gregory and Kemaev, Iurii and Calian, Dan A. and Hessel, Matteo and Zintgraf, Luisa and Singh, Satinder and {van Hasselt}, Hado and Silver, David},
  year = 2025,
  month = oct,
  journal = {Nature},
  pages = {1--8},
  issn = {1476-4687},
  doi = {10.1038/s41586-025-09761-x},
  urldate = {2025-12-02},
  abstract = {Humans and other animals use powerful reinforcement learning (RL) mechanisms that have been discovered by evolution over many generations of trial and error. By contrast, artificial agents typically learn using handcrafted learning rules. Despite decades of interest, the goal of autonomously discovering powerful RL algorithms has proven to be elusive1--6. Here we show that it is possible for machines to discover a state-of-the-art RL rule that outperforms manually designed rules. This was achieved by meta-learning from the cumulative experiences of a population of agents across a large number of complex environments. Specifically, our method discovers the RL rule by which the agent's policy and predictions are updated. In our large-scale experiments, the discovered rule surpassed all existing rules on the well-established Atari benchmark and outperformed a number of state-of-the-art RL algorithms on challenging benchmarks that it had not seen during discovery. Our findings suggest that the RL algorithms required for advanced artificial intelligence may soon be automatically discovered from the experiences of agents, rather than manually designed.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science}
}

@inproceedings{pmlr-v80-krause18a,
  title = {Dynamic Evaluation of Neural Sequence Models},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  author = {Krause, Ben and Kahembwe, Emmanuel and Murray, Iain and Renals, Steve},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = 2018,
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {80},
  pages = {2766--2775},
  publisher = {PMLR},
  abstract = {We explore dynamic evaluation, where sequence models are adapted to the recent sequence history using gradient descent, assigning higher probabilities to re-occurring sequential patterns. We develop a dynamic evaluation approach that outperforms existing adaptation approaches in our comparisons. We apply dynamic evaluation to outperform all previous word-level perplexities on the Penn Treebank and WikiText-2 datasets (achieving 51.1 and 44.3 respectively) and all previous character-level cross-entropies on the text8 and Hutter Prize datasets (achieving 1.19 bits/char and 1.08 bits/char respectively).},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/J6XHV2IG/Krause et al. - Dynamic Evaluation of Neural Sequence Models.pdf}
}

@article{regehrShortTermPresynapticPlasticity2012,
  title = {Short-{{Term Presynaptic Plasticity}}},
  author = {Regehr, Wade G.},
  year = 2012,
  month = jul,
  journal = {Cold Spring Harbor Perspectives in Biology},
  volume = {4},
  number = {7},
  pages = {a005702},
  issn = {1943-0264},
  doi = {10.1101/cshperspect.a005702},
  urldate = {2025-12-13},
  abstract = {Different types of synapses are specialized to interpret spike trains in their own way by virtue of the complement of short-term synaptic plasticity mechanisms they possess. Numerous types of short-term, use-dependent synaptic plasticity regulate neurotransmitter release. Short-term depression is prominent after a single conditioning stimulus and recovers in seconds. Sustained presynaptic activation can result in more profound depression that recovers more slowly. An enhancement of release known as facilitation is prominent after single conditioning stimuli and lasts for hundreds of milliseconds. Finally, tetanic activation can enhance synaptic strength for tens of seconds to minutes through processes known as augmentation and posttetantic potentiation. Progress in clarifying the properties, mechanisms, and functional roles of these forms of short-term plasticity is reviewed here., Use-dependent presynaptic plasticity lasting tens of milliseconds to minutes can be divided into these major categories: depression, facilitation, and augmentation/posttetanic potentiation.},
  pmcid = {PMC3385958},
  pmid = {22751149},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/FFG38UF4/Regehr - 2012 - Short-Term Presynaptic Plasticity.pdf}
}

@inproceedings{santoroMetaLearningMemoryAugmentedNeural2016,
  title = {Meta-{{Learning}} with {{Memory-Augmented Neural Networks}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  year = 2016,
  month = jun,
  pages = {1842--1850},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2025-03-04},
  abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
  langid = {english},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/7AYFANM8/Santoro et al. - 2016 - Meta-Learning with Memory-Augmented Neural Networks.pdf}
}

@article{schmidhuberLearningControlFastweight1992,
  title = {Learning to Control Fast-Weight Memories: {{An}} Alternative to Dynamic Recurrent Networks},
  author = {Schmidhuber, J{\"u}rgen},
  year = 1992,
  month = jan,
  journal = {Neural Computation},
  volume = {4},
  number = {1},
  eprint = {https://direct.mit.edu/neco/article-pdf/4/1/131/812242/neco.1992.4.1.131.pdf},
  pages = {131--139},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.1.131},
  abstract = {Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding.},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/YP674PBI/Schmidhuber - 1992 - Learning to control fast-weight memories An alternative to dynamic recurrent networks.pdf}
}

@inproceedings{shinContinualLearningDeep2017,
  title = {Continual Learning with Deep Generative Replay},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
  year = 2017,
  series = {{{NIPS}}'17},
  pages = {2994--3003},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of the hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
  isbn = {978-1-5108-6096-4},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/7QEFJ78G/Shin et al. - 2017 - Continual learning with deep generative replay.pdf}
}

@article{Tack2024OnlineAO,
  title = {Online Adaptation of Language Models with a Memory of Amortized Contexts},
  author = {Tack, Jihoon and Kim, Jaehyung and Mitchell, Eric and Shin, Jinwoo and Teh, Yee Whye and Schwarz, Jonathan Richard},
  year = 2024,
  journal = {ArXiv},
  volume = {abs/2403.04317},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/JBUUKG84/Tack et al. - Online Adaptation of Language Models with a Memory of Amortized Contexts.pdf}
}

@inproceedings{wangAugmentingLanguageModels2023,
  title = {Augmenting Language Models with Long-Term Memory},
  booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
  author = {Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
  year = 2023,
  series = {Nips '23},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {Existing large language models (LLMs) can only afford fix-sized inputs due to the input length limit, preventing them from utilizing rich long-context information from past inputs. To address this, we propose a framework, Language Models Augmented with Long-Term Memory (LONGMEM), which enables LLMs to memorize long history. We design a novel decoupled network architecture with the original backbone LLM frozen as a memory encoder and an adaptive residual side-network as a memory retriever and reader. Such a decoupled memory design can easily cache and update long-term past contexts for memory retrieval without suffering from memory staleness. Enhanced with memory-augmented adaptation training, LONGMEM can thus memorize long past context and use long-term memory for language modeling. The proposed memory retrieval module can handle unlimited-length context in its memory bank to benefit various downstream tasks. Typically, LONGMEM can enlarge the long-form memory to 65k tokens and thus cache many-shot extra demonstration examples as long-form memory for in-context learning. Experiments show that our method outperforms strong long-context models on ChapterBreak, a challenging long-context modeling benchmark, and achieves remarkable improvements on memory-augmented in-context learning over LLMs. The results demonstrate that the proposed method is effective in helping language models to memorize and utilize long-form contents. Our code is open-sourced at https://aka.ms/LongMem.},
  articleno = {3259},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/T23T4LP7/Wang et al. - 2023 - Augmenting language models with long-term memory.pdf}
}

@article{westonMemoryNetworks2014,
  title = {Memory {{Networks}}},
  author = {Weston, J. and Chopra, S. and Bordes, Antoine},
  year = 2014,
  month = oct,
  journal = {CoRR},
  urldate = {2025-03-04},
  abstract = {Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/JN3LRJQR/Weston et al. - 2014 - Memory Networks.pdf}
}

@inproceedings{wuMemorizingTransformers2022,
  title = {Memorizing Transformers},
  booktitle = {The Tenth International Conference on Learning Representations, {{ICLR}} 2022, Virtual Event, April 25-29, 2022},
  author = {Wu, Yuhuai and Rabe, Markus Norman and Hutchins, DeLesley and Szegedy, Christian},
  year = 2022,
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  file = {/home/jaden/snap/zotero-snap/common/Zotero/storage/TZR7BBNN/Wu et al. - 2022 - Memorizing transformers.pdf}
}

@misc{gu_mamba_2024,
	title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	shorttitle = {Mamba},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models ({SSMs}) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the {SSM} parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective {SSMs} into a simplified end-to-end neural network architecture without attention or even {MLP} blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	number = {{arXiv}:2312.00752},
	publisher = {{arXiv}},
	author = {Gu, Albert and Dao, Tri},
	urldate = {2026-01-09},
	date = {2024-05-31},
	eprinttype = {arxiv},
	eprint = {2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/home/jaden/Zotero/storage/E69TXKG6/Gu and Dao - 2024 - Mamba Linear-Time Sequence Modeling with Selective State Spaces.pdf:application/pdf;Snapshot:/home/jaden/Zotero/storage/QJL2E8BY/2312.html:text/html},
}

@inproceedings{peng-etal-2023-rwkv,
    title = "{RWKV}: Reinventing {RNN}s for the Transformer Era",
    author = "Peng, Bo  and
      Alcaide, Eric  and
      Anthony, Quentin  and
      Albalak, Alon  and
      Arcadinho, Samuel  and
      Biderman, Stella  and
      Cao, Huanqi  and
      Cheng, Xin  and
      Chung, Michael  and
      Derczynski, Leon  and
      Du, Xingjian  and
      Grella, Matteo  and
      Gv, Kranthi  and
      He, Xuzheng  and
      Hou, Haowen  and
      Kazienko, Przemyslaw  and
      Kocon, Jan  and
      Kong, Jiaming  and
      Koptyra, Bart{\l}omiej  and
      Lau, Hayden  and
      Lin, Jiaju  and
      Mantri, Krishna Sri Ipsit  and
      Mom, Ferdinand  and
      Saito, Atsushi  and
      Song, Guangyu  and
      Tang, Xiangru  and
      Wind, Johan  and
      Wo{\'z}niak, Stanis{\l}aw  and
      Zhang, Zhenyuan  and
      Zhou, Qinghua  and
      Zhu, Jian  and
      Zhu, Rui-Jie",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.936/",
    doi = "10.18653/v1/2023.findings-emnlp.936",
    pages = "14048--14077",
    abstract = "Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, thus parallelizing computations during training and maintains constant computational and memory complexity during inference. We scale our models as large as 14 billion parameters, by far the largest dense RNN ever trained, and find RWKV performs on par with similarly sized Transformers, suggesting future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling trade-offs between computational efficiency and model performance in sequence processing tasks."
}


@article{graves_hybrid_2016,
	title = {Hybrid computing using a neural network with dynamic external memory},
	volume = {538},
	issn = {1476-4687},
	url = {https://doi.org/10.1038/nature20101},
	doi = {10.1038/nature20101},
	abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.},
	number = {7626},
	journal = {Nature},
	author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
	month = oct,
	year = {2016},
	pages = {471--476},
}