{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 18422222637 sequences.\n",
      "\n",
      "Batch 1\n",
      "Inputs shape: torch.Size([1, 100])\n",
      "Sequence: he little blind girl proves to be of gentle birth as well as of gentle manners only dollie by nina r\n",
      "\n",
      "Batch 2\n",
      "Inputs shape: torch.Size([1, 100])\n",
      "Sequence: nose cheeks and chin nose and chin were long her were high her eyes were pale the lashes so light an\n"
     ]
    }
   ],
   "source": [
    "from dataset_creation import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate the dataset\n",
    "text_dataset = TextDataset(directory='data/SPGC-tokens-2018-07-18/', sequence_length=100)\n",
    "print(f\"Dataset created with {len(text_dataset)} sequences.\")\n",
    "\n",
    "# Create a DataLoader without a sampler\n",
    "dataloader = DataLoader(text_dataset, batch_size=1)\n",
    "\n",
    "# Iterate over a few batches and print their contents\n",
    "for i, (sequences, inputs) in enumerate(dataloader):\n",
    "    if i >= 2:  # Adjust this value to see more/less batches\n",
    "        break\n",
    "\n",
    "    print(f\"\\nBatch {i+1}\")\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "\n",
    "    # Optionally print the actual sequences (comment out if too verbose)\n",
    "    sequence = ''.join([text_dataset.idx_to_char[int(idx)] for idx in inputs[0]])\n",
    "    # target = text_dataset.idx_to_char[int(targets[0])]\n",
    "    print(f\"Sequence: {sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 70\n",
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',', '.', ';', \"'\", '\"', '?', '!', ' ']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define chars using keys of char_to_idx\n",
    "chars = list(text_dataset.char_to_idx.keys())\n",
    "\n",
    "n_characters = len(chars)  # Number of unique characters\n",
    "print(f\"Number of unique characters: {n_characters}\")\n",
    "print(f\"Characters: {chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights: torch.Size([5, 10])\n",
      "Shape of imprints: torch.Size([5, 10])\n",
      "Are the shapes identical? True\n",
      "Weights:\n",
      "  Parameter containing:\n",
      "tensor([[ 0.1692, -0.2211, -0.3019, -0.0297, -0.0795,  0.0222, -0.2560,  0.1446,\n",
      "         -0.2675, -0.1766],\n",
      "        [-0.1050, -0.2448,  0.2424,  0.1025, -0.1480,  0.0234, -0.1034,  0.0003,\n",
      "         -0.2454, -0.2560],\n",
      "        [-0.0302,  0.2932,  0.1877, -0.1359, -0.1733, -0.2020,  0.2475, -0.1967,\n",
      "          0.2036, -0.0674],\n",
      "        [ 0.1616,  0.1043,  0.2931, -0.2403, -0.1408,  0.1341, -0.1179, -0.2809,\n",
      "          0.0460,  0.0395],\n",
      "        [-0.3023,  0.2941,  0.1605,  0.2310,  0.0389, -0.0380, -0.3069,  0.2827,\n",
      "         -0.0042,  0.1564]], requires_grad=True)\n",
      "Weights after imprint:\n",
      "  Parameter containing:\n",
      "tensor([[ 0.2261, -0.3276, -0.6356,  0.0757, -0.1885, -0.0025, -0.3019, -0.0540,\n",
      "         -0.3873, -0.0955],\n",
      "        [-0.1774, -0.1310,  0.3206,  0.0988, -0.1224,  0.0910, -0.1225, -0.0594,\n",
      "         -0.2282, -0.3740],\n",
      "        [-0.0342,  0.4065,  0.3937, -0.1792, -0.1700, -0.1754,  0.3774, -0.1631,\n",
      "          0.2441, -0.2121],\n",
      "        [ 0.0097,  0.3191,  0.4768, -0.2643, -0.0634,  0.2623, -0.1767, -0.3498,\n",
      "          0.1002, -0.1679],\n",
      "        [-0.5361,  0.4647,  0.4777,  0.1247,  0.2790,  0.0804, -0.5368,  0.4568,\n",
      "          0.1600,  0.0904]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HebbianLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(HebbianLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.imprints = nn.Parameter(torch.zeros_like(self.weight))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(input)\n",
    "        output = super(HebbianLinear, self).forward(input)\n",
    "        self.update_imprints(input, output)\n",
    "        # print(output)\n",
    "        return output\n",
    "\n",
    "    def update_imprints(self, input, output):\n",
    "        # print(\"input shape:\", input.shape)\n",
    "        # print(\"output shape:\", output.shape)\n",
    "    \n",
    "        # Hebbian update rule: imprint = input * output\n",
    "        # Adjusting to compute the required [5, 10] imprint matrix for each batch\n",
    "        # Reshape input and output for broadcasting\n",
    "        input_expanded = input.unsqueeze(1)  # Shape: [batch_size, 1, in_features]\n",
    "        output_expanded = output.unsqueeze(2)  # Shape: [batch_size, out_features, 1]\n",
    "\n",
    "        # Element-wise multiplication with broadcasting\n",
    "        # Results in a [batch_size, out_features, in_features] tensor\n",
    "        imprint_update = output_expanded * input_expanded\n",
    "\n",
    "        # Sum over the batch dimension to get the final imprint update\n",
    "        self.imprints.data = imprint_update.sum(dim=0)\n",
    "\n",
    "\n",
    "\n",
    "    def apply_imprints(self, reward, learning_rate, imprint_rate):\n",
    "        # Apply the imprints to the weights\n",
    "        # self.weight.data += reward * learning_rate * self.imprints\n",
    "        imprint_update = self.imprints.data\n",
    "        # print(\"norm_imprint_update:\", norm_imprint_update)\n",
    "\n",
    "        # Apply the normalized imprints\n",
    "        # The reward can be positive (for LTP) or negative (for LTD)\n",
    "        self.weight.data += reward * learning_rate * imprint_update + reward * imprint_rate * imprint_update\n",
    "\n",
    "\n",
    "# Example instantiation of HebbianLinear\n",
    "layer = HebbianLinear(in_features=10, out_features=5)\n",
    "\n",
    "# Checking if the shapes are the same\n",
    "print(\"Shape of weights:\", layer.weight.shape)\n",
    "print(\"Shape of imprints:\", layer.imprints.shape)\n",
    "print(\"Are the shapes identical?\", layer.weight.shape == layer.imprints.shape)\n",
    "\n",
    "# Generate random data\n",
    "input_data = torch.randn(3, 10)  # Batch size of 3, input features 10\n",
    "\n",
    "# Pass data through the HebbianLinear layer\n",
    "output = layer(input_data)\n",
    "\n",
    "print(\"Weights:\\n \", layer.weight)\n",
    "layer.apply_imprints(reward=0.5, learning_rate=0.1, imprint_rate=0.1)\n",
    "print(\"Weights after imprint:\\n \", layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Using HebbianLinear instead of Linear\n",
    "        self.linear_layers = torch.nn.ModuleList([HebbianLinear(input_size + hidden_size, hidden_size)])\n",
    "        for _ in range(1, num_layers):\n",
    "            self.linear_layers.append(HebbianLinear(hidden_size, hidden_size))\n",
    "\n",
    "        # Final layers for hidden and output, also using HebbianLinear\n",
    "        self.i2h = HebbianLinear(hidden_size, hidden_size)\n",
    "        self.i2o = HebbianLinear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "\n",
    "        # Pass through the Hebbian linear layers with ReLU\n",
    "        for layer in self.linear_layers:\n",
    "            combined = layer(combined)\n",
    "            combined = F.relu(combined)\n",
    "\n",
    "        # Split into hidden and output\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        # print(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def apply_imprints(self, reward, learning_rate, imprint_rate):\n",
    "        # Apply imprints for all HebbianLinear layers\n",
    "        for layer in self.linear_layers:\n",
    "            layer.apply_imprints(reward, learning_rate, imprint_rate)\n",
    "        self.i2h.apply_imprints(reward, learning_rate, imprint_rate)\n",
    "        self.i2o.apply_imprints(reward, learning_rate, imprint_rate)\n",
    "\n",
    "\n",
    "# Ensure the input size matches the number of features for each input\n",
    "input_size = n_characters\n",
    "output_size = n_characters\n",
    "n_hidden = 128\n",
    "rnn = SimpleRNN(input_size, n_hidden, output_size,3)\n",
    "\n",
    "# Define the loss function (criterion) and optimizer\n",
    "criterion = torch.nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "\n",
    "# Apply Clipping\n",
    "def clip_weights(model, max_norm):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data.clamp_(-max_norm, max_norm)\n",
    "\n",
    "# In your training loop, after the weight update step\n",
    "clip_weights(rnn, max_norm=0.5)  # Choose an appropriate max_norm value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([5, 1, 70])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return text_dataset.char_to_idx[letter]\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_characters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_characters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_characters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_characters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ty little winifred efforts to find some children of whom she reads in a book lead to the acquaintanc',\n",
       " tensor([19, 24, 69, 11,  8, 19, 19, 11,  4, 69, 22,  8, 13,  8,  5, 17,  4,  3,\n",
       "         69,  4,  5,  5, 14, 17, 19, 18, 69, 19, 14, 69,  5,  8, 13,  3, 69, 18,\n",
       "         14, 12,  4, 69,  2,  7,  8, 11,  3, 17,  4, 13, 69, 14,  5, 69, 22,  7,\n",
       "         14, 12, 69, 18,  7,  4, 69, 17,  4,  0,  3, 18, 69,  8, 13, 69,  0, 69,\n",
       "          1, 14, 14, 10, 69, 11,  4,  0,  3, 69, 19, 14, 69, 19,  7,  4, 69,  0,\n",
       "          2, 16, 20,  0,  8, 13, 19,  0, 13,  2]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('well then here are his words shalt not make unto thee any graven image or any this passage refers on',\n",
       " tensor([22,  4, 11, 11, 69, 19,  7,  4, 13, 69,  7,  4, 17,  4, 69,  0, 17,  4,\n",
       "         69,  7,  8, 18, 69, 22, 14, 17,  3, 18, 69, 18,  7,  0, 11, 19, 69, 13,\n",
       "         14, 19, 69, 12,  0, 10,  4, 69, 20, 13, 19, 14, 69, 19,  7,  4,  4, 69,\n",
       "          0, 13, 24, 69,  6, 17,  0, 21,  4, 13, 69,  8, 12,  0,  6,  4, 69, 14,\n",
       "         17, 69,  0, 13, 24, 69, 19,  7,  8, 18, 69, 15,  0, 18, 18,  0,  6,  4,\n",
       "         69, 17,  4,  5,  4, 17, 18, 69, 14, 13]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def randomTrainingExample():\n",
    "    \"\"\"Generate a random training example from the dataset\"\"\"\n",
    "    sequence, line_tensor = text_dataset[np.random.randint(len(text_dataset))]\n",
    "    return sequence, line_tensor\n",
    "\n",
    "randomTrainingExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "imprint_rate = 0.001\n",
    "last_n_rewards = [0]\n",
    "last_n_reward_avg = 0\n",
    "n_rewards = 100\n",
    "def train(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    losses = []\n",
    "    output = None\n",
    "    for i in range(line_tensor.size()[0] - 1):\n",
    "        hot_input_char_tensor = torch.nn.functional.one_hot(line_tensor[i], num_classes=n_characters).type(torch.float).unsqueeze(0)\n",
    "        output, hidden = rnn(hot_input_char_tensor, hidden)\n",
    "\n",
    "        # print(\"output shape:\", output.shape)\n",
    "        # print(\"line_tensor shape:\", line_tensor.shape)\n",
    "        # print(output)\n",
    "        # print(line_tensor[-1].unsqueeze(0))\n",
    "        loss = criterion(output, line_tensor[-1].unsqueeze(0))\n",
    "        # print(loss)\n",
    "\n",
    "        # Convert loss to a reward signal\n",
    "        reward = 1 / (1 + loss.item())  # Example conversion, assuming loss is non-negative\n",
    "        # print(reward)\n",
    "\n",
    "        # update last_n_rewards\n",
    "        last_n_rewards.append(reward)\n",
    "        if len(last_n_rewards) > n_rewards:\n",
    "            last_n_rewards.pop(0)\n",
    "        last_n_reward_avg = sum(last_n_rewards) / len(last_n_rewards)\n",
    "        reward_update = reward - last_n_reward_avg\n",
    "        # print(reward_update)\n",
    "        clip_weights(rnn, max_norm=0.5)  # Choose an appropriate max_norm value\n",
    "\n",
    "        # Apply Hebbian updates\n",
    "        rnn.apply_imprints(reward_update, learning_rate, imprint_rate)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "    return output, loss_avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0% (0m 3s) 4.2440 ay something to expostulate but she got a fresh start and hurried on i recognised you in that silly  / F ✗ ( )\n",
      "100 0% (0m 7s) 4.2872  initial plants commemorating individual men douglas spruce coulter pine are written without the mar / n ✗ (r)\n",
      "150 0% (0m 10s) 4.2138 odern norway iii the people and their industries iv on the farm manners and customs vi school and pl / B ✗ (l)\n",
      "200 0% (0m 13s) 4.2438 core que pour lui est un être mère et les autres objets il leur parle ils se taisent les touche ils  / B ✗ ( )\n",
      "250 0% (0m 16s) 4.2870  he helped her from the buggy at the gate chapter xxxvi on the morning following their arrival at br / F ✗ (r)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_iters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     sequence, line_tensor \u001b[38;5;241m=\u001b[39m randomTrainingExample()\n\u001b[0;32m---> 25\u001b[0m     output, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Print ``iter`` number, loss, name and guess\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(line_tensor)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(line_tensor\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     12\u001b[0m     hot_input_char_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(line_tensor[i], num_classes\u001b[38;5;241m=\u001b[39mn_characters)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhot_input_char_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# print(\"output shape:\", output.shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# print(\"line_tensor shape:\", line_tensor.shape)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# print(output)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# print(line_tensor[-1].unsqueeze(0))\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, line_tensor[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mSimpleRNN.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Pass through the Hebbian linear layers with ReLU\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layers:\n\u001b[0;32m---> 24\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     combined \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(combined)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Split into hidden and output\u001b[39;00m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mHebbianLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(input)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHebbianLinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_imprints(\u001b[38;5;28minput\u001b[39m, output)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# print(output)\u001b[39;00m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 50\n",
    "plot_every = 10\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    sequence, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        # Use the output to generate a character prediction\n",
    "        topv, topi = output.topk(1, dim=1)  # Change dim to 1\n",
    "        predicted_char = text_dataset.idx_to_char[topi[0, 0].item()]\n",
    "        target_char = sequence[-1]\n",
    "        correct = '✓' if predicted_char == target_char else '✗ (%s)' % target_char\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, sequence, predicted_char, correct))\n",
    "\n",
    "        # also print some weights:\n",
    "        # print(\"i2h weights:\", rnn.i2h.weight)\n",
    "        # print(\"i2o weights:\", rnn.i2o.weight)\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
