{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 18422222637 sequences.\n",
      "\n",
      "Batch 1\n",
      "Inputs shape: torch.Size([1, 100])\n",
      "Sequence: e what you say things have gone too far to stop here did you say that spotswoode knew something abou\n",
      "\n",
      "Batch 2\n",
      "Inputs shape: torch.Size([1, 100])\n",
      "Sequence:  elder negroes mam chloe whatever may have been her reserved rights of private judgment backed him u\n"
     ]
    }
   ],
   "source": [
    "from dataset_creation import TextDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate the dataset\n",
    "text_dataset = TextDataset(directory='data/SPGC-tokens-2018-07-18/', sequence_length=100)\n",
    "print(f\"Dataset created with {len(text_dataset)} sequences.\")\n",
    "\n",
    "# Create a DataLoader without a sampler\n",
    "dataloader = DataLoader(text_dataset, batch_size=1)\n",
    "\n",
    "# Iterate over a few batches and print their contents\n",
    "for i, (sequences, inputs) in enumerate(dataloader):\n",
    "    if i >= 2:  # Adjust this value to see more/less batches\n",
    "        break\n",
    "\n",
    "    print(f\"\\nBatch {i+1}\")\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "\n",
    "    # Optionally print the actual sequences (comment out if too verbose)\n",
    "    sequence = ''.join([text_dataset.idx_to_char[int(idx)] for idx in inputs[0]])\n",
    "    # target = text_dataset.idx_to_char[int(targets[0])]\n",
    "    print(f\"Sequence: {sequence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 70\n",
      "Characters: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',', '.', ';', \"'\", '\"', '?', '!', ' ']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define chars using keys of char_to_idx\n",
    "chars = list(text_dataset.char_to_idx.keys())\n",
    "\n",
    "n_characters = len(chars)  # Number of unique characters\n",
    "print(f\"Number of unique characters: {n_characters}\")\n",
    "print(f\"Characters: {chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights: torch.Size([5, 10])\n",
      "Shape of imprints: torch.Size([5, 10])\n",
      "Are the shapes identical? True\n",
      "Weights:\n",
      "  Parameter containing:\n",
      "tensor([[-0.1377, -0.0629,  0.0757,  0.0547, -0.2133, -0.1716, -0.0403, -0.0059,\n",
      "         -0.1945,  0.0075],\n",
      "        [ 0.2724, -0.1911,  0.1146, -0.2831, -0.0203, -0.1604, -0.2103,  0.0553,\n",
      "          0.1948,  0.2969],\n",
      "        [-0.1136,  0.0831,  0.1487, -0.1349, -0.0984,  0.1712,  0.2403, -0.2754,\n",
      "          0.0765, -0.1830],\n",
      "        [ 0.3091, -0.0926,  0.1333, -0.2558, -0.2457, -0.2646, -0.1673, -0.0724,\n",
      "          0.2906,  0.0629],\n",
      "        [-0.2091,  0.2978, -0.1066,  0.1143,  0.2084,  0.0126,  0.0767,  0.1859,\n",
      "         -0.1296, -0.0390]], requires_grad=True)\n",
      "Weights after imprint:\n",
      "  Parameter containing:\n",
      "tensor([[-1.8243e-01,  5.3678e-02,  1.4110e-01,  5.7566e-02, -2.9015e-01,\n",
      "         -2.2302e-01, -8.8420e-02,  1.2002e-01, -2.4858e-01, -5.0518e-02],\n",
      "        [ 4.1269e-01, -4.2201e-01,  1.3695e-01, -3.6886e-01,  1.2698e-01,\n",
      "         -1.9216e-01, -2.3746e-02, -1.2704e-01,  3.0208e-01,  4.4414e-01],\n",
      "        [-3.4241e-04, -8.3088e-02,  1.9788e-02, -2.3330e-01, -1.8817e-02,\n",
      "          2.3298e-01,  3.6723e-01, -4.9738e-01,  1.3616e-01, -7.0634e-02],\n",
      "        [ 3.9798e-01, -1.9257e-01,  1.7903e-01, -3.4123e-01, -1.9119e-01,\n",
      "         -3.2116e-01, -4.4318e-02, -1.4232e-01,  3.3259e-01,  1.4391e-01],\n",
      "        [-3.6031e-01,  5.7029e-01, -1.5006e-02,  2.0061e-01,  4.3750e-02,\n",
      "         -3.5661e-02, -1.0592e-01,  4.6432e-01, -2.4875e-01, -2.0322e-01]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class HebbianLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(HebbianLinear, self).__init__(in_features, out_features, bias)\n",
    "        self.imprints = nn.Parameter(torch.zeros_like(self.weight))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(input)\n",
    "        output = super(HebbianLinear, self).forward(input)\n",
    "        self.update_imprints(input, output)\n",
    "        # print(output)\n",
    "        return output\n",
    "\n",
    "    def update_imprints(self, input, output):\n",
    "        # print(\"input shape:\", input.shape)\n",
    "        # print(\"output shape:\", output.shape)\n",
    "    \n",
    "        # Hebbian update rule: imprint = input * output\n",
    "        # Adjusting to compute the required [5, 10] imprint matrix for each batch\n",
    "        # Reshape input and output for broadcasting\n",
    "        input_expanded = input.unsqueeze(1)  # Shape: [batch_size, 1, in_features]\n",
    "        output_expanded = output.unsqueeze(2)  # Shape: [batch_size, out_features, 1]\n",
    "\n",
    "        # Element-wise multiplication with broadcasting\n",
    "        # Results in a [batch_size, out_features, in_features] tensor\n",
    "        imprint_update = output_expanded * input_expanded\n",
    "\n",
    "        # Sum over the batch dimension to get the final imprint update\n",
    "        self.imprints.data = imprint_update.sum(dim=0)\n",
    "\n",
    "\n",
    "\n",
    "    def apply_imprints(self, reward, learning_rate, imprint_rate):\n",
    "        # Apply the imprints to the weights\n",
    "        # self.weight.data += reward * learning_rate * self.imprints\n",
    "        imprint_update = self.imprints.data\n",
    "        # print(\"norm_imprint_update:\", norm_imprint_update)\n",
    "\n",
    "        # Apply the normalized imprints\n",
    "        # The reward can be positive (for LTP) or negative (for LTD)\n",
    "        self.weight.data += reward * learning_rate * imprint_update + reward * imprint_rate * imprint_update\n",
    "\n",
    "\n",
    "# Example instantiation of HebbianLinear\n",
    "layer = HebbianLinear(in_features=10, out_features=5)\n",
    "\n",
    "# Checking if the shapes are the same\n",
    "print(\"Shape of weights:\", layer.weight.shape)\n",
    "print(\"Shape of imprints:\", layer.imprints.shape)\n",
    "print(\"Are the shapes identical?\", layer.weight.shape == layer.imprints.shape)\n",
    "\n",
    "# Generate random data\n",
    "input_data = torch.randn(3, 10)  # Batch size of 3, input features 10\n",
    "\n",
    "# Pass data through the HebbianLinear layer\n",
    "output = layer(input_data)\n",
    "\n",
    "print(\"Weights:\\n \", layer.weight)\n",
    "layer.apply_imprints(reward=0.5, learning_rate=0.1, imprint_rate=0.1)\n",
    "print(\"Weights after imprint:\\n \", layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Using HebbianLinear instead of Linear\n",
    "        self.linear_layers = torch.nn.ModuleList([HebbianLinear(input_size + hidden_size, hidden_size)])\n",
    "        for _ in range(1, num_layers):\n",
    "            self.linear_layers.append(HebbianLinear(hidden_size, hidden_size))\n",
    "\n",
    "        # Final layers for hidden and output, also using HebbianLinear\n",
    "        self.i2h = HebbianLinear(hidden_size, hidden_size)\n",
    "        self.i2o = HebbianLinear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "\n",
    "        # Pass through the Hebbian linear layers with ReLU\n",
    "        for layer in self.linear_layers:\n",
    "            combined = layer(combined)\n",
    "            combined = F.relu(combined)\n",
    "\n",
    "        # Split into hidden and output\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        # print(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def apply_imprints(self, reward, learning_rate, imprint_rate):\n",
    "        # Apply imprints for all HebbianLinear layers\n",
    "        for layer in self.linear_layers:\n",
    "            layer.apply_imprints(reward, learning_rate, imprint_rate)\n",
    "        self.i2h.apply_imprints(reward, learning_rate, imprint_rate)\n",
    "        self.i2o.apply_imprints(reward, learning_rate, imprint_rate)\n",
    "\n",
    "\n",
    "# Ensure the input size matches the number of features for each input\n",
    "input_size = n_characters\n",
    "output_size = n_characters\n",
    "n_hidden = 128\n",
    "rnn = SimpleRNN(input_size, n_hidden, output_size,3)\n",
    "\n",
    "# Define the loss function (criterion) and optimizer\n",
    "criterion = torch.nn.NLLLoss()\n",
    "# optimizer = torch.optim.Adam(rnn.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "\n",
    "# Apply Clipping\n",
    "def clip_weights(model, max_norm):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.data.clamp_(-max_norm, max_norm)\n",
    "\n",
    "# In your training loop, after the weight update step\n",
    "clip_weights(rnn, max_norm=0.5)  # Choose an appropriate max_norm value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([5, 1, 70])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return text_dataset.char_to_idx[letter]\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_characters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_characters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_characters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_characters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "print(letterToTensor('J'))\n",
    "\n",
    "print(lineToTensor('Jones').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('es i repeated aloud in dancing along the sentence sounded important and pleased my ears presently i ',\n",
       " tensor([ 4, 18, 69,  8, 69, 17,  4, 15,  4,  0, 19,  4,  3, 69,  0, 11, 14, 20,\n",
       "          3, 69,  8, 13, 69,  3,  0, 13,  2,  8, 13,  6, 69,  0, 11, 14, 13,  6,\n",
       "         69, 19,  7,  4, 69, 18,  4, 13, 19,  4, 13,  2,  4, 69, 18, 14, 20, 13,\n",
       "          3,  4,  3, 69,  8, 12, 15, 14, 17, 19,  0, 13, 19, 69,  0, 13,  3, 69,\n",
       "         15, 11,  4,  0, 18,  4,  3, 69, 12, 24, 69,  4,  0, 17, 18, 69, 15, 17,\n",
       "          4, 18,  4, 13, 19, 11, 24, 69,  8, 69]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('eks but his eyes like crystal clear with truth and the woman who knew not that she was a widow but f',\n",
       " tensor([ 4, 10, 18, 69,  1, 20, 19, 69,  7,  8, 18, 69,  4, 24,  4, 18, 69, 11,\n",
       "          8, 10,  4, 69,  2, 17, 24, 18, 19,  0, 11, 69,  2, 11,  4,  0, 17, 69,\n",
       "         22,  8, 19,  7, 69, 19, 17, 20, 19,  7, 69,  0, 13,  3, 69, 19,  7,  4,\n",
       "         69, 22, 14, 12,  0, 13, 69, 22,  7, 14, 69, 10, 13,  4, 22, 69, 13, 14,\n",
       "         19, 69, 19,  7,  0, 19, 69, 18,  7,  4, 69, 22,  0, 18, 69,  0, 69, 22,\n",
       "          8,  3, 14, 22, 69,  1, 20, 19, 69,  5]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def randomTrainingExample():\n",
    "    \"\"\"Generate a random training example from the dataset\"\"\"\n",
    "    sequence, line_tensor = text_dataset[np.random.randint(len(text_dataset))]\n",
    "    return sequence, line_tensor\n",
    "\n",
    "randomTrainingExample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "imprint_rate = 0.1\n",
    "last_n_rewards = [0]\n",
    "last_n_reward_avg = 0\n",
    "n_rewards = 100\n",
    "def train(line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0] - 1):\n",
    "        hot_input_char_tensor = torch.nn.functional.one_hot(line_tensor[i], num_classes=n_characters).type(torch.float).unsqueeze(0)\n",
    "        output, hidden = rnn(hot_input_char_tensor, hidden)\n",
    "\n",
    "    # print(\"output shape:\", output.shape)\n",
    "    # print(\"line_tensor shape:\", line_tensor.shape)\n",
    "    # print(output)\n",
    "    # print(line_tensor[-1].unsqueeze(0))\n",
    "    loss = criterion(output, line_tensor[-1].unsqueeze(0))\n",
    "    # print(loss)\n",
    "\n",
    "    # Convert loss to a reward signal\n",
    "    reward = 1 / (1 + loss.item())  # Example conversion, assuming loss is non-negative\n",
    "    # print(reward)\n",
    "\n",
    "    # update last_n_rewards\n",
    "    last_n_rewards.append(reward)\n",
    "    if len(last_n_rewards) > n_rewards:\n",
    "        last_n_rewards.pop(0)\n",
    "    last_n_reward_avg = sum(last_n_rewards) / len(last_n_rewards)\n",
    "    reward_update = reward - last_n_reward_avg\n",
    "    # print(reward_update)\n",
    "    clip_weights(rnn, max_norm=0.5)  # Choose an appropriate max_norm value\n",
    "\n",
    "    # Apply Hebbian updates\n",
    "    rnn.apply_imprints(reward_update, learning_rate, imprint_rate)\n",
    "\n",
    "    # Perform backward pass and optimizer step if using gradient descent for other parameters\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0% (0m 1s) 4.2209 �龍率眾見師兄 說情由放走二班頭 斷魂山巧遇獨行俠 人認識我 鄧飛雄訪友走天涯 黃花鋪救人打惡棍 英雄奮勇鬥群賊 惡霸安心施詭計 鄭華雄慷慨救友 惡匪棍見色起心 定巧計曹先生受贓 囑賊人惡家奴弄權 你 / f ✗ (你)\n",
      "100 0% (0m 3s) 4.3242  arline pushed away the proffered money impatiently time are you goin to tell how much you need lem  / f ✗ ( )\n",
      "150 0% (0m 5s) 4.3460  the broad frozen river and upon its icy surface it was so deep that meadow and river were undisting / f ✗ (g)\n",
      "200 0% (0m 6s) 4.2411 eir mill running full force for more than six months is this hunter an expert also oh no parkinson s / f ✗ (s)\n",
      "250 0% (0m 8s) 4.2205 nd unwatched or she will accomplish nothing one will be in waiting who will restore the and claim th / f ✗ (h)\n",
      "300 0% (0m 9s) 4.3121 ent et maintenant il vous faut ma vie et vous me tuez grâce monsieur le duc norbert poussa un cri te / f ✗ (e)\n",
      "350 0% (0m 11s) 4.2338 t in pitting his talent against that of betterton there was no doubt that a couple of years ago harr / f ✗ (r)\n",
      "400 0% (0m 13s) 4.2226 put butter in a stewpan set it on the fire when melted put the pumpkin in stir about five minutes ha / f ✗ (a)\n",
      "450 0% (0m 15s) 4.3281 lonsakin osoittaa kuinka syvästi hänen sydämmensä on liikutettu raatiherra sitä ei voi ollenkaan san / f ✗ (n)\n",
      "500 0% (0m 16s) 4.3265 ssion of the country above ava for three years he at last compelled the burmese to sue for peace on  / f ✗ ( )\n",
      "550 0% (0m 18s) 4.2516 roduction in vain circular notes were exhibited and letters of credit on beyrout elias was inexorabl / f ✗ (l)\n",
      "600 0% (0m 20s) 4.2208  and the maiden came forth but before she could return into the tree the prince caught hold of her a / f ✗ (a)\n",
      "650 0% (0m 21s) 4.3258 n northwest and southeast instead of between southwest and northeast as it ought to do unless there  / f ✗ ( )\n",
      "700 0% (0m 23s) 4.2476 armaan silloin niin kiireellä piilotti heti kun näki vieraan lähestyvän minä otin sen alas ja katsel / f ✗ (l)\n",
      "750 0% (0m 25s) 4.2230 her his own faith nor her serenity could keep from depression and discouragement through the whole p / f ✗ (p)\n",
      "800 0% (0m 26s) 4.3283 he hoped the property of all sorts would be allowed to rest in abeyance until the fact was ascertain / f ✗ (n)\n",
      "850 0% (0m 28s) 4.3239  leant against a railing to steady himself for he was dizzy with agitation he looked up to the calm  / f ✗ ( )\n",
      "900 0% (0m 30s) 4.3263 eedily rescue him from their talons describe the gentleman madam and my messenger shall go and seek  / f ✗ ( )\n",
      "950 0% (0m 31s) 4.3264 en you become telegraphic in manner i always grow nervous there was suppressed excitement at the on  / f ✗ ( )\n",
      "1000 1% (0m 33s) 4.2192 nt from the fulfilment of many prophecies and the authority of many miracles that these prophecies h / f ✗ (h)\n",
      "1050 1% (0m 35s) 4.3264 es he wings the sail the shore is gained and thou art free sir aubrey de vere is a poet profound in  / f ✗ ( )\n",
      "1100 1% (0m 36s) 4.3284 what do i care for the county d the county i often wish that i been a younger son as you are captain / f ✗ (n)\n",
      "1150 1% (0m 38s) 4.2257 th a stone do you suppose there are any more in the tree asked the rich boy still keeping at a dista / f ✗ (a)\n",
      "1200 1% (0m 40s) 4.2953  the very essence of all that is tame and commonplace compared to this darling rural village look do / f ✗ (o)\n",
      "1250 1% (0m 41s) 4.2949 ooler in the barn than it is here anyhow that so admitted bert and oh i know how we can have packs o / f ✗ (o)\n",
      "1300 1% (0m 43s) 4.3269  present see it served as the residence of the bishop within the see where the governor now resides  / f ✗ ( )\n",
      "1350 1% (0m 45s) 4.1223 fratelli visigoti o goti occidentali partiti poco men che un secolo addietro a correr a capitare e f / f ✓\n",
      "1400 1% (0m 46s) 4.2335  inventions he had not the least intention of ever seeing or communicating with the late vicar of br / f ✗ (r)\n",
      "1450 1% (0m 48s) 4.2379 ment of the french revolution this was published at davies and its announcement in the papers was pr / f ✗ (r)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_iters \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     sequence, line_tensor \u001b[38;5;241m=\u001b[39m randomTrainingExample()\n\u001b[0;32m---> 25\u001b[0m     output, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     current_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Print ``iter`` number, loss, name and guess\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(line_tensor)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(line_tensor\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     11\u001b[0m     hot_input_char_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(line_tensor[i], num_classes\u001b[38;5;241m=\u001b[39mn_characters)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhot_input_char_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print(\"output shape:\", output.shape)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(\"line_tensor shape:\", line_tensor.shape)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(output)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(line_tensor[-1].unsqueeze(0))\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, line_tensor[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mSimpleRNN.forward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Pass through the Hebbian linear layers with ReLU\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_layers:\n\u001b[0;32m---> 24\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     combined \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(combined)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Split into hidden and output\u001b[39;00m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m, in \u001b[0;36mHebbianLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print(input)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mHebbianLinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_imprints(\u001b[38;5;28minput\u001b[39m, output)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# print(output)\u001b[39;00m\n",
      "File \u001b[0;32m~/memory_encoding/memory_encoding/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 50\n",
    "plot_every = 10\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    sequence, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print ``iter`` number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        # Use the output to generate a character prediction\n",
    "        topv, topi = output.topk(1, dim=1)  # Change dim to 1\n",
    "        predicted_char = text_dataset.idx_to_char[topi[0, 0].item()]\n",
    "        target_char = sequence[-1]\n",
    "        correct = '✓' if predicted_char == target_char else '✗ (%s)' % target_char\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, sequence, predicted_char, correct))\n",
    "\n",
    "        # also print some weights:\n",
    "        # print(\"i2h weights:\", rnn.i2h.weight)\n",
    "        # print(\"i2o weights:\", rnn.i2o.weight)\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(current_loss / plot_every)\n",
    "        current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
